The Biolinguistic Enterprise

Oxford Studies in Biolinguistics
General editor: Cedric Boeckx, Catalan Institute for Advanced Studies (ICREA) and Center for Theoretical Linguistics at the Universitat Autònoma de Barcelona Advisory editors: Anna Maria Di Sciullo, Université du Québec à Montréal; Simon Fisher, The Wellcome Trust Centre for Human Genetics; W. Tecumseh Fitch, Universität Wien; Angela D. Friederici, Max Planck Institute for Human Cognitive and Brain Sciences; Andrea Moro, Vita-Salute San Raﬀaele University; Kazuo Okanoya, Brain Science Institute, Riken; Massimo Piattelli-Palmarini, University of Arizona; David Poeppel, New York University; Maggie Tallerman, Newcastle University
Published The Biolinguistic Enterprise Edited by Anna Maria Di Sciullo and Cedric Boeckx
In preparation The Phonological Architecture: A Biolinguistic Perspective By Bridget Samuels
The series welcomes contributions from researchers in many ﬁelds, including linguistic computation, language development, language evolution, cognitive neuroscience, and genetics. It also considers proposals which address the philosophical and conceptual foundations of the ﬁeld, and is open to work informed by all theoretical persuasions.

The Biolinguistic Enterprise
New Perspectives on the Evolution and Nature of the Human Language Faculty
Edited by ANNA MARIA DI SCIULLO AND CEDRIC BOECKX
1

3
Great Clarendon Street, Oxford ox2 6dp Oxford University Press is a department of the University of Oxford. It furthers the University’s objective of excellence in research, scholarship, and education by publishing worldwide in Oxford New York Auckland Cape Town Dar es Salaam Hong Kong Karachi Kuala Lumpur Madrid Melbourne Mexico City Nairobi New Delhi Shanghai Taipei Toronto With oﬃces in Argentina Austria Brazil Chile Czech Republic France Greece Guatemala Hungary Italy Japan Poland Portugal Singapore South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press in the UK and in certain other countries
Published in the United States by Oxford University Press Inc., New York
© editorial matter and organization Anna Maria Di Sciullo and Cedric Boeckx 2011 © the chapters their several authors 2011
The moral rights of the authors have been asserted Database right Oxford University Press (maker)
First published 2011
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, without the prior permission in writing of Oxford University Press, or as expressly permitted by law, or under terms agreed with the appropriate reprographics rights organization. Enquiries concerning reproduction outside the scope of the above should be sent to the Rights Department, Oxford University Press, at the address above
You must not circulate this book in any other binding or cover and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data Data available
Library of Congress Cataloging in Publication Data
Library of Congress Control Number: 2010930309
Typeset by SPI Publisher Services, Pondicherry, India Printed in Great Britain on acid-free paper by MPG Books Group, Bodmin and King’s Lynn
ISBN 978–0–19–955327–3 (Hbk) 978–0–19–955328–0 (Pbk)
1 3 5 7 9 10 8 6 4 2

Contents

General Preface

vii

The Contributors

viii

Abbreviations

xii

1. Introduction: Contours of the Biolinguistic Research Agenda

1

Anna Maria Di Sciullo and Cedric Boeckx

Part I. Evolution

2. The Biolinguistic Program: The Current State of its Development

19

Robert C. Berwick and Noam Chomsky

3. Some Reﬂections on Darwin’s Problem in the Context of Cartesian

Biolinguistics

42

Cedric Boeckx

4. Syntax Facit Saltum Redux: Biolinguistics and the Leap to Syntax

65

Robert C. Berwick

5. A Geneticist’s Dream, a Linguist’s Nightmare: The Case of FOXP2 100 Massimo Piattelli-Palmarini and Juan Uriagereka

6. Biolinguistic Investigations: Genetics and Dynamics

126

Lyle Jenkins

7. “Deep Homology” in the Biology and Evolution of Language

135

W. Tecumseh Fitch

Part II. Variation

8. The Three Design Factors in Evolution and Variation

169

Lyle Jenkins

9. Three Factors in Language Variation

180

Charles Yang

10. Approaching Parameters from Below

205

Cedric Boeckx

11. (Bio)linguistic Variation: Have/be Alternations in the Present Perfect 222 M. Rita Manzini and Leonardo M. Savoia

vi Contents

12. The Biolinguistic Program and Historical Reconstruction

266

Giuseppe Longobardi and Cristina Guardiano

13. A Biolinguistic Approach to Variation

305

Anna Maria Di Sciullo

Part III. Computation

14. Antisymmetry and the Lexicon

329

Richard S. Kayne

15. What Kind of Computing Device is the Human Language Faculty? 354 Howard Lasnik

16. Clauses, Propositions, and Phases

366

Richard K. Larson

17. Reﬂections on the Optimal Solution: On the Syntactic

Representation of Indexicality

392

Alessandra Giorgi

18. Emergence of a Systemic Semantics through Minimal and

Underspeciﬁed Codes

417

Wolfram Hinzen

19. Bridging the Gap between Brain and Syntax: A Case for a Role of

the Phonological Loop

440

Carlo Cecchetto and Costanza Papagno

20. All You Need is Merge: Biology, Computation, and Language from

the Bottom Up

461

Robert C. Berwick

References

492

Index

545

General Preface
This series aims to shed light on the biological foundations of human language. Biolinguistics is an important new interdisciplinary ﬁeld that sets out to explore the basic properties of human language and to investigate how it matures in the individual, how it is put to use in thought and communication, what brain circuits implement it, what combination of genes supports it, and how it emerged in our species. In addressing these questions the series aims to advance our understanding of the interactions of mind and brain in the production and reception of language, to discover the components of the brain that are unique to language (especially those that also seem unique to humans), and to distinguish them from those that are shared with other cognitive domains.
Advances in theoretical linguistics, genetics, developmental and comparative psychology, the evo–devo program in biology, and cognitive neuroscience have made it possible to formulate novel, testable hypotheses concerning these basic questions. Oxford Studies in Biolinguistics will contribute to the emerging synthesis among these ﬁelds by encouraging and publishing books that show the value of transdisciplinary dialogue, and which highlight the unique research opportunities such a dialogue oﬀers.
Contributions to the series are likely to come from researchers in many ﬁelds, including linguistic computation, language development, language evolution, cognitive neuroscience, and genetics. The series welcomes work that addresses the philosophical and conceptual foundations of the ﬁeld, and is open to work informed by all theoretical persuasions. We expect authors to present their arguments and ﬁndings in a manner that can be understood by scholars in every discipline on which their work has a bearing.
The Biolinguistic Enterprise launches the series. It showcases some of the most important work in progress, especially those aspects of biolinguistics that bear on the evolution of the language faculty and observed variations of the linguistic phenotype.
Cedric Boeckx Barcelona, May 2010

The Contributors
Robert C. Berwick is Professor of Computer Science and Computational Linguistics in the Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology. Professor Berwick is the recipient of a Guggenheim Fellowship and co-Director of the MIT Center for Biological and Computational Learning. He is the author of ten books covering language acquisition, computational complexity and language, and parsing.
Cedric Boeckx is Research Professor at the Catalan Institute for Advanced Studies (ICREA), and a member of the Center for Theoretical Linguistics at the Universitat Autònoma de Barcelona. Most recently he was Associate Professor of Linguistics at Harvard University. He is the author of Islands and Chains (John Benjamins, 2003), Linguistic Minimalism (Oxford University Press, 2006), Understanding Minimalist Syntax (Blackwell, 2007), Bare Syntax (Oxford University Press, 2008) and Language in Cognition (Wiley-Blackwell, 2009); he is founding co-editor, with Kleanthes K. Grohmann, of the Open Access journal Biolinguistics.
Carlo Cecchetto received his Ph.D. from the University of Milan and is currently Professor of Linguistics at the University of Milan-Bicocca. He has published several papers in major journals on the following topics: the theory of Quantiﬁer Raising and the syntax–semantics interface, labeling and phrase structure theory, the syntax of left and right dislocation, sign-language syntax, and the role of memory resources in (spoken and sign) language comprehension. He has previously taught at the University of Siena and at UCLA.
Noam Chomsky was born on December 7, 1928 in Philadelphia, Pennsylvania. He received his Ph.D. in linguistics in 1955 from the University of Pennsylvania. During the years 1951 to 1955, Chomsky was a Junior Fellow of the Harvard University Society of Fellows. The major theoretical viewpoints of his doctoral dissertation appeared in the monograph Syntactic Structures, 1957. This formed part of a more extensive work, The Logical Structure of Linguistic Theory, circulated in mimeograph in 1955 and published in 1975. Chomsky joined the staﬀ of the Massachusetts Institute of Technology in 1955 and in 1961 was appointed full professor. In 1976 he was appointed Institute Professor in the Department of Linguistics and Philosophy. Chomsky has lectured at many universities across the globe, and is the recipient of numerous honorary degrees and awards. He has written and lectured widely on linguistics, philosophy, intellectual history, contemporary issues, international aﬀairs, and U.S. foreign policy. Among his recent books are New Horizons in the Study of Language and Mind and On Nature and Language.

The Contributors ix
Anna Maria Di Sciullo is Full Professor of Linguistics at the University of Quebec in Montreal, and the director of Major Collaborative Research Initiatives on Asymmetry and Interfaces. She held visiting positions at MIT and at the University of Venice. She published several books, including Hedges, Heads, and Projections (John Benjamins, 2010), Asymmetry in Morphology (MIT Press, 2005), UG and External Systems (John Benjamins, 2005), Asymmetry in Grammar (John Benjamins, 2003), Projections and Interface Conditions: Essays on Modularity (Oxford University Press, 1997), and, with Edwin Williams, On the Deﬁnition of Word (MIT Press, 1987). She is the founder of the International Network on Biolinguistics.
W. Tecumseh Fitch is Professor of Cognitive Biology in the Department of Cognitive Biology, University of Vienna. His main interests are in bioacoustics, especially vertebrate vocal production, and the evolution of cognition, particularly the evolution of human speech, language, music, and art, all studied from a strongly comparative perspective. His research focuses on experimental investigations of vocal production and cognition in humans and a wide range of vertebrates, including chimpanzees, monkeys, seals, deer, dogs, alligators, ravens, and parrots.
Alessandra Giorgi studied in Rome, University La Sapienza, in Pisa, Scuola Normale Superiore, and in the Department of Linguistics and Philosophy at MIT. She is presently Professor of Linguistics at the University Ca’ Foscari of Venice, Italy. She is the author of the following monographs: The Syntax of Noun Phrases (Cambridge University Press, 1991, with G. Longobardi); Tense and Aspect: From Semantics to Morphosyntax (Oxford University Press, 1997, with F. Pianesi); and, most recently, About the Speaker: Towards a Syntax of Indexicality (Oxford University Press, 2010).
Cristina Guardiano is currently assistant professor at the University of Modena and Reggio Emilia. She earned her Ph.D. in historical (Greek) linguistics from the University of Pisa in 2003. Her research interests are historical syntax, parametric theories, and the syntax of the nominal domain.
Wolfram Hinzen is a professor in the department of philosophy at Durham University, United Kingdom, where he moved coming from the University of Amsterdam in 2006. He is the author of the books Mind Design and Minimal Syntax and An Essay on Names and Truth, both published by Oxford University Press.
Lyle Jenkins studied problems in theoretical syntax (modality and the existential construction) at MIT under Noam Chomsky within the Extended Standard Theory, one of the ﬁrst theories of generative grammar to address the study of language from a biolinguistic perspective. After completing his Ph.D., Jenkins taught at a number of universities, including the University of Vienna, the University of Salzburg, and the University of Paris VIII (Vincennes). At Salzburg he helped to organize the Linguistic Society of America’s (LSA) Summer Institute on the Biology of Language, which included the participation of Konrad Lorenz and other biologists. While a Visiting Fellow at the Department of Biological Chemistry, Harvard University, Jenkins worked

x The Contributors
on the SV-40 monkey virus. Together with Allan Maxam, who (with Walter Gilbert) had developed the chemical method of DNA sequencing and done pioneering work in the area of immunology, Jenkins helped organize the Harvard Medical School Biolinguistics Group and later the Biolinguistics Institute in Cambridge, Massachusetts, to promote interdisciplinary work in the ﬁeld of biolinguistics.
Richard S. Kayne received his Ph.D. from MIT in 1969 and his Docteur ès Lettres degree from the University of Paris VIII in 1976. In 1995, he was awarded a Doctorate honoris causa by the University of Leiden, the Netherlands. His work over the years has concentrated on syntactic theory, with recent emphasis on comparative syntax. He is the editor of Oxford Studies in Comparative Syntax, and has published six books: French Syntax (MIT Press, 1975); Connectedness and Binary Branching (Foris, 1984); The Antisymmetry of Syntax (MIT Press, 1994); Parameters and Universals (Oxford University Press, 2000); Movement and Silence (Oxford University Press, 2005); Comparisons and Contrasts (Oxford University Press, 2010). He is currently Silver Professor in the Department of Linguistics at New York University.
Howard Lasnik is Distinguished University Professor of Linguistics at the University of Maryland. He has played a prominent role in syntactic theorizing in the Chomskyan framework, starting with the Extended Standard Theory, through Government– Binding theory, to Minimalism. His main research areas are syntactic theory and the syntax–semantics interface. Alongside more foundational issues of language learnability and the general properties of linguistic theories, among the speciﬁc topics he has worked on are scope, anaphora, ellipsis, verbal morphology, Case, and locality constraints on movement.
Richard K. Larson held appointments at the University of Pennsylvania (1984–5) and the Massachusetts Institute of Technology (1985–9), before joining the faculty at the University at Stony Brook, where he is currently Professor of Linguistics. His research has examined a wide variety of topics in syntax and semantics, including relative and adverbial clauses, NP adverbs, disjunctions, prepositional phrases, double objects, and clausal complements. It has also involved a wide variety of languages, including Warlpiri, Japanese, Korean, Mandarin Chinese, Persian, Zazaki, Gilaki, and Pashto.
Giuseppe Longobardi is Professor of General Linguistics at the University of Trieste. He has held positions at the Scuola Normale Superiore in Pisa, the University of Venice, University of Vienna, UCLA, USC, Harvard, CNRS, and published extensively on syntactic theory and historical syntax.
Maria Rita Manzini is Professor of General Linguistics at the University of Florence, studied at MIT (Ph.D., 1983) and held the positions of lecturer and then reader at University College London. She is the author of several books, including Locality (MIT Press, 1992), and, in collaboration with Leonardo Savoia, A Uniﬁcation of Morphology and Syntax (Routledge, 2007) as well as the three-volume work I dialetti italiani

The Contributors xi
(Dell’Orso, 2005). She is the author of several dozen articles in international journals and books.
Costanza Papagno received her M.D. and her Ph.D. at the University of Milan and is currently full professor of Neuropsychology at the University of Milan-Bicocca. She has published several papers in major journals on the following topics: verbal short-term memory and language acquisition and comprehension, aphasia, ﬁgurative language processing, and impairment of semantic memory. She has previously worked at the MRC Applied Psychology Unit in Cambridge and at the University of Palermo.
Massimo Piattelli-Palmarini is Professor of Cognitive Science at the University of Arizona. From January 1994 to July 1999 he was the director of the Department of Cognitive Science (Dipsco), of the Scientiﬁc Institute San Raﬀaele, in Milan (Italy), and professor of Cognitive Psychology at the San Raﬀaele University. From September 1985 to December 1993 he was Principal Research Scientist at the Center for Cognitive Science at MIT. He has been a visiting professor at the University of Maryland, at MIT, at the Collège de France, at Rutgers University, and at Harvard University. He is the author of Inevitable Illusions (Wiley, 1994), the editor of Language and Learning: The Debate between Jean Piaget and Noam Chomsky (Harvard University Press, 1980, translated into eleven languages) and the co-editor of Of Minds and Language: A Dialogue with Noam Chomsky in the Basque Country (Oxford University Press, 2009).
Leonardo M. Savoia is Full Professor of General Linguistics at the University of Florence, Director of the Department of Linguistics, and has been Head of the Faculty of Education (Magistero) at the same university. He is the author of several books, including A Uniﬁcation of Morphology and Syntax (Routledge, 2007) and the threevolume work I dialetti italiani (Dell’Orso, 2005), in collaboration with Rita Manzini, as well as of several dozen articles in journals and books.
Juan Uriagereka’s research ranges from comparative grammar to the neurobiological bases of language. He has (co-)directed twenty Ph.D. theses, (co-) authored/edited eight books, eighty-ﬁve articles and chapters, and given two hundred talks, including a dozen keynote speeches. He has received several awards for his research, advising and teaching, and has worked at various universities in four continents. Participating in many professional groups internationally, he has obtained a dozen research grants.
Charles Yang is Associate Professor of Linguistics at the University of Pennsylvania. His book, Knowledge and Learning in Natural Language, was published by OUP in 2001. His interests include language acquisition and change, computational linguistics, morphology, and psycholinguistics.

Abbreviations

1

ﬁrst person

2

second person

3

third person

A abs acc AD Adj AFP akefi AR ASL asp aux

Adenine (as used in Berwick chapter) absolutive accusative Alzheimer’s dementia adjective anterior forebrain pathway aﬀected KE family individuals Ariellese American Sign Language aspect auxiliary

BEI

passive marker in Mandarin

C C CHL C-I CD CG comp cop CS CypG

Cytosine Complementizer computational system of human language conceptual–intentional Complementizer Deletion Classical Greek Complementizer Copula computational language system Cypriot Greek

D D-structure DAR

determiner Deep structure double-access reading

dat decl Det DP DVD

dative declarative determiner determiner phrase Developmental Verbal Dyspraxia

ECP EM EPP EST evo–devo

Empty Category Principle external merge Extended Projection Principle Extended Standard Theory evolutionary–developmental biology

f FA FI FL FLB FLN fMRI fut

Feminine Fallese full interpretation faculty of language faculty of language in the broad sense faculty of language in the narrow sense functional magnetic resonance imaging future

G

Guanine

GB

Government–Binding

gen

genitive

GR

Grico

H HCN HLN HOX HVC

head human congnition in the narrow sense human language in the narrow sense Homeobox high vocal center

I ILF IM Imp, impf ind

inﬂection interpreted logical form internal merge imperfect indicative

Abbreviations xiii

xiv Abbreviations

INTR IT ITV

intransitive Italian intentional transitive verb

LCA LCA LDA LF LIS LMAN loc LOTN

last common ancestor linear-correspondence axiom long-distance anaphor Logical Form Italian Sign Langauge lateral magnocellular nucleus of the nidopallium locative language of thought in the narrow sense

M m MA masc MG MGP MP MtB

middle/intransitive marker masculine Marcillian masculine Modern Greek Modularized Global Parametrization Minimalist Program Mitteilungsbedürfnis

N nom NTC

noun nominative no-tampering condition

O

object

P&P part PCM PDH Perf, prf PF Pl, pl PP PRED

Principles and Parameters partitive Parametric Comparison Method pervasive deep homology perfect Phonological Form, Phonetic Form plural prepositional phrase predicate

Abbreviations xv

PRES PRO prt PS

present pronoun participle phrase structure

refl

reﬂexive

S s S-M S-R S-structure Sg, sg SLI SMT SNP STM STP subj SUT

subject singular sensory-motor stimulus–response Surface structure singular speciﬁc language impairment Strong Minimalist Thesis single nucleotide polymorphism short-term memory superior temporal plane subjunctive Strong Uniformity Thesis

T

Thymine (as used in Berwick chapter)

T

Tense

Tns

tense

top

topic

TV

transitive verb

UG

Universal Grammar

USV

ultrasonic vocalization

V

verb

V2

Verb second

VP

verb phrase

WBS WGA WM

Williams–Beuren syndrome whole-genome (or genome-wide) association studies working memory

This page intentionally left blank

1
Introduction: Contours of the Biolinguistic Research Agenda
ANNA MARIA DI SCIULLO AND CEDRIC BOECKX
The term biolinguistics is currently enjoying a new lease of life, and the interdisciplinary research program behind it, renewed appreciation. The ﬁrst mention of the term (Meader and Muysken 1950) went largely unnoticed, as far as we have been able to determine from the vanishingly few references to it in the literature. It was not until the early 1970s that the term caught on. It was used in the title of a conference “A Debate on Bio-Linguistics,” organized by Massimo Piattelli-Palmarini in 1974, and it ﬁgures prominently in an address at the annual meeting of the American Association for the Advancement of Science made by Salvador Luria in 1976, which we will come back to in this introduction. After that, the term went underground and did not reappear until the turn of the millennium, as the title of a book by Lyle Jenkins (2000). Since then it has ﬁgured prominently in titles of talks, courses, articles, books, workshops, and conferences; it even became the title of a new journal and is the rallying call behind the formation of various research initiatives in Montreal, Barcelona, and elsewhere, and an international biolinguistic network was created in 2007. The present volume itself is the fruit of two conferences on biolinguistics that took place in Santo Domingo and Venice in 2007. Because of this recent re-emergence of the term, many who come across it feel the need for clariﬁcation, which we hope to provide in this introduction, meant to provide a context for the chapters that follow.
“Biolinguistics” expresses more transparently than any other term we know of what deﬁnes modern linguistics since the ‘cognitive revolution’ of the 1950s. Back then, under the impetus of Noam Chomsky, Eric Lenneberg, and Morris Halle, the ﬁeld of linguistics abandoned its focus on external behavior and followed a path that was more decidedly cognitive—indeed biological—as it turned its attention to the organism that makes language possible. Chomsky’s devastating review of Skinner’s Verbal Behavior in 1959 (Chomsky 1959a)

2 Anna Maria Di Sciullo and Cedric Boeckx
quickly became a landmark, and, combined with the analysis of English verbal morphology in Syntactic Structures (1957), laid the foundations of generative grammar masterfully articulated in the introductory chapter of Aspects of the Theory of Syntax (1965). At the same time Eric Lenneberg was working on his Biological Foundations of Language (1967), where he stressed the need to study “language as a natural phenomenon—an aspect of [man’s] biological nature, to be studied in the same manner as, for instance, his anatomy.”
Biological Foundations of Language was a seminal, ground-breaking work that anticipated many of the issues in the agenda of contemporary research programs concerned with the biology of language. Even a cursory look over the table of contents of the monograph shows that the book discusses and/or foreshadows such topics as the genetics of language, the biological (physiological) correlates of language, the growth of language in the individual (i.e. the progress of language acquisition), the interplay of brain processes (e.g. lateralization) with the growth of language, the neurological aspects of speech and language, the neural mechanism supporting language, and the evolution of language in the species. It also contains an appendix, written by Noam Chomsky, on formal aspects of grammar, and another by Otto Marx on the history of this research program. Each of these topics has spawned many areas of research that are currently being intensely explored anew in a necessarily interdisciplinary framework.
We believe that the renewed appreciation for biolinguistic research questions is due to a variety of factors. First, the decidedly interdisciplinary perspective adopted in introductory texts such as Uriagereka (1998) and Jenkins (2000) no doubt led many linguists to reopen the dialog with their colleagues in adjacent disciplines. Both Jenkins and Uriagereka stress the importance of addressing head-on the “logical problem of language evolution” (how could such an object as the human language faculty have emerged in the species?) from a multi-disciplinary perspective, to avoid the facile adaptationist, just-so story traps that are all too familiar (cf. Pinker and Bloom 1990). Interestingly, the same message in favor of a pluralist, less genocentric/adaptationist approach to central issues in biology can be seen with the rise of the evolutionary developmental (evo–devo) paradigm (see, among many others, Carroll 2005a) that (bio)linguists are now appealing to on a regular basis.1
1 Incidentally, Jenkins’s and Uriagereka’s messages are not without antecedent. Both refer to Piattelli-Palmarini’s (1989) compelling rejection of adaptationist scenarios in the context of language evolution (the article that sparked Pinker and Bloom’s 1990 response), and both point out that Chomsky has voiced the same concerns on numerous occasions since his earliest writings. For an excellent collection of relevant passages from the Chomsky corpus, see Otero (1990).

Introduction 3
At roughly the same time, geneticists in Europe made the breakthrough discovery of a link between the language deﬁcit manifested by the now famous KE family members and a speciﬁc gene (FOXP2). Though long suspected, the genetic basis of language received a tremendous boost from this (still ongoing) research. Although it is crystal clear that FOXP2 is not the language gene, investigations into the role of this gene brought linguists, cognitive scientists, neurologists, and biologists to ask one another questions, and to learn from one another’s ﬁelds how to interpret the data, and this discovery would be our second force behind the renewal of interest in biolinguistics. This process requires temporarily abandoning, and at least moving beyond the jargon of one’s ﬁeld to articulate linking hypotheses. The need to adopt concepts that would be commensurable across disciplines has been stressed for several years by neurolinguists such as David Poeppel in an attempt to circumvent the sterile interactions among subﬁelds of linguistics and adjacent disciplines that so far have led many to express skepticism about the very feasibility of biolinguistics. Fortunately these calls to develop a lingua franca for biolinguistic investigations have been taking place at a time when theoretical linguists themselves, under Chomsky’s impetus, decided (for largely independent reasons) to revisit the very foundations of their ﬁeld, and explore the plausibility of a ‘minimalist’ program for linguistic theory—the third force, in our opinion, behind the re-emergence of biolinguistics.
Let us (all too brieﬂy) sketch what the minimalist program amounts to. After roughly ﬁfteen years of intensive work on properties of the language faculty under the banner of ‘generative grammar’ (more accurately, the extended standard theory), Chomsky got the impression in the late 1980s that the overall approach was well established, and that it was time to take the next step on the research agenda of the generative enterprise. This next step amounts to an attempt to go beyond explanatory adequacy. Chomsky (1965) distinguishes between three kinds of adequacy: observational, descriptive, and explanatory; not surprisingly, he puts a premium on explanatory adequacy. The aim of (generative) linguistics was ﬁrst and foremost to account for the amazing feat of human language acquisition in all its subtleties. Once it was felt that the model was suﬃciently well established, it became natural to ask how one could make sense of the properties of the language faculty that the model posits— how much sense can we make of this architecture of language? Put diﬀerently, why does the language faculty have this sort of architecture?
Quite reasonably, Chomsky formulated this quest beyond explanatory adequacy in the most ambitious form (what is known as the strong minimalist thesis), in the form of a challenge to the linguistic community: Can it be shown that the computational system at the core of the language faculty is

4 Anna Maria Di Sciullo and Cedric Boeckx
optimally or perfectly designed to meet the demands on the systems of the mind/brain it interacts with? By optimal or perfect design Chomsky meant to explore the idea that all properties of the computational system of language can be made to follow from minimal design speciﬁcations, also known as “bare output conditions”—the sort of properties that the system would have to have to be usable at all (e.g. all expressions generated by the computational system should be legible, that is, formatted in a way that the external systems can handle). In other words, the computational system of language, minimalistically construed, would consist solely of the most eﬃcient algorithm to interface with the other components of the mind, the simplest procedure to compute (generate) its outputs (expressions) and communicate them to the organs of the mind that will interpret them and allow them to enter into thought and action. If the strong minimalist thesis were true, the language faculty (FL) would be an “ideal” linguistic system. But it should be stressed that the point of the minimalist program is not to prove the validity of this extreme thesis, but to see how far the thesis can take us, how productive this mode of investigation can be. The strong minimalist thesis amounts to asking whether we can make perfect sense of the FL. Asking this question is the best way to ﬁnd out how much sense we can make out of the FL. The points where the minimalist program fails will mark the limits of our understanding. If one cannot make perfect sense of some property P of the FL (i.e. if P cannot be given a minimalist rationale in terms of computational eﬃciency towards interface demands), then P is just something one must live with, some accident of history, a quirk of brain evolution, some aspect of the FL that one must recognize in some brute force fashion.
Strategically speaking, the minimalist program forces linguists to reformulate previous ﬁndings in terms of elementary units, operations, and interface conditions. Many of these, minimalists anticipate, will have such a generic ﬂavor to them (“combine,” “map onto a linear sequence,” etc.) that they are plausibly not speciﬁc to the language faculty. This should be very good news to researchers in other areas, as the concepts articulated by minimalists may ﬁnd an equivalent in their own ﬁeld or be more readily testable using familiar techniques. At the same time, these generic operations make it more plausible to entertain “descent with modiﬁcation” scenarios concerning the evolution of language.
In sum, linguistic minimalism contributes to the end of what we would like to call linguistic isolationism—an inevitable period of over-modularization in generative grammar during which the language faculty as a whole was studied sui generis, as an autonomous system bearing little or no resemblance to other cognitive modules in humans, or in other species.

Introduction 5
We realize that by placing the minimalist program at the center of the revived ﬁeld of biolinguistics (both in this introduction and in the contributions to this volume) we are formulating a controversial hypothesis—at the very least one that runs the risk of alienating those who view the theoretical aspects of ‘Chomskyan’ linguistics with skepticism (not to say contempt), but still see language as a biological object, to be studied ‘biolinguistically.’ Let us be clear. Biolinguistics is a fairly broad research program, and allows for the exploration of many avenues of research: formalist; functionalist; nativist and insisting on the uniqueness of the language faculty; nativist about general (human) cognition, but not about language per se; etc. From Chomsky to Givón, from Lenneberg to Tomasello—all of this is biolinguistics. In practice, though, it is fair to say that the term “biolinguistics” is more narrowly construed as a more transparent label for generative-oriented studies (but see Givón 2002 for an important exception). This narrower characterization is perhaps in part due to the fact that the term biolinguistics was ﬁrst used with the generative enterprise in mind (Piattelli-Palmarini 1974; Luria 1976; Jenkins 2000). But we think that it also reﬂects the fact that despite much criticism, the basic tenets of the generative approach to the language faculty remain the very best bet we have to carry the biolinguistic program forward. One should bear in mind that the latest instantiation of the generative enterprise, the minimalist program, is just as broad and ‘theory-neutral’ as biolinguistics itself. As Chomsky himself has remarked (2007c: 4):
Recent inquiry into these questions in the case of language has come to be called “the minimalist program”, but there has been so much misunderstanding, even within professional circles, that it is perhaps worth reiterating that it is a program, not a theory, and a program that is both traditional in its general ﬂavor and pretty much theoryneutral, insofar as the biolinguistic framework is adopted. [. . . ] And whatever one’s beliefs about design of language may be, the questions of the research program arise. It may also be worth mentioning that the program can only be pursued, whatever theoretical framework one adopts, insofar as some descriptive account of the phenomena to be explained is reasonably unproblematic, often not the case of course, as expected with any system of at least apparent intricacy.
In other words, there are many alternative minimalist visions one can entertain. As soon as one begins to ask “why is the language faculty that way,” like it or not, we are in minimalist territory as well as in biolinguistic territory.
We regard Eric Lenneberg’s book, Biological Foundations of Language, published forty years ago, as the best example of interdisciplinary research in biolinguistics. We feel that with the advent of the minimalist program and its emphasis on interfaces and primitive operations, it has become

6 Anna Maria Di Sciullo and Cedric Boeckx
harder to formulate “purely” theoretical proposals, without any regard to interdisciplinary issues. It has certainly become more common to see theoretical linguists speculate on some of the biolinguistic implications of their proposals, or even motivate their premises on biolinguistic grounds. Thus it is more and more common to ﬁnd introductory statements such as this (taken from Yang, this volume):
How much should we ask of Universal Grammar? Not too little, for there must be a place for our unique ability to acquire a language along with its intricacies and curiosities. But asking for too much won’t do either. A theory of Universal Grammar is a statement of human biology, and one needs to be mindful of the limited structural modiﬁcation that would have been plausible under the extremely brief history of Homo sapiens evolution.
This is what Chomsky has in recent years called “Approaching UG from Below.” The present volume contains contributions to biolinguistics; but even in those contributions that are less explicit about interdisciplinary issues we hope that the biolinguistic implications will be fairly clear. Most of these contributions have been placed in the computation section of the book, but as editors we insist that this was not in any way intended to marginalize them against the (necessarily) interdisciplinary contributions in the section on evolution. We regard issues of computation as central to the biolinguistic enterprise, and just as we have encouraged the authors of these contributions to formulate their hypotheses in a way that would be transparent to experts in other ﬁelds (a request facilitated by the authors’ adoption of the minimalist perspective), we urge non-theoretical linguists to take these claims about linguistic computation very seriously indeed, for failure to do so has almost invariably led to unconstrained, unrealistic, and ultimately doomed hypotheses regarding the evolution, neural implementation, acquisition, and use of language (witness the obvious limitations of adaptationist and connectionist hypotheses concerning language, both in terms of empirical coverage and explanation).
In addition to the sections on computation and evolution that we just mentioned, the present volume oﬀers contributions pertaining to variation. In many ways variation is the natural ‘interface’ or meeting ground between language and biology. Both linguistics and biology have long focused on diversity, its nature and origin. Both biolinguistics (of a generative persuasion) and what biologists (following in Goethe’s footsteps) call (Theoretical) Morphology are interested in deﬁning the limits of variation (the ‘morphospaces’) for the language organ and organisms more generally. Furthermore, Chomsky has long made clear that the concept of parameter grew out of interactions

Introduction 7
with biologists like Jacob and Monod, whose works gave rise to the modern evo–devo paradigm (see Chomsky 1980). These forces conspire to making variation a central aspect of the biolinguistic agenda. The contributions included in this volume are not intended to provide an exhaustive list of issues that arise in biolinguistics but we feel that as a whole they oﬀer a representative panorama of current research in this growing ﬁeld.
In an address delivered at the 1976 AAAS meeting, Nobel laureate Salvador Luria said this about biolinguistics:
In closing, let me single out one frontier that today may seem as unreachable as Mount Everest seemed to be 50 years ago. And yet, it is exciting enough to warrant serious attention. I refer to what I may call Biolinguistics, or the biology of human language. The reason for singling out this ﬁeld is two-fold. First, human language is the special faculty through which all conscious human activity is ﬁltered; a faculty whose development may well have played the driving role in the almost catastrophic evolution from ape-kind to human-kind. And second, language alone, among the uniquely human faculties of the human brain oﬀers to the scientist a systematic theory, the structural linguistics developed by Chomsky and his colleagues, which may be amenable to confrontation with biological data. What I mean is that formal language structure, plus some facts already known about language and brain structure, plus the advancing knowledge of brain organization provided by physiologists, hold some promise that between linguistics and neurobiology a kind of convergence may soon be possible that may provide a wedge into the biological exploration of the human mind.
The confrontation, and possible uniﬁcation, envisaged by Luria strikes us as more feasible now than ever before. Work in theoretical linguistics to which most of the contributors of this volume have actively contributed over the years has re-opened the necessarily interdisciplinary dialog that deﬁned the biolinguistic agenda; they have provided us with a new golden opportunity to shed new light on the language faculty, its nature, origin, acquisition, and use, an opportunity that the present volume hopes to highlight.
Summary of the contributions
The Biolinguistic Enterprise: New Perspectives on the Evolution and Nature of the Human Language Faculty brings together three sets of chapters. The ﬁrst addresses the evolutionary origin of language: How could such an object as the human language faculty have emerged in the species? They also speak to the uniﬁcation problem. The second part addresses the question of why languages vary. The chapters in this section provide evidence that understanding language variation from a biolinguistic perspective may lead to a deeper

8 Anna Maria Di Sciullo and Cedric Boeckx
understanding of linguistic diversity. Part 3 is concerned with questions of computation. The chapters explore the properties of the computational system in the faculty of language, narrowly construed (FLN), as well as its relation to the broader language faculty (FLB), which includes the sensory–motor and conceptual–intentional systems. Within the foundational assumptions of the minimalist program, these chapters show that it has been a productive assumption that the FLN is an optimal solution to certain legibility conditions imposed by the sensory–motor and conceptual–intentional systems. Taken has a whole, the chapters contribute to the biolinguistic program, a program that speciﬁcally asks why-questions (why these principles not others) and raises issues related to biology, evolution, and neural representations. The following paragraphs summarize them.
Part I: Evolution
Robert Berwick and Noam Chomsky’s chapter focuses on the dual questions of language’s origin and variation, in particular how these questions may be informed by both recent biological work on evolution and development and the framework provided by the Minimalist Program. Human language appears to be best regarded as an internal computational system driven by a single recursive operator yielding hierarchical structure, with its particular functional design shaped by the link to systems of thought and action, rather than by the secondary process of externalization, that is, sensory-motor parsing or speech/sign. What relevant biological and evolutionary results there are, including recent results on FOXP2 and its analogous eﬀects in other species such as songbirds, as well as computational considerations of parsing eﬃciency, all point to this conclusion. This chapter also reviews how an account of the apparently recent and saltational evolutionary emergence of this recursive system of “discrete inﬁnity,” apparently unique to the human species, proﬁts from an examination of the ongoing revision of evolutionary explanation in light of new research on the link between evolution and development, which highlights the possibility of relatively rapid shifts in an organism’s biology, given even minor genomic variation.
Cedric Boeckx goes in the same direction in drawing a parallelism between the problem in biology of explaining how language, with no apparent homolog in the biological world, originated in humans, and the problem of explaining how language growth is possible through interaction with an environment lacking in negative evidence (Plato’s problem). Boeckx examines the nature of lexicalization and suggests “that turning concepts into lexical items that can merge at inﬁnitum goes a long way toward characterizing the

Introduction 9
source of what makes humans cognitively special.” The chapter also highlights a certain degree of convergence between recent developments in biolinguistics and evo–devo biology.
The chapter by Robert Berwick is related to the preceding two. It too points out that the fact that human language seems to be unique in the biological world presents a problem for an evolutionary theory of language based on the assumption that the evolution of language has proceeded on the basis of a continuous, adaptive process. This is because language is unique and therefore apparently a discontinuous trait. Berwick considers how to resolve the discontinuity problem, arguing that the micromutational adaptationist view adopted in the modern synthesis theory of evolution need not hold, and is replaceable by a deeper deductive system of regulatory interaction, or, alternatively, by single adaptive changes with far-reaching eﬀect. Capitalizing on Hauser, Chomsky, and Fitch’s (2002) proposal that, of the components of the FLB, only the FLN is unique to the human species, he argues that if the FLN consists of a single structure-building operation with many cascading eﬀects, human language is less discontinuous than previously thought. The punchline is that only merge is to be explained and so far seems to remain discontinuous; there is no need to look for adaptive explanations for each of the cascading eﬀects following automatically from the operation.
The following chapters in this section, too, are concerned with questions of methodology in biolinguistics, dealing, even if indirectly, with questions pertaining to the uniﬁcation problem. They relate to the preceding chapters as they also discuss questions on the evolution of language, taking into consideration what we know of the genetic basis of language, in particular the evolution of FOXP2, and comparative studies on humans and non-human primates.
Massimo Piattelli-Palmarini and Juan Uriagereka point out that much publicity has been given to the identiﬁcation of a speciﬁc mutation in the FOXP2 gene that causes various language impairments in the aﬀected members of a British and Canadian family designated by the letters KE. Myrna Gopnik and collaborators had initially diagnosed a very speciﬁc “feature blindness” (problems with tense, number, agreement, and gender) in otherwise perfectly normal subjects, a perfect case of dissociation between general intelligence and language. Subsequent reﬁned neurological (PET and fMRI) analyses and a rich battery of tests have corrected this claim. Orofacial dyspraxia and a limited capacity to learn complex sequences of oral movements have been proposed (contra Gopnik and colleagues) as the cause of reduced morphosyntactic “skills” [sic]. A reconstruction of the evolution of the FOXP2 gene

10 Anna Maria Di Sciullo and Cedric Boeckx
in the mammalian world has prompted claims of an adaptive “sweep” from chimpanzees to humans, stressing the role of communication in the shaping of language. The most detailed genetic analysis today possible and a very detailed analysis of the neuronal correlates of the mutation seem to lead to a conception of language that many ﬁnd quite problematic (language as part of motor planning and language acquisition as a step in the learning of complex motor sequences). The authors present the case and suggest that this picture of language (oﬀered by distinguished neurologists and geneticists, but non-linguists) is objectionable. A diﬀerent lesson from this case is proposed, centrally involving the evolution of language.
Jenkins argues for a comparative method under the assumption that homologs to natural language could be found in other organisms and domains. Jenkins starts by stating that the study of the biology of language, biolinguistics, encompasses the study of knowledge of language, its development in the child, and its evolution in the species. Each of these areas can be studied while abstracting away from underlying physical, neural, and genetic mechanisms. For example, one can study the language faculty by studying properties of language such as word order and distribution of pronouns and anaphors. The development of language can be investigated by looking for both universal principles and language-speciﬁc variation across many languages. Language evolution, its use and development, can be studied by mathematical models, simulating language change in time. Jenkins emphasizes that the biolinguistic program hopes to relate the properties of the language faculty discovered in linguistics (and in particular in the research areas outlined above) to physical mechanisms at the level of language micro-circuits of the brain and of language genetics. Finally, he emphasizes that abstract principles of language such as (a)symmetry and minimization principles may be related to and be illuminated by similar kinds of mechanisms found in other biological systems.
Tecumseh Fitch argues that many of the brain components that interact with the faculty of language are widely shared with other species although some seem to be uniquely human (unusual) in the biological world. Fitch points out that some of the novel capacities may depend on diﬀerent underlying mechanisms that have a separate evolutionary history. Addressing how the mechanisms in question evolved, Fitch argues, requires a comparative approach that goes beyond the study of non-human primates to include virtually any living organism. “Only data from widely disparate species will allow us to determine selective forces that might drive convergent evolution . . . and determine the time depth of homologous mechanisms that are broadly shared.” Recent molecular and developmental data oﬀer an exciting

Introduction 11
and unexpected additional argument for a very broad comparative approach: shared traits that were not present in a common ancestor (and thus are not homologies in the traditional sense) may nonetheless derive from shared genetic and developmental mechanisms. Here, Fitch advances the hypothesis that such “deep homology” is characteristic of language evolution as it is for other traits (eye and limb evolution). To the extent that this hypothesis of “pervasive deep homology” is correct, molecular and developmental data from a very wide range of species, from ﬂies and worms to mice and birds, will oﬀer us a powerful and exciting new set of insights into the mechanisms underlying the workings of the human mind.
Part II: Variation
Lyle Jenkins observes that biolinguistics studies language as a biological system and, as such, investigates language in the traditional areas of biology: form– function, development, and evolution. The biolinguistic perspective concerns itself with the interplay between what Chomsky has termed the “three factors” in language design: genetic endowment, experience, and principles not speciﬁc to the faculty of language. Taking into consideration properties of relations that have been shown to be central in diﬀerent ﬁelds, including physics and linguistics, Jenkins discusses the role of concepts such as symmetry in the language design. Such concepts, Jenkins argues, are non-domain speciﬁc and non-species speciﬁc, but can contribute to the variation in biological systems seen in development and evolution. Lyle Jenkins’s chapter is related to Anna Maria Di Sciullo’s, which argues that concepts such as anti-symmetry and asymmetry contribute to an explanatory account of variation and change in language, as they do in biology.
Charles Yang presents a further step toward the incorporation of methods of evolutionary genetics into the study of language acquisition and change. The author explores how, as a result of learning, syntactic variations may originate from and spread within a speciﬁc set of lexical items, and these variations then cause further changes in the global properties of the grammar. He shows that the mathematical models of genetic drift can be used to capture the stochastic aspects of linguistic transmission, with implications in the study of linguistic typology as well. Yang’s chapter is related to Chomsky and Berwick’s with respect to the relation between language variation and language acquisition, as well as to Guardiano and Longobardi’s chapter on the relation between parametric syntax and phylogenetic variation. It is also related to Cedric Boeckx’s chapter, which relates parameter settings, learning, and optimization.
In Chapter 10, Cedric Boeckx outlines a new theoretical framework to understand the nature and limits of language variation. Although it is often

12 Anna Maria Di Sciullo and Cedric Boeckx
said that minimalism grew out of the perceived success of the principlesand-parameters approach at solving “Plato’s problem,” the classical notion of parameter, with its attendant “hierarchy,” does not ﬁt well with the attempt to approach Universal Grammar from below. Boeckx claims that parameters can aﬀect only the morpho-phonological expressions, and that the FLN is therefore non-parametrized/truly universal, as one might expect if much of it is the result of “third factor” eﬀects. Furthermore, the way language develops must be taken into consideration, and Boeckx proposes a Superset Bias, which amounts to saying that the child entertains a certain setting for parameters that minimizes the learning required, and only resets the value of the parameters involved if there is too much contrary evidence from the linguistic input. If correct, the Superset Bias suggests that recurrent patterns of variation show signs of optimization.
Rita Manzini and Leonardo Savoia analyze variation in auxiliary choice in the present perfect of Romance and Albanian, arguing that it reduces to the lexical, selectional, properties of have and be—and ultimately to the interaction of primitive notions of person split, transitivity, and voice. They characterize the person split in terms of event- vs. discourse-anchoring, while we take it that transitivity involves the n-adicity of the predicate—the basic split being between monadic and polyadic. The authors conclude that the uniﬁcation of reﬂexives, anticausatives, passives, and impersonals in the middlepassive voice (such as Italian si) cannot be achieved on the basis of movement from object to subject position, as in the classical generative account of passives, proposing instead that voice distinctions require reference to notions of open/generic variables in the argument structure. This amounts to upholding the minimalist hypothesis of lexical parametrization. Manzini and Savoia also suggest that variation in the so-called functional lexicon is not distinguishable from variation in the substantive lexicon. This supports the hypothesis presented in Richard Kayne’s chapter that the distinction between functional and lexical categories is not part of the primitives of the language faculty.
In Chapter 12, Giuseppe Longobardi and Cristina Guardiano state that in addition to its theoretical impact, the development of molecular biology has brought about the possibility of extraordinary historical progress in the study of phylogenetic classiﬁcation of diﬀerent species and human populations. The authors argue that parametric analyses of grammatical diversity in theoretical linguistics can prompt analogous progress in the historical classiﬁcation of language families, by showing that abstract syntactic properties are reliable indicators of phylogenetic relations. The pursuit of this approach radically questions the traditional belief in the orthogonality of grammatical typology and language genealogy and ultimately contributes to establishing formal

Introduction 13
grammar as a population science and historical linguistics as an important part of cognitive inquiry.
Anna Maria Di Sciullo’s chapter relates to Richard Kayne’s regarding the idea that parametric variation is a property of unvalued features. It is also related to Cedric Boeckx’s chapter with respect to the role of third factors in variation and to the view that the FLN is immune to variation. Furthermore, it also echoes Longobardi and Guardiano’s and Berwick and Chomsky’s chapters on the role of parameters in language and in biology. Di Sciullo formulates new links between variation in language and variation in biology, focusing on asymmetry. She starts by relating the dynamics of Homeobox (HOX) genes to the dynamics of parameters, and suggests that they too have an asymmetric if–then logic. She then relates the directionality of language change to antisymmetry breaking. She suggests that anti-symmetry breaking might be part of language evolution as well as it is part of the evolution of body plans. Finally she asks the question why asymmetry should be a core concept of the language faculty.
Part III: Computation
Richard Kayne starts by assuming that recursion is unique to the FLN, asks the question why the FLN is not available to other species, and lists several properties of the FLN that could be absent in other species. He focuses on the role of the antisymmetry property (Kayne 1994) as a diﬀerentiating property. He points out that if antisymmetry is correct, and if there is no mirrorimage language of English, the question arises whether this is a possibility in a given species of bird, for example. He then discusses issues related to the notion of optionality of projection in the case of head–head merger, and proposes a solution in terms of singleton-set formation and antisymmetry/antioptionality, which he suggests could be a property of the FLN. Kayne proposes to derive what we think of as the open–closed-class items distinction, the noun–verb distinction, as well as other properties of nouns from the antisymmetry-driven approach to the lexicon.
Howard Lasnik’s chapter asks what kind of computing device the FLN is. He reconsiders the question of the generative power of the syntactic system associated with the FLN. To generate such common linguistic expressions as those involving agreement among certain constituents at least the power of contextfree procedures is needed over the less powerful ﬁnite-state Markov model (an argument used by Chomsky 1957). This is because context-free grammars introduce structures that are not visible on the surface or linearized linguistic expressions. Lasnik points out that there are areas of syntax that appear to

14 Anna Maria Di Sciullo and Cedric Boeckx
involve a ﬂattened structure. Such structures were used as motivation for the move into transformational grammar. Lasnik expresses uncertainty as to whether transformations can solve the problem of coordination and wonders if the lower power of Markov procedures for such cases should be retained. Arguing that Markovian vs. non-Markovian properties have been discussed with respect to transformational derivations too, Lasnik notices that there have been early and recent arguments for globality in derivations and discusses the implications of some such cases.
Richard Larson discusses the computation of propositions and their interpretation by the psychological faculty. Larson observes that the minimalist view raises the clear expectation that one will ﬁnd signiﬁcant properties of linguistic representations, and perhaps of the architecture itself, that can be traced back to the articulatory–perceptual or the conceptual–intentional systems. Larson suggests that semantic intensionality and its representation in syntax are a promising place to seek such properties. More precisely, he argues for the following three points. First, that natural language seems to project the semantic property of intensionality uniquely into the syntactic domain of clausal complement, although this can be concealed by grammatical phenomena to some extent. Second, that children’s mastery of intensionality appears to be crucially tied to mastery of clausal complementation, which also correlates with the development of the child’s theory of mind. Third, that the correlation between intensionality and clausal complementation (“sententialism”) plausibly reﬂects an interface constraint: roughly, that the conceptual– intentional system inputs propositions derived by the language faculty only when these are presented in appropriate format. These results suggest an attractive view of the syntactic notion of “phase” rather diﬀerent from the current one, which adverts to grammar-extrinsic notions like memory and processing load. Phases can be seen as the point where the language faculty computes propositions for the psychological faculty.
Alessandra Giorgi’s chapter focuses on the computation of temporal dependencies, and thus is related to the other chapters in this section. It also considers temporal dependencies across languages from the point of view of Universal Grammar, and thus her chapter is related to the chapters on variation, including Rita Manzini and Leonardo Savoia’s chapter on variation in auxiliary choice. Giorgi argues that temporal relations are usually implemented in the various languages (mostly) by means of verbal morpho-syntax. Temporal, aspectual and modal morphemes codify the possible relations between events in main and embedded clauses. However, whereas on the one hand the inventory of the possible interpretive relations is universal— as well as relatively limited—on the other, verbal morpho-syntax varies across

Introduction 15
languages. On the other hand, the ability of expressing a preceding relation— that is, past or future—between events does not signiﬁcantly diﬀer among languages, and in a way, this relation is not expected to vary. The presence of considerable morpho-syntactic diﬀerences is therefore rather puzzling. The important issue is to reduce language variability—by means of an appropriate level of abstraction—in order to predict and explain the rise of the various language types, by identifying the relevant dimensions and their universal properties.
Like Richard Larson, Wolfram Hinzen addresses questions related to the computation of semantics in Chapter 18. He notes that minimalist inquiries often assume that semantics comes for free: semantics or “thought” is what so-called conceptual–intentional systems incorporate, and the primary task is that of explaining the emergence of a syntactic system that interfaces with these pre-given systems, so as to express the relevant “thoughts.” Taking that as a basis, various syntactic constraints are rationalized as answering various “conditions imposed” by the conceptual–intentional systems, in optimal ways. On this model, internal computational complexity matches some speciﬁc task domain (composing predicates, taking arguments, thematic relations, “discourse,” etc.). In this chapter, Hinzen pursues an alternative which he argues to be both conceptually superior and more in line with plausible principles of economy and conservativity in the building of adaptive complexity in biological evolution. In particular, he argues that syntactic structure should be regarded as radically underspeciﬁed with regard to the semantic task performed; domain-general principles of organizing information economically lend themselves to semantic uses, they engender semantic consequences.
In Chapter 19, Carlo Cecchetto and Costanza Papagno consider the questions related to processing of syntax by the brain, which are also discussed in Richard Larson’s chapter, albeit from a diﬀerent perspective. Cecchetto and Papagno start with the premise that working memory (WM) is involved in language comprehension, since this requires the temporary storage of the linguistic information. The most established model of WM is Baddeley and Hitch’s. A crucial question is whether natural-language processing relies on one component of this model, namely the Phonological Loop (or Phonological Short-Term Memory, STM). Although the prevailing answer in the literature is negative, after reviewing the available evidence and reporting ongoing research, Cecchetto and Papagno claim that the Phonological Loop plays an important role in language processing. Finally, they discuss possible consequences for theoretical models of linguistic competence and for brain activation studies.

16 Anna Maria Di Sciullo and Cedric Boeckx
In the last chapter, Robert Berwick discusses theoretical issues on the computation of linguistic expressions within the Minimalist Program. He states that the basic question underlying the Strong Minimalist Thesis is how little can be attributed to Universal Grammar while still accounting for the variety of I-languages attained, relying on third-factor principles. The author notes that it has recently been argued that interface conditions at the conceptual– intentional side may largely ﬁx the design properties of Univeral Grammar. Thus, his chapter relates to Cedric Boeckx’s and Wolfram Hinzen’s on the underspeciﬁcation of the FLN. Berwick shows that these design properties also dovetail nearly perfectly with constraints on the sensory–motor side, with, for example, the no-tampering condition, Edge labels, binary Merge, and the like all meshing with a computational model that imposes the minimal possible cost for sensory–motor system. In this restricted sense, then, the entire system, from sensory–motor through to conceptual–intentional, is optimal.
The issues related to the mergence of language evolution and variation are central in the biolinguistic enterprise, which seeks to understand why certain principles and not others are part of the language faculty, why variation and change as well as language acquisition have certain formal properties and not others. Such questions can be raised only when questions on the form of computation have been addressed and at least partially resolved. We hope that this book will strengthen the import of biolinguistics and the Minimalist Program for the understanding of the properties of the human speciﬁc faculty of language and will foster research in this exciting interdisciplinary ﬁeld.
Acknowledgments
We would like to thank Calixto Aguero-Bautista, who was part of the organization of the Biolinguistic Conference in Santo Domingo, which was sponsored by the Government of the Dominican Republic. We also wish to thank Guglielmo Cinque, who participated in the organization of the Biolinguistic Conference at the University of Venice. This conference was sponsored by the Social Sciences and Humanities Research Council of Canada, whose aid to research we gratefully acknowledge: a Major Collaborative Research Initiative project directed by Anna Maria Di Sciullo on Interface asymmetries and cognitive processing (SSHRCC #412-2003-1003). Finally we are grateful to the students that helped us at diﬀerent stages of the editing of this book, Paul John, Calin Batori and Stanca Somesfalean at the Université du Québec à Montréal, Adriana Fasanella and Carlos Rubio at the Universitat Autònoma de Barcelona.

Part I Evolution

This page intentionally left blank

2
The Biolinguistic Program: The Current State of its Development
ROBERT C. BERWICK AND NOAM CHOMSKY
Before discussing language, particularly in a biological context, we should be clear about what we mean by the term, which has engendered much confusion. Sometimes the term “language” is used to refer to human language; sometimes it is used to refer to any symbolic system or mode of communication or representation, as when one speaks of the language of the bees, or programming languages, or the language of the stars, and so on. Here we will keep to the ﬁrst sense: human language, a particular object of the biological world. The study of language, so understood, has come to be called the biolinguistic perspective.
Among the many puzzling questions about language, two are salient: First, why are there any languages at all, evidently unique to the human lineage, what evolutionary biologists call an “autapomorphy”? Second, why are there so many languages? These are in fact the basic questions of origin and variation that so occupied Darwin and other evolutionary thinkers and comprise modern biology’s explanatory core: why do we observe this particular array of living forms in the world and not others? From this standpoint, linguistic science stands squarely within the modern biological tradition, despite its seemingly abstract details, as has often been observed.
According to a fairly general consensus among paleoanthropologists and archaeologists, these questions are very recent ones in evolutionary time. Roughly 100,000 years ago, the ﬁrst question did not arise, because there were no languages. About 50,000 years ago, the answers to both questions were settled: our ancestors began their trek from Africa, spreading over the entire world, and as far as is known, the language faculty has remained essentially unchanged—which is not surprising in such a brief period. An infant from a Stone Age tribe in the Amazon, if brought to Boston, will be indistinguishable in linguistic and other cognitive functions from children born in Boston who

20 Robert Berwick and Noam Chomsky
trace their ancestry to the ﬁrst English colonists; and conversely. The actual dates are uncertain, and do not matter much for our purposes. The general picture appears to be roughly accurate.
We are therefore concerned with a curious biological object, language, which has appeared on earth quite recently.It is a species property of humans, a common endowment with no signiﬁcant variation apart from serious pathology, unlike anything else known in the organic world in its essentials, and surely central to human life since its emergence. It is a central component of what the co-founder of modern evolutionary theory, Alfred Russell Wallace, called “man’s intellectual and moral nature”: the human capacities for creative imagination, language and symbolism generally, recording and interpretation of natural phenomena, intricate social practices and the like, a complex that is sometimes simply called “the human capacity.” This complex seems to have crystallized fairly recently among a small group in East Africa of whom we are all descendants, distinguishing contemporary humans sharply from other animals, with enormous consequences for the whole of the biological world. It is commonly and plausibly assumed that the emergence of language was a core element in this sudden and dramatic transformation. Furthermore, language is one component of the human capacity that is accessible to study in some depth. That is another reason why even research that is purely linguistic in character actually falls under the heading of biolinguistics despite its superﬁcial remove from biology, as exempliﬁed in the chapters by Lasnik and Larson in this volume.
From the biolinguistic perspective, we can think of language as, in essence, an “organ of the body,” more or less on a par with the visual or digestive or immune systems. Like others, it is a subcomponent of a complex organism that has suﬃcient internal integrity so that it makes sense to study it in abstraction from its complex interactions with other systems in the life of the organism. In this case it is a cognitive organ, like the systems of planning, interpretation, reﬂection, and whatever else falls among those aspects of the world loosely “termed mental,” which reduce somehow to “the organical structure of the brain,” in the words of the eighteenth-century scientist and philosopher Joseph Priestley. He was articulating the natural conclusion after Newton had demonstrated, to Newton’s own great dismay and disbelief, that the world is not a machine, contrary to the core assumptions of the seventeenth-century scientiﬁc revolution—a conclusion that in eﬀect eliminated the traditional mind–body problem, because there is no longer a coherent concept of body (matter, physical), a matter well understood in the eighteenth and nineteenth centuries. We can think of language as a mental organ, where the term “mental” simply refers to certain aspects of the world, to be studied in the same way

The Biolinguistic Program: The Current State of its Development 21
as chemical, optical, electrical, and other aspects, with the hope for eventual uniﬁcation—noting that such uniﬁcation in these other domains in the past was often achieved in completely unexpected ways, not necessarily by reduction.
As mentioned at the outset with regard to the curious mental organ language, two obvious questions arise. One is: Why does it exist at all, evidently unique to our species? Second: Why is there more than one language? In fact, why is there such a multitude and variety that languages appear to “diﬀer from each other without limit and in unpredictable ways” and therefore the study of each language must be approached “without any preexistent scheme of what a language must be,” here quoting the formulation of the prominent theoretical linguist Martin Joos 50 years ago, summarizing the reigning “Boasian tradition,” as he plausibly called it, tracing it to the work of one of the founders of modern anthropology and anthropological linguistics, Franz Boas. The publication that was the foundation of American structural linguistics in the 1950s, Zellig Harris’s Methods in Structural Linguistics (1951), was called “methods” because there seemed to be little to say about language beyond the methods for reducing the data from limitlessly varying languages to organized form. European structuralism was much the same. Nikolai Trubetzkoy’s classic introduction to phonological analysis was similar in conception. More generally, structuralist inquiries focused almost entirely on phonology and morphology, the areas in which languages do appear to diﬀer widely and in complex ways, a matter of broader interest, to which we will return.1
The dominant picture in general biology at about the same time was rather similar, captured in molecular biologist Gunther Stent’s observation that the variability of organisms is so free as to constitute “a near inﬁnitude of particulars which have to be sorted out case by case” (as quoted in Carroll, 2005a: 24).
In fact the problem of reconciling unity and diversity has constantly arisen in general biology as well as in linguistics. The study of language that developed within the seventeenth-century scientiﬁc revolution distinguished universal from particular grammar, though not quite in the sense of the contemporary biolinguistic approach. Universal grammar was taken to be the intellectual core of the discipline; particular grammars were regarded as accidental instantiations of the universal system. With the ﬂourishing of anthropological linguistics, the pendulum swung in the other direction, towards diversity, well articulated in the Boasian formulation we quoted. In general
1 See Joos (1957); Trubetzkoy (1939, trans. 1969).

22 Robert Berwick and Noam Chomsky
biology, the issue had been raised sharply in a famous debate between the naturalists Georges Cuvier and Geoﬀroy St. Hilaire in 1830. Cuvier’s position, emphasizing diversity, prevailed, particularly after the Darwinian revolution, leading to the conclusions about the near inﬁnitude of variety that have to be sorted out case by case. Perhaps the most quoted sentence in biology is Darwin’s ﬁnal observation in Origin of Species about how “from so simple a beginning, endless forms most beautiful and most wonderful have been, and are being, evolved.” It is unclear if the irony was intended, but these words were taken by evolutionary biologist Sean Carroll as the title of his introduction to “the new science of evo-devo,” which seeks to show that the forms that have evolved are far from endless, in fact are remarkably uniform.
Reconciliation of the apparent diversity of organic forms with their evident underlying uniformity—why do we see this array of living things in the world and not others, just as why do we see this array of languages/grammars and not others?—comes about through the interplay of three factors, famously articulated by the biologist Monod in his book Le Hasard et la Nécessité: (1970; Chance and Necessity, 1972). First, there is the historically contingent fact that we are all common descendants from a single tree of life, and so share common ancestry with all other living things, which apparently have explored only a minute fraction of a space that includes a much larger set of possible biological outcomes. It should by now be no surprise that we therefore possess common genes, biochemical pathways, and much else.
Second, there are the physio-chemical constraints of the world, necessities that delimit biological possibilities, like the near-impossibility of wheels for locomotion due to the physical diﬃculty of providing a nerve control and a blood supply to a rotating object. Third, there is the sieving eﬀect of natural selection, which winnows out from a pre-existing menu of possibilities— oﬀered by historical contingency and physio-chemical constraints—the actual array of organisms that we observe in the world around us. Note that the eﬀect of the constrained menu of options is of utmost importance; if the options are extremely constrained, then selection would have very little to choose from: it should be no surprise that when one goes to a fast-food restaurant one is usually seen leaving with a hamburger and french fries. Thus, just as Darwin would have it, natural selection is by no means the “exclusive” means that has shaped the natural world: “Furthermore, I am convinced that Natural Selection has been the main but not exclusive means of modiﬁcation” (Darwin 1859: 7).
Recent discoveries have reinvigorated the general approach of D’Arcy Thompson (1992) and Alan Turing on principles that constrain the variety of organisms. In Turing and Wardlaw’s words, the true science of biology

The Biolinguistic Program: The Current State of its Development 23
should regard each “living organism as a special kind of system to which the general laws of physics and chemistry apply,” sharply constraining their possible variety and ﬁxing their fundamental properties (Turing and Wardlaw 1953). That perspective may sound less extreme today after the discovery of master genes, deep homologies and conservation, and much else, perhaps even restrictions of evolutionary–developmental processes so narrow that “replaying the protein tape of life might be surprisingly repetitive,” quoting a report by Weinreich et al. (2006) on feasible mutational paths, reinterpreting a famous image of Steven Gould’s, who had suggested that the tape of life, if replayed, might follow a variety of paths. As Michael Lynch further notes (2007: 367), “we have known for decades that all eukaryotes share most of the same genes for transcription, translation, replication, nutrient uptake, core metabolism, cytoskeletal structure, and so forth. Why would we expect anything diﬀerent for development?”
In a recent review of the evo–devo approach, Gerd Müller (2007) notes how much more concrete our understanding of the Turing-type patterning models have become, observing that several
Generic forms . . . result from the interaction of basic cell properties with diﬀerent pattern-forming mechanisms. Diﬀerential adhesion and cell polarity when modulated by diﬀerent kinds of physical and chemical patterning mechanisms . . . lead to standard organizational motifs . . . . diﬀerential adhesion properties and their polar distribution on cell surfaces lead to hollow spheres when combined with a diﬀusion gradient, and to invaginated spheres when combined with a sedimentation gradient. . . . The combination of diﬀerential adhesion with a reaction-diﬀusion mechanism generates radially periodic structures, whereas a combination with chemical oscillation results in serially periodic structures. Early metazoan body plans represent an exploitation of such generic patterning repertoires. (Müller 2007: 947)
For example, the contingent fact that we have ﬁve ﬁngers and ﬁve toes may be better explained by an appeal to how toes and ﬁngers develop than that ﬁve is optimal for their function.2
Biochemist Michael Sherman (2007) argues, perhaps more controversially, that a “Universal Genome that encodes all major developmental programs essential for various phyla of Metazoa emerged in a unicellular or a primitive multicellular organism shortly before the Cambrian period” about 500 million years ago, when there was a sudden explosion of complex animal forms;
2 As Ahouse and Berwick (1998) note, “Five ﬁngers and toes were not the original number of digits in tetrapods (see the discussion by M. I. Coates and J. A. Clark in Nature 347, 1990, 66–9) and amphibians probably never had more than four digits (and generally have three) on their front and back feet. There is a clever explanation from molecular developmental genetics that rationalizes why there are at most ﬁve diﬀerent types of digits even if some are duplicated.”

24 Robert Berwick and Noam Chomsky
and, further, that the many “Metazoan phyla, all having similar genomes, are nonetheless so distinct because they utilize speciﬁc combinations of developmental programs.” On this view, there is but one multicellular animal from a suﬃciently abstract point of view—the point of view that might be taken by a Martian scientist from a much more advanced civilization viewing events on earth. Superﬁcial variety would result in part from various arrangements of an evolutionarily conserved “developmental–genetic toolkit,” as it is sometimes called. If ideas of this kind prove to be on the right track, the problem of unity and diversity will be reformulated in ways that would have surprised some recent generations of scientists. The degree to which the conserved toolkit is the sole explanation for the observed uniformity deserves some care. As mentioned, observed uniformity arises in part because there has simply not been enough time, and contingent ancestry-by-descent bars the possibility of exploring “too much” of the genetic–protein–morphological space— particularly given the virtual impossibility of “going backwards” and starting the search over again for greater success. Given these inherent constraints, it becomes much less of a surprise that organisms are all built according to a certain set of Baupläne, as Steven Gould has emphasized, among others. It is in this sense that if sophisticated Martian scientists came to earth, they would probably see in eﬀect just one organism, though with many apparent superﬁcial variations.
The uniformity had not passed unnoticed in Darwin’s day. The naturalistic studies of Darwin’s close associate and expositor Thomas Huxley led him to observe, with some puzzlement, that there appear to be “predetermined lines of modiﬁcation” that lead natural selection to “produce varieties of a limited number and kind” for each species (Maynard-Smith et al. 1985: 266). Indeed, the study of the sources and nature of possible variation constituted a large portion of Darwin’s own research program after Origin, as summarized in his Variation of Plants and Animals under Domestication (1868). Huxley’s conclusion is reminiscent of earlier ideas of “rational morphology,” a famous example being Goethe’s theories of archetypal forms of plants, which have been partially revived in the evo–devo revolution. Indeed, as indicated earlier, Darwin himself was sensitive to this issue, and, grand synthesizer that he was, dealt more carefully with such “laws of growth and form”: the constraints and opportunities to change are due to the details of development, chance associations with other features that may be strongly selected for or against, and ﬁnally selection on the trait itself. Darwin noted that such laws of “correlation and balance” would be of considerable importance to his theory, remarking, for example, that “white cats if they have blue eyes are almost invariably deaf ” (Darwin, 1856 letter to W. D. Fox).

The Biolinguistic Program: The Current State of its Development 25
When the evolutionary Modern Synthesis, pioneered by Fisher, Haldane, and Wright, held sway through most of the last half of the previous century, emphasis in evolution was focused on micro-mutational events and gradualism, singling out the power of natural selection operating via very small incremental steps. More recently, however, in general biology the pendulum has been swinging towards a combination of Monod’s three factors, yielding new ways of understanding traditional ideas.
Let us return to the ﬁrst of the two basic questions: Why should there be any languages at all, apparently an autapomorphy? As mentioned, very recently in evolutionary time the question would not have arisen: there were no languages. There were, of course, plenty of animal communication systems. But they are all radically diﬀerent from human language in structure and function. Human language does not even ﬁt within the standard typologies of animal communication systems—Marc Hauser’s, for example, in his comprehensive review of the evolution of communication (1996). It has been conventional to regard language as a system whose function is communication. This is indeed the widespread view invoked in most selectionist accounts of language, which almost invariably start from this interpretation. However, to the extent that the characterization has any meaning, this appears to be incorrect, for a variety of reasons to which we turn below.
The inference of a biological trait’s “purpose” or “function” from its surface form is always rife with diﬃculties. Lewontin’s remarks in The Triple Helix (2000: 79) illustrate how diﬃcult it can be to assign a unique function to an organ or trait even in the case of what at ﬁrst seems like a far simpler situation: bones do not have a single, unambiguous function. While it is true that bones support the body, allowing us to stand up and walk, they are also a storehouse for calcium and bone marrow for producing new red blood cells, so they are in a sense part of the circulatory system.
What is true for bones is also true in spades for human language. Moreover, there has always been an alternative tradition, expressed by Burling (1993), among others, that humans may well possess a secondary communication system like those of other primates, namely a nonverbal system of gestures or even calls, but that this is not language, since, as Burling notes, “our surviving primate communication system remains sharply distinct from language.”3
Language can of course be used for communication, as can any aspect of what we do: style of dress, gesture, and so on. And it can be and commonly is used for much else. Statistically speaking, for whatever that is worth, the
3 Laura Petitto’s work on the acquisition of sign language (1987) demonstrates Burling’s point rather dramatically—the same gesture is used for pointing and pronominal reference, but in the latter case the gesture is counter-iconic at the age when infants typically reverse “I” and “you.”

26 Robert Berwick and Noam Chomsky
overwhelming use of language is internal—for thought. It takes an enormous act of will to keep from talking to oneself in every waking moment—and asleep as well, often a considerable annoyance. The distinguished neurologist Harry Jerison (1973: 55) among others expressed a stronger view, holding that “language did not evolve as a communication system . . . the initial evolution of language is more likely to have been . . . for the construction of a real world,” as a “tool for thought.” Not only in the functional dimension, but also in all other respects—semantic, syntactic, morphological, and phonological— the core properties of human language appear to diﬀer sharply from animal communication systems, and to be largely unique in the organic world.
How, then, did this strange object appear in the biological record, apparently within a very narrow evolutionary window, perhaps about 50–100,000 years ago? There are of course no deﬁnite answers, but it is possible to sketch what seem to be some reasonable speculations, which relate closely to work of recent years in the biolinguistic framework.
Anatomically modern humans are found in the fossil record several hundred thousand years ago, but evidence of the human capacity is much more recent, not long before the trek from Africa. Paleoanthropologist Ian Tattersall reports that “a vocal tract capable of producing the sounds of articulate speech” existed over half a million years before there is any evidence that our ancestors were using language. “We have to conclude,” he writes, “that the appearance of language and its anatomical correlates was not driven by natural selection, however beneﬁcial these innovations may appear in hindsight”—a conclusion which raises no problems for standard evolutionary biology, contrary to illusions in popular literature (Tattersall 1998). It appears that human brain size reached its current level recently, perhaps about 100,000 years ago, which suggests to some specialists that “human language probably evolved, at least in part, as an automatic but adaptive consequence of increased absolute brain size” (neuroscientist Georg Striedter 2004). With regard to language, Tattersall concludes that “after a long—and poorly understood—period of erratic brain expansion and reorganization in the human lineage, something occurred that set the stage for language acquisition. This innovation would have depended on the phenomenon of emergence, whereby a chance combination of preexisting elements results in something totally unexpected,” presumably “a neural change . . . in some population of the human lineage . . . rather minor in genetic terms, [which] probably had nothing whatever to do with adaptation” though it conferred advantages, and then proliferated. Perhaps it was an automatic consequence of absolute brain size, as Striedter suggests, or perhaps some minor chance mutation. Sometime later—not very long in evolutionary time—came further innovations,

The Biolinguistic Program: The Current State of its Development 27
perhaps culturally driven, that led to behaviorally modern humans, the crystallization of the human capacity, and the trek from Africa (Tattersall 1998, 2002, 2005).
What was that neural change in some small group that was rather minor in genetic terms? To answer that, we have to consider the special properties of language. The most elementary property of our shared language capacity is that it enables us to construct and interpret a discrete inﬁnity of hierarchically structured expressions: discrete because there are ﬁve-word sentences and sixword sentences, but no ﬁve-and-a-half-word sentences; inﬁnite because there is no longest sentence. Language is therefore based on a recursive generative procedure that takes elementary word-like elements from some store, call it the lexicon, and applies repeatedly to yield structured expressions, without bound. To account for the emergence of the language faculty—hence for the existence of at least one language—we have to face two basic tasks. One task is to account for the “atoms of computation,” the lexical items—commonly in the range of 30–50,000. The second is to discover the computational properties of the language faculty. This task in turn has several facets: we must seek to discover the generative procedure that constructs inﬁnitely many expressions in the mind, and the methods by which these internal mental objects are related to two interfaces with language-external (but organism-internal) systems: the system of thought, on the one hand, and also to the sensorymotor system, thus externalizing internal computations and thought. This is one way of reformulating the traditional conception, at least back to Aristotle, that language is sound with a meaning. All of these tasks pose very serious problems, far more so than was believed in the recent past, or often today.
Let us turn then to the basic elements of language, beginning with the generative procedure, which, it seems, emerged some time in the 50,000– 100,000 year range, barely a ﬂick of an eye in evolutionary time, presumably involving some slight rewiring of the brain. At this point the evo–devo revolution in biology becomes relevant. It has provided compelling evidence for two relevant conclusions. One is that genetic endowment even for regulatory systems is deeply conserved. A second is that very slight changes can yield great diﬀerences in observed outcome—though phenotypic variation is nonetheless limited, in virtue of the deep conservation of genetic systems, and laws of nature of the kind that interested Thompson and Turing. To cite a simple illustration, there are two kinds of stickleback ﬁsh, with or without spiky spines on the pelvis. About 10,000 years ago, a mutation in a genetic “switch” near a gene involved in spine production diﬀerentiated the two varieties, one with spines and one without, one adapted to oceans and the other to lakes (Colosimo et al. 2004, 2005; Orr 2005a).

28 Robert Berwick and Noam Chomsky
Much more far-reaching results have to do with the evolution of eyes, an intensively studied topic. It turns out that there are very few types of eye, in part because of constraints imposed by the physics of light, in part because only one category of proteins, opsin molecules, can perform the necessary functions. The genes encoding opsin had very early origins, and are repeatedly recruited, but only in limited ways, again because of physical constraints. The same is true of eye lens proteins. The evolution of eyes illustrates the complex interactions of physical law, stochastic processes, and the role of selection in choosing within a narrow physical channel of possibilities (Gehring 2005).
Jacob and Monod’s work from 1961 on the discovery of the “operon” in E. coli for which they won the Nobel Prize, led to Monod’s famous quote (cited in Jacob 1988): “what is true for the colon bacillus [E. coli] is true for the elephant” (Jacob 1988: 290). While this has sometimes been interpreted as anticipating the modern evo–devo account, it seems that what Monod actually meant was that his and François Jacob’s generalized negative regulation theory should be suﬃcient to account for all cases of gene regulation. But this conjecture turned out to be incorrect. Eukaryotes (non-bacteria) do not (cannot) use the operon regulatory machinery that bacteria (Prokaryotes) use, because they do not have their genes lined up neatly in a linear fashion, strung out like beads along a string, without breaks and without intervening non-protein coding regions (introns). Roughly, it is this arrangement in Prokaryotes that permits the negative feedback operon system to work. Indeed, much of the modern evo–devo revolution is about the discovery of the rather more sophisticated methods for gene regulation and development employed by Eukaryotes. Nonetheless, Monod’s basic notion that slight diﬀerences in timing and arrangement of regulatory mechanisms that activate genes could result in enormous diﬀerences did turn out to be correct, though the machinery was unanticipated. It was left to Jacob (1977) to provide a suggestive model for the development of other organisms based on the notion that “thanks to complex regulatory circuits” what “accounts for the diﬀerence between a butterﬂy and a lion, a chicken and a ﬂy . . . are the result of mutations which altered the organism’s regulatory circuits more than its chemical structure” (1977: 26). Jacob’s model in turn provided part of the inspiration for the Principles and Parameters (P&P) approach to language, a matter discussed in lectures shortly after (Chomsky 1980: 67).
The P&P approach is based on the assumption that languages consist of ﬁxed and invariant principles connected to a kind of switchbox of parameters, questions that the child has to answer on the basis of presented data in order to ﬁx a language from the limited variety available in principle—or perhaps,

The Biolinguistic Program: The Current State of its Development 29
as Charles Yang has argued (2002), to determine a probability distribution over languages resulting from a learning procedure for parameter setting. For example, the child has to determine whether the language to which it is exposed is head-initial, like English, a language in which substantive elements precede their objects, as in read the book or in the room; or whether it is headﬁnal, like Japanese, where the counterparts would be book read and room in. As in the somewhat analogous case of rearrangement of regulatory mechanisms, the approach suggests a framework for understanding how essential unity might yield the appearance of the limitless diversity that was assumed not long ago for language (as for biological organisms generally).
The P&P research program has been very fruitful, yielding rich new understanding of a very broad typological range of languages, opening new questions that had never been considered, sometimes providing answers. It is no exaggeration to say that more has been learned about languages in the past twenty-ﬁve years than in the earlier millennia of serious inquiry into language. With regard to the two salient questions with which we began, the approach suggests that what emerged, fairly suddenly in evolutionary terms, was the generative procedure that provides the principles, and that diversity of language results from the fact that the principles do not determine the answers to all questions about language, but leave some questions as open parameters. Notice that the single illustration above has to do with ordering. Though the matter is contested, it seems that there is by now substantial linguistic evidence that ordering is restricted to externalization of internal computation to the sensory-motor system, and plays no role in core syntax and semantics, a conclusion for which there is also accumulating biological evidence of a sort familiar to mainstream biologists, to which we return below.
The simplest assumption, hence the one we adopt unless counter-evidence appears, is that the generative procedure emerged suddenly as the result of a minor mutation. In that case we would expect the generative procedure to be very simple. Various kinds of generative procedures have been explored in the past 50 years. One approach familiar to linguists and computer scientists is phrase-structure grammar, developed in the 1950s and since extensively employed. The approach made sense at the time. It ﬁt very naturally into one of the several equivalent formulations of the mathematical theory of recursive procedures—Emil Post’s rewriting systems—and it captured at least some basic properties of language, such as hierarchic structure and embedding. Nevertheless, it was quickly recognized that phrase-structure grammar is not only inadequate for language but is also quite a complex procedure with many arbitrary stipulations, not the kind of system we would hope to ﬁnd, and unlikely to have emerged suddenly.

30 Robert Berwick and Noam Chomsky
Over the years, research has found ways to reduce the complexities of these systems, and ﬁnally to eliminate them entirely in favor of the simplest possible mode of recursive generation: an operation that takes two objects already constructed, call them X and Y , and forms from them a new object that consists of the two unchanged, hence simply the set with X and Y as members. Call this operation Merge. Provided with conceptual atoms of the lexicon, the operation Merge, iterated without bound, yields an inﬁnity of hierarchically constructed expressions. If these can be interpreted by conceptual systems, the operation provides an internal language of thought.
A very strong thesis, called the “strong minimalist thesis,” is that the generative process is optimal: the principles of language are determined by eﬃcient computation and language keeps to the simplest recursive operation, Merge, designed to satisfy interface conditions in accord with independent principles of eﬃcient computation. Language is something like a snowﬂake, assuming its particular form by virtue of laws of nature—in this case principles of computational eﬃciency—once the basic mode of construction is available, and satisfying whatever conditions are imposed at the interfaces. The basic thesis is expressed in the title of a recent collection of technical essays: “Interfaces + Recursion = Language?” (Sauerland and Gärtner 2007). Optimally, recursion can be reduced to Merge. The question mark in the title is of course highly appropriate: the questions arise at the border of current research. We will suggest below that there is a signiﬁcant asymmetry between the two interfaces, with the semantic–pragmatic interface—the link to systems of thought and action—having primacy. Just how rich these external conditions may be is also a serious research question, and a hard one, given the lack of much evidence about these thought–action systems that is independent of language. A very strong thesis, suggested by Wolfram Hinzen (2006) is that central components of thought, such as propositions, are basically derived from the optimally constructed generative procedure. If such ideas can be sharpened and validated, then the eﬀect of the semantic–pragmatic interface on language design would be correspondingly reduced.
The strong minimalist thesis is very far from established, but it looks much more plausible than it did only a few years ago. Insofar as it is correct, the evolution of language will reduce to the emergence of the operation Merge, the evolution of conceptual atoms of the lexicon, the linkage to conceptual systems, and the mode of externalization. Any residue of principles of language not reducible to Merge and optimal computation will have to be accounted for by some other evolutionary process—one that we are unlikely to learn much about, at least by presently understood methods, as Lewontin (1998) notes.

The Biolinguistic Program: The Current State of its Development 31
Notice that there is no room in this picture for any precursors to language— say a language-like system with only short sentences. There is no rationale for postulation of such a system: to go from seven-word sentences to the discrete inﬁnity of human language requires emergence of the same recursive procedure as to go from zero to inﬁnity, and there is of course no direct evidence for such protolanguages. Similar observations hold for language acquisition, despite appearances, a matter that we put to the side here.
Crucially, the operation Merge yields the familiar displacement property of language: the fact that we pronounce phrases in one position, but interpret them somewhere else as well. Thus in the sentence Guess what John is eating, we understand what to be the object of eat, as in John is eating an apple, even though it is pronounced somewhere else. This property has always seemed paradoxical, a kind of imperfection of language. It is by no means necessary in order to capture semantic facts, but it is ubiquitous. It surpasses the capacity of phrase structure grammars, requiring that they be still further complicated with additional devices. But it falls within Merge, automatically. To see how, suppose that the operation Merge has constructed the mental expression corresponding to John is eating what. A larger expression can be constructed by Merge in two ways: Internal Merge can add something from within the expression, so as to form what John is eating what; and External Merge can add something new, yielding Guess what John is eating what.
That carries us part of the way towards displacement. In what John is eating what, the phrase what appears in two positions, and in fact those two positions are required for semantic interpretation: the original position provides the information that what is understood to be the direct object of eat, and the new position, at the edge, is interpreted as a quantiﬁer ranging over a variable, so that the expression means something like “for which thing x, John is eating the thing x.”
These observations generalize to a wide range of constructions. The results are just what is needed for semantic interpretation, but they do not yield the objects that are pronounced in English. We do not pronounce guess what John is eating what, but rather guess what John is eating, with the original position suppressed. That is a universal property of displacement, with minor (and interesting) qualiﬁcations that we can ignore here. The property follows from elementary principles of computational eﬃciency. In fact, it has often been noted that serial motor activity is computationally costly, a matter attested by the sheer quantity of motor cortex devoted to both motor control of the hands and for oro-facial articulatory gestures.
To externalize the internally generated expression what John is eating what, it would be necessary to pronounce what twice, and that turns out to place

32 Robert Berwick and Noam Chomsky
a very considerable burden on computation, when we consider expressions of normal complexity and the actual nature of displacement by Internal Merge. With all but one of the occurrences of what suppressed, the computational burden is greatly eased. The one occurrence that must be pronounced is the most prominent one, the last one created by Internal Merge: otherwise there will be no indication that the operation has applied to yield the correct interpretation. It appears, then, that the language faculty recruits a general principle of computational eﬃciency for the process of externalization.
The suppression of all but one of the occurrences of the displaced element is computationally eﬃcient, but imposes a signiﬁcant burden on interpretation, hence on communication. The person hearing the sentence has to discover the position of the gap where the displaced element is to be interpreted. That is a highly non-trivial problem in general, familiar from parsing programs. There is, then, a conﬂict between computational eﬃciency and interpretive– communicative eﬃciency. Universally, languages resolve the conﬂict in favor of computational eﬃciency. These facts at once suggest that language evolved as an instrument of internal thought, with externalization a secondary process. There is a great deal of evidence from language design that yields similar conclusions; so called “island properties,” for example.
There are independent reasons for the conclusion that externalization is a secondary process. One is that externalization appears to be modalityindependent, as has been learned from studies of sign language in recent years. The structural properties of sign and spoken language are remarkably similar. Additionally, acquisition follows the same course in both, and neural localization seems to be similar as well. That tends to reinforce the conclusion that language is optimized for the system of thought, with mode of externalization secondary.
Note further that the constraints on externalization holding for the auditory modality also appear to hold in the case of the visual modality in signed languages. Even though there is no physical constraint barring one from “saying” with one hand that John likes ice-cream and Mary likes beer with the other hand, nevertheless it appears that one hand is dominant throughout and delivers sentences (via gestures) in a left-to-right order in time, linearized as in vocal-tract externalization, while the non-dominant hand adds markings for emphasis, morphology, and the like.
Indeed, it seems possible to make a far stronger statement: all recent relevant biological and evolutionary research leads to the conclusion that the process of externalization is secondary. This includes the recent and highly publicized discoveries of genetic elements putatively involved in language,

The Biolinguistic Program: The Current State of its Development 33
speciﬁcally, the FOXP2 regulatory (transcription factor) gene. FOXP2 is implicated in a highly heritable language defect, so-called verbal dyspraxia. Since this discovery it has been intensely analyzed from an evolutionary and comparative standpoint, with small amino-acid diﬀerences between the human variant and other primates and non-human mammals posited as the target of recent positive natural selection, perhaps concomitant with language emergence (Fisher et al. 1998; Enard et al. 2002); and with similarities between those same two amino acids in humans and Neandertals also suggested as possibly signiﬁcant with respect to language (Krause, Lalueza-Fox, et al. 2007; Science Daily, 21 Oct. 2007).
However, we might ask whether this gene is centrally involved in language or, as now seems more plausible, is part of the secondary externalization process. Recent discoveries in birds and mice over the past few years point to an “emerging consensus” that this transcription factor gene is not so much part of a blueprint for internal syntax, the narrow faculty of language, and most certainly not some hypothetical “language gene” (just as there are no single genes for eye color or autism) but rather part of regulatory machinery related to externalization (Vargha-Khadem et al. 2005; Groszer et al. 2008). FOXP2 aids in the development of serial ﬁne motor control, orofacial or otherwise: the ability to literally put one sound or gesture down in place, one point after another in time.
In this respect it is worth noting that members of the KE family in which this genetic defect was originally isolated exhibit a quite general motor dyspraxia, not localized to simply their oro-facial movements. Recent studies where a mutated FOXP2 gene built to replicate the defects found in the KE family was inserted in mice conﬁrm this view: “We ﬁnd that Foxp2R552H heterozygous mice display subtle but highly signiﬁcant deﬁcits in learning of rapid motor skills . . . These data are consistent with proposals that human speech faculties recruit evolutionarily ancient neural circuits involved in motor learning” (Groszer et al. 2008: 359).
If this view is on the right track, then FOXP2 is more akin to the blueprint that aids in the construction of a properly functioning input–output system for a computer, like its printer, rather than the construction of the computer’s central processor itself. From this point of view, what has gone wrong in the aﬀected KE family members is thus something awry with the externalization system, the “printer,” not the central language faculty itself. If this is so, then the evolutionary analyses suggesting that this transcription factor was under positive selection approximately 100,000 years ago (in itself arguable) could in fact be quite inconclusive about the evolution of the core components of the faculty of language, syntax, and the mapping to the

34 Robert Berwick and Noam Chomsky
“semantic” (conceptual–intensional) interface. It is diﬃcult to determine the causal sequence: the link between FOXP2 and high-grade serial motor coordination could be regarded as either an opportunistic prerequisite substrate for externalization, no matter what the modality, as is common in evolutionary scenarios, or the result of selection pressure for eﬃcient externalization solutions after Merge arose. In either case, FOXP2 becomes part of a system extrinsic to core syntax/semantics.
There is further recent evidence from Michael Coen (p.c.) regarding serial coordination in vocalization suggesting that discretized serial motor control might simply be a substrate common to all mammals, and possibly all vertebrates. If so, then the entire FOXP2 story, and motor externalization generally, is even further removed from the picture of core syntax/semantics evolution. The evidence comes from the ﬁnding that all mammals tested (people, dogs, cats, seals, whales, baboons, tamarin monkeys, mice) and unrelated vertebrates (crows, ﬁnches, frogs, etc.) possess what was formerly attributed just to the human externalization system: each of the vocal repertoires of these various species is drawn from a ﬁnite set of distinctive phonemes (or, more accurately, songemes in the case of birds, barkemes in the case of dogs, etc.). Coen’s hypothesis is that each species has some ﬁnite number of articulatory productions, for example, phonemes, that are genetically constrained by its physiology, according to principles such as minimization of energy during vocalization, physical constraints, and the like. This is similar to Kenneth Stevens’s picture of the quantal nature of speech production (Stevens 1972, 1989).
On this view, any given species uses a subset of species-speciﬁc primitive sounds to generate the vocalizations common to that species. (It would not be expected that each animal uses all of them, in the same way that no human employs all phonemes.) If so, then our hypothetical Martian would conclude that even at the level of peripheral externalization, there is one human language, one dog language, one frog language, and the like.
Summarizing, FOXP2 does not speak to the question of the core faculty of human language because it really has nothing to do with the core language phenotype, Merge and syntax. From an explanatory point of view, this makes it quite unlike the case of, say, sickle-cell anemia where a genetic defect directly leads to the aberrant trait, the formation of an abnormal haemoglobin protein and resulting red blood cell distortion. To be sure, FOXP2 remains a possibly necessary component of the language system in the same way that a printer is part of a computer system. But it is not human language tout court. If all this is so, then the explanation “for” the core language phenotype

The Biolinguistic Program: The Current State of its Development 35
may be even more indirect and diﬃcult than Richard Lewontin (1998) has sketched.4
In fact, in many respects this focus on FOXP2 and dyspraxia is quite similar to the near-universal focus on language as communication.5 Both eﬀorts examine properties apparently particular only to the externalization process, which, we conjecture, is not part of the core faculty of human language. In this sense both eﬀorts are misdirected, unrevealing of the internal computations of the mind/brain, the cognitive revolution notwithstanding. By expressly stating the distinction between internal syntax and externalization, many new research directions may be opened up, and new concrete, testable predictions posed particularly from a biological perspective, as the example of animal vocal productions illustrates.
Returning to the core principles of language, unbounded Merge (hence displacement) must have arisen from some rewiring of the brain, hence in an individual, not a group. The individual so endowed would have had many advantages: capacities for complex thought, planning, interpretation, and so on. The capacity would be partially transmitted to oﬀspring, and because of the selective advantages it confers, it might come to dominate a small breeding group, though as with all such novel mutations, there is an issue about how an initially small number of copies of such an allele might survive, despite a large selective advantage. As ﬁrst noted by Haldane (1927), the probability of even a highly advantageous heterozygous mutation with a selective advantage of 1 percent—about an order of magnitude greater than any selective advantage found in ﬁeld measurement—would nevertheless have approximately a 1/e or about a 30 percent chance of going extinct within one generation. Gillespie (1991) notes that a 99 percent certainty of ﬁxing such an advantageous mutation is attained only after reaching approximately 4000 copies of such an allele. Assuming an eﬀective population size of about
4 Note that the argument still goes through if we suppose that there’s another possibility: that FOXP2 builds part of the input–output system for vocal learning where one must externalize and then reinternalize song/language—sing or talk to oneself. This would remain a way to “pipe” items in and out of the internal system, and serialize them, possibly a critical component to be sure, in the same sense that one might require a way to print output from a computer.
5 This is much like attending solely to the diﬀerent means by an LCD television and the old cathoderay tube TVs display moving images without paying any attention to what image is being displayed. The old TVs “painted” a picture by sweeping an electron beam over a set of chemical dots that would glow or not. Liquid crystal displays operate by an entirely diﬀerent means: roughly, they pass light or not through a liquid crystal array of dots depending on an electric charge applied to each dot, but there is no single sweeping beam. One generates the same ﬂat image by an entirely diﬀerent means. Similarly, whether the externalized, linear timing slots are being set out by motor commands to the vocal tract or by moving ﬁngers is irrelevant to the more crucial “inner” representations.

36 Robert Berwick and Noam Chomsky
this number—not unreasonable for what we understand about early demography in Africa at this time, albeit poorly—this would suggest that such a beneﬁcial allele would have to spread to the entire breeding group in order that natural selection could operate unimpeded to sweep the mutation to ﬁxation. This is not paradoxical, but simply reﬂects the stochastic character of evolution by natural selection itself; the same principle applies to all beneﬁcial mutations. What it implies is that the emergence of language in this sense could indeed have been a unique event, accounting for its species-speciﬁc character. Such founder eﬀects in population bottleneck situations are not uncommon.
When the beneﬁcial mutation has spread through the group, there would be an advantage to externalization, so the capacity would be linked as a secondary process to the sensorimotor system for externalization and interaction, including communication as a special case. It is not easy to imagine an account of human evolution that does not assume at least this much, in one or another form. Any additional assumption requires both evidence and rationale, not easy to come by.
Most alternatives do in fact posit additional assumptions, grounded on the language-as-communication viewpoint, presumably related to externalization as we have seen. In a recent survey Számado and Szathmáry (2006) list what they consider to be the major alternative theories explaining the emergence of human language. These include: (1) language as gossip; (2) language as social grooming; (3) language as outgrowth of hunting cooperation; (4) language as outcome of motherese; (5) sexual selection; (6) language as requirement of exchanging status information; (7) language as song; (8) language as requirement for tool making or the outcome of tool making; (9) language as an outgrowth of gestural systems; (10) language as Machiavellian device for deception; and ﬁnally, (11) language as internal mental tool.
Note that it is only this last theory, language as internal mental tool, that does not assume, explicitly or implicitly, that the primary function of language is for external communication. But this leads to a kind of adaptive paradox, since animal signaling ought to then suﬃce. Számado and Szathmáry note:
Most of the theories do not consider the kind of selective forces that could encourage the use of conventional communication in a given context instead of the use of ‘traditional’ animal signals . . . thus, there is no theory that convincingly demonstrates a situation that would require a complex means of symbolic communication rather than the existing simpler communication systems. (2006: 679)

The Biolinguistic Program: The Current State of its Development 37
They further note that the language-as-mental-tool theory does not suﬀer from this defect. However, they, like most researchers in this area, do not seem to draw the obvious inference but instead maintain a focus on externalization and communication.
Proposals as to the primacy of internal language—similar to Harry Jerison’s observation, already noted, about language as an “inner tool”—have also been made by eminent evolutionary biologists. At an international conference on biolinguistics in 1974, Salvador Luria was the most forceful advocate of the view that communicative needs would not have provided “any great selective pressure to produce a system such as language,” with its crucial relation to “development of abstract or productive thinking.” The same idea was taken up by François Jacob, who suggested that “the role of language as a communication system between individuals would have come about only secondarily . . . . The quality of language that makes it unique does not seem to be so much its role in communicating directives for action” or other common features of animal communication, but rather “its role in symbolizing, in evoking cognitive images,” in molding our notion of reality and yielding our capacity for thought and planning, through its unique property of allowing “inﬁnite combinations of symbols” and therefore “mental creation of possible worlds.” These ideas trace back to the cognitive revolution of the seventeenth century, which in many ways foreshadows developments from the 1950s (Jacob 1982; Luria 1974).
We can, however, go beyond speculation. Investigation of language design can yield evidence on the relation of language to the sensory-motor system and thought systems. As noted, we think there is mounting evidence to support the natural conclusion that the relation is asymmetrical in the manner illustrated in the critical case of displacement.
Externalization is not a simple task. It has to relate two quite distinct systems: one is a sensory-motor system that appears to have been basically intact for hundreds of thousands of years; the second is a newly emerged computational system for thought, which is perfect insofar as the strong minimalist thesis is correct. We would expect, then, that morphology and phonology— the linguistic processes that convert internal syntactic objects to the entities accessible to the sensory-motor system—might turn out to be quite intricate, varied, and subject to accidental historical events. Parameterization and diversity, then, would be mostly—possibly entirely—restricted to externalization. That is pretty much what we seem to ﬁnd: a computational system eﬃciently generating expressions interpretable at the semantic–pragmatic interface, with diversity resulting from complex and highly varied modes

38 Robert Berwick and Noam Chomsky
of externalization which, furthermore, are readily susceptible to historical change.6
If this picture is more or less accurate, we may have an answer to the second of the two basic questions: Why are there so many languages? The reason might be that the problem of externalization can be solved in many diﬀerent and independent ways, either before or after the dispersal of the original population.
We have no reason to suppose that solving the externalization problem involved an evolutionary change—that is, genomic change. It might simply be a problem addressed by existing cognitive processes, in diﬀerent ways, and at diﬀerent times. There is sometimes an unfortunate tendency to confuse literal evolutionary (genomic) change with historical change, two entirely distinct phenomena. As already noted, there is very strong evidence that there has been no relevant evolution of the language faculty since the trek from Africa some 50,000 years ago, though undoubtedly there has been a great deal of change, even invention of modes of externalization (as in sign language). Confusion about these matters could be overcome by replacing the metaphoric notions “evolution of language” and “language change” by their more exact counterparts: evolution of the organisms that use language, and change in the ways they do so. In these more accurate terms, emergence of the language faculty involved evolution, while historical change (which continues constantly) does not.
Again, these seem to be the simplest assumptions, and there is no known reason to reject them. If they are generally on the right track, it follows that externalization may not have evolved at all; rather, it might have been a process of problem solving using existing cognitive capacities. Evolution in the biological sense of the term would then be restricted to the mutation that yielded the operation Merge, along with whatever residue resists explanation in terms of the strong minimalist thesis and any language-speciﬁc constraints that might exist on the solution to the cognitive problem of externalization. Accordingly, any approach to evolution of language that focuses on communication, or the sensory-motor system, or statistical properties of spoken language, and the like, may well be seriously misguided. That judgment covers quite a broad range, as those familiar with the literature will be aware.
Returning to the two initial salient questions, we have at least some suggestions—reasonable ones we think—about how it came about that there is even one language, and why languages appear to vary so widely—the latter
6 Positing an independent, recursive, “language of thought” as a means to account for recursion in syntax leads to an explanatory regress as well as being unnecessary and quite obscure.

The Biolinguistic Program: The Current State of its Development 39
partly an illusion, much like the apparent limitless variety of organisms, all of them based on deeply conserved elements with phenomenal outcomes restricted by laws of nature (for language, computational eﬃciency).
There are other factors that may strongly inﬂuence language design— notably properties of the brain, now unknown—and there is plainly a lot more to say even about the topics to which we have brieﬂy alluded here. But instead of pursuing these questions, let us turn brieﬂy to lexical items, the conceptual atoms of thought and its ultimate externalization in varied ways.
Conceptual structures are found in other primates: probably actor–action– goal schemata, categorization, possibly the singular–plural distinction, and others. These were presumably recruited for language, though the conceptual resources of humans that enter into language use appear to be far richer. Speciﬁcally, even the “atoms” of computation, lexical items/concepts, appear to be uniquely human.
Crucially, even the simplest words and concepts of human language and thought lack the relation to mind-independent entities that appears to be characteristic of animal communication. The latter is held to be based on a one–one relation between mind/brain processes and “an aspect of the environment to which these processes adapt the animal’s behavior,” to quote cognitive neuroscientist Randy Gallistel, introducing a major collection of papers on animal communication (Gallistel 1990a). According to Jane Goodall, the closest observer of chimpanzees in the wild, for them “the production of a sound in the absence of the appropriate emotional state seems to be an almost impossible task” (Goodall, cited in Tattersall 2002).
The symbols of human language and thought are sharply diﬀerent. Their use is not automatically keyed to emotional states, and they do not pick out mind-independent objects or events in the external world. For human language and thought, it seems, there is no reference relation in the sense of Frege, Peirce, Tarski, Quine, and contemporary philosophy of language and mind. What we understand to be a river, a person, a tree, water, and so on, consistently turns out to be a creation of what seventeenth-century investigators called the human “cognoscitive powers,” which provide us with rich means to refer to the outside world from intricate perspectives. As the inﬂuential neo-Platonist Ralph Cudworth put the matter, it is only by means of the “inward ideas” produced by its “innate cognoscitive power” that the mind is able to “know and understand all external individual things,” articulating ideas that inﬂuenced Kant. The objects of thought constructed by the cognoscitive powers cannot be reduced to a “peculiar nature belonging” to the thing we are talking about, as David Hume summarized a century of inquiry. In this regard, internal conceptual symbols are like the phonetic units of mental

40 Robert Berwick and Noam Chomsky
representations, such as the syllable [ba]; every particular act externalizing this mental object yields a mind-independent entity, but it is idle to seek a mindindependent construct that corresponds to the syllable. Communication is not a matter of producing some mind-external entity that the hearer picks out of the world the way a physicist could. Rather, communication is a more-or-less aﬀair in which the speaker produces external events and hearers seek to match them as best they can to their own internal resources. Words and concepts appear to be similar in this regard, even the simplest of them. Communication relies on shared cognoscitive powers, succeeding insofar as shared mental constructs, background, concerns, presuppositions, and so on, allow for common perspectives to be (more or less) attained. These properties of lexical items seem to be unique to human language and thought and have to be accounted for somehow in the study of their evolution. How, no one has any idea. The fact that there even is a problem has barely been recognized, as a result of the powerful grip of the doctrines of referentialism.
Human cognoscitive powers provide us with a world of experience, diﬀerent from the world of experience of other animals. Being reﬂective creatures, thanks to the emergence of the human capacity, humans try to make some sense of experience. These eﬀorts are called myth, or religion, or magic, or philosophy, or, in modern English usage, science. For science, the concept of reference in the technical sense is a normative ideal: we hope that the invented concepts photon or verb phrase pick out some real thing in the world. And of course the concept of reference is just ﬁne for the context for which it was invented in modern logic: formal systems, in which the relation of reference is stipulated, holding for example between numerals and numbers. But human language and thought do not seem to work that way, and endless confusion has resulted from failure to recognize that fact.
We enter here into large and extremely interesting topics that we will have to put aside. Let us just summarize brieﬂy what seems to be the current best guess about unity and diversity of language and thought. In some completely unknown way, our ancestors developed human concepts. At some time in the very recent past, maybe about 75,000 years ago, an individual in a small group of hominids in East Africa underwent a minor mutation that provided the operation Merge—an operation that takes human concepts as computational atoms, and yields structured expressions that provide a rich language of thought. These processes might be computationally perfect, or close to it, hence the result of physical laws independent of humans. The innovation had obvious advantages, and took over the small group. At some later stage, the internal language of thought was connected to the sensory-motor system, a complex task that can be solved in many diﬀerent ways and at diﬀerent times,

The Biolinguistic Program: The Current State of its Development 41
and quite possibly a task that involves no evolution at all. In the course of these events, the human capacity took shape, yielding a good part of our “moral and intellectual nature,” using Wallace’s phrase (1871). The outcomes appear to be highly diverse, but they have an essential unity, reﬂecting the fact that humans are in fundamental respects identical, just as the hypothetical extraterrestrial scientist we conjured up earlier might conclude that there is only one language with minor dialectal variations, primarily—perhaps entirely—in mode of externalization.
To conclude, recall that even if this general story turns out to be more or less valid, and the huge gaps can be ﬁlled in, it will still leave unresolved problems that have been raised for hundreds of years. Among these is the question of how properties “termed mental” relate to “the organical structure of the brain,” in the eighteenth-century formulation; and the more mysterious problems of the creative and coherent ordinary use of language, a central concern of Cartesian science, still scarcely even at the horizons of inquiry.

3
Some Reﬂections on Darwin’s Problem in the Context of Cartesian Biolinguistics
CEDRIC BOECKX
3.1 Darwin’s Problem and Rationalist Commitments
Already in the early days of modern science (seventeenth and eighteenth centuries) it was clear to natural philosophers like Descartes, Hobbes, Humboldt, and Hume that a detailed understanding of the human language faculty (FL) would be critical to the development of a genuine “Science of Man.” As Chomsky has remarked on numerous occasions (see Chomsky 1965, 1966, 1972b), the “Cartesians”1 saw in the essence of language the direct reﬂex of Man’s most distinctive cognitive attributes at work—most prominent among which the unbounded creativity that is so unique to us. Under Chomsky’s impetus modern linguistics has recaptured the central themes of the ﬁrst cognitive revolution and is now a core area of modern cognitive science, a branch of biology. It is in order to emphasize the true nature of this research program that linguists of a generative orientation have begun to use the term
Early versions of the present work were presented in talks at the Universitat de Barcelona (Grup de Biolingüística), Potsdam University (Workshop on Biolinguistics), the University of Maryland, and at the San Sebastian encounter with Noam Chomsky, organized by Massimo Piattelli-Palmarini, Pello Salaburu, and Juan Uriagereka in the summer of 2006. I am grateful to the organizers of these venues for inviting me, and to the participants in these events for comments. For invaluable discussions of the issues touched on here over the years, I thank Noam Chomsky, Norbert Hornstein, Massimo Piattelli-Palmarini, Juan Uriagereka, Marc Hauser, Paul Pietroski, and Dennis Ott. Thanks also to the participants in my Spring 08 seminar on Biolinguistics at Harvard University for penetrating questions, and to Anna-Maria di Sciullo for her interest in this piece, and her support.
1 To the dismay of historians of philosophy, Chomsky (1966) lumps together traditional rationalists and empiricists under the umbrella adjective Cartesian. I will follow him in this regard, as I believe that empiricists like Hume and Hobbes were much more rationalists than their modern reincarnations (behaviorists, connectionists, etc.). For further evidence, see Jerry Fodor’s treatment of Hume in Fodor (2003).

Darwin’s Problem in the Context of Cartesian Biolinguistics 43
“Biolinguistics,” a term ﬁrst used with this intention by Massimo PiattelliPalmarini at a meeting in 1974.
The immediate aim of biolinguistics is still to reveal as accurately as possible the nature of the FL, but biolinguistic inquiry does not stop there. It also seeks to understand the course of development of language in the organism and the way it is put to use once it has reached its mature state; in particular, how linguistic form may give rise to meaning. Ultimately, biolinguists hope to contribute to our understanding of how core properties of language are implemented in neural tissues and how it evolved in the species. These last two tasks have an obvious interdisciplinary character, requiring linguists to join forces with psychologists, biologists, philosophers, and so on. I ﬁrmly believe that the linguists’ own works on the nature of the FL will be of critical importance in the formulation of detailed hypotheses to be tested at diﬀerent levels of scientiﬁc analysis. Furthermore, with their emphasis on formal/structural aspects of the FL and on the existence of both universality and diversity in language, biolinguists also hope to contribute signiﬁcantly to the emergence (currently underway) of an expanded modern synthesis in biology, about which I will have more to say in this chapter.
After 50 years of research in this domain, we can say with some conﬁdence that progress has been made on two fronts: what Chomsky calls Humboldt’s Problem (the characterization of what the FL is) and Plato’s Problem (how the FL grows into a mature state in the individual).2 How FL is put to use (“Descartes’ Problem”) remains as much a mystery to us as it was to the Cartesians. Likewise, the issue of brain implementation remains elusive.3 The evolutionary question—what one might call, for obvious reasons, Darwin’s
2 In laying out the research agenda of modern (bio-)linguistics, Chomsky appeals to important historical ﬁgures like Plato, Humboldt, and Descartes, who not only worried about the very questions generative grammarians focus on, but also made insightful suggestions that ought to be incorporated into current discussions.
3 To the best of my knowledge, Chomsky has not given a venerable name to this problem. This is not because no important historical ﬁgure thought about the mind/brain (Descartes, Spinoza, and many others did), but because virtually no one managed to formulate insights that could be useful at this stage in our understanding. Perhaps one could call it Broca’s Problem, or Gall’s Problem (I am here referring to Gall’s desire to “put an end to the highhanded generalizations of the philosophers,” and his wish to study language as the “creation of our internal faculties, the latter being represented in the brain by organs”). But one should bear in mind that at this stage this is more of a mystery than a genuine problem (on the distinction between problem vs. mystery, see Chomsky 1975). Although the concern of mind/brain uniﬁcation is certainly legitimate, it is often thought that solving this problem is a prerequisite for the legitimacy of the biolinguistic enterprise. This is incorrect, in my opinion. Just like it would be incorrect to characterize Mendel’s results as not part of genetics. At numerous levels, it can be shown that linguistic inquiry, taking place in a generative/biolinguistic context, is continuous with research in various areas of the life sciences. I refer the interested readers to Boeckx and PiattelliPalmarini (2005) and Boeckx (2006).

44 Cedric Boeckx
problem4—was consciously set aside for much of the past ﬁve decades5 but it has gained momentum in recent years, for reasons that I think are worth detailing.
3.1.1 The conceptual relationship between Plato’s Problem and Darwin’s Problem
The central problem in generative grammar, as made explicit in Chomsky (1965: ch. 1), is to account for the human capacity for language acquisition; how any child, short of pathology or highly unusual environmental circumstances, acquires at least one language by the time they reach puberty (at the very latest) in a way that is remarkably uniform and relatively eﬀortless. The acquisition of language is all the more remarkable when we take into account the enormous gap between what human adults (tacitly) know about their language and the evidence that is available to them during the acquisition process. It should be obvious to anyone that the linguistic input a child receives is radically impoverished and extremely fragmentary. It is in order to cope with this “poverty of stimulus” that Chomsky claimed that humans are biologically endowed with a capacity to develop a language. The biological equipment that makes language acquisition possible is called Universal Grammar (UG). In positing UG, Chomsky was doing what ethologists like Lorenz and Tinbergen had been led to do to account for the range of highly speciﬁc behaviors that many animals display. At a more general level, Chomsky was doing for language what Plato had done in Meno for geometry and what the Rationalists (Descartes, Leibniz, etc.) had done for ideas more generally. All of them were making it possible in principle for information to be gathered from experience (i.e. for learning to take place). In the absence of some innate bias toward interpreting incoming data in speciﬁc ways, knowledge (of language, mathematics—of anything!) would never be attained.6 As the structure of UG became clearer in the Principles-and-Parameters era, attention shifted away
4 Like Humboldt, Plato, and Descartes, Darwin recognized the distinctive character of our ability to pair sound and meaning and was fascinated with the nature of our language faculty, our “‘instinctive tendency to speak,” as he called it; an ability he related to birdsong, as in the following passage): The sounds uttered by birds oﬀer in several respects the nearest analogy to language, for all the members of the same species utter the same instinctive cries expressive of their emotions; and all the kinds that sing, exert their power instinctively; but the actual song, and even the call notes, are learnt from their parents or foster-parents. (Darwin 1871: 108)
In addition, like Humboldt, Plato, and Descartes, Darwin made observations that are variants of, or sources of hypotheses entertained today. Koji Fujita has brought to my attention that he independently coined the term “Darwin’s problem” in Fujita (2002).
5 Although the problem was set aside, a very insightful discussion of many relevant issues can be found in Lenneberg’s work (Lenneberg 1967).
6 On this point, see Gallistel (2007).

Darwin’s Problem in the Context of Cartesian Biolinguistics 45
from the logical problem of language acquisition and toward its cousin, the logical problem of language evolution.
It was already evident to the Rationalists that a discussion of the origin of the FL would be relevant to our understanding of the nature of the FL (see e.g. Viertel’s discussion of Herder’s theory of language origin (Viertel 1966)—after all, any investigation into the emergence of some faculty x depends on speciﬁc hypotheses regarding what x is7—but our notion of what the FL was likely to be ﬁrst had to rest on somewhat secure grounds to prevent evolutionary scenarios from being more than fanciful just-so stories.
Once linguists felt the foundations were solid, they indeed began to approach UG “from below,” and formulate informed speculations (hypotheses) concerning the emergence of language in the species, beginning with Hauser, Chomsky, and Fitch (2002). In the preceding paragraph I referred to Darwin’s Problem as the cousin of Plato’s Problem. This is because the logical structure of Darwin’s Problem turns out to be very similar to that of Plato’s Problem.8 Both revolve around a Poverty of Stimulus situation. In the context of Plato’s Problem, the argument goes like this: Given the richness and complexity of our human knowledge of language, the short time it takes for children to master their native languages, the uniformity displayed within and across languages during the acquisition process, and the poverty of the linguistic input to children, there does not seem to be any way out of positing some head start (in the guise of some innate component, UG) in the language acquisition process. This head start not only allows linguists to make sense of the speed at which (ﬁrst) languages are acquired, but also why the acquisition process takes the paths it takes (as opposed to the paths it could logically take). By minimizing the role of the environment, UG allows us to begin to grasp how Plato’s Problem could be solved.
Similarly, when it comes to Darwin’s Problem, everyone seems to grant that the FL emerged in the species very recently (within the last 200,000 years, according to most informed estimates). Everyone also seems to grant that this was a one-time event: the faculty is remarkably uniform across the
7 This is the very reason why Hume put so much emphasis on developing a theory of history as part of his Science of Man project.
8 In the context of language, the similarity between the two issues, acquisition and evolution, had not escaped Skinner, as Chomsky points out in his famous 1959 review. I ﬁrst discussed the similarity between the two problems in a minimalist context in Boeckx (2009). Hornstein (2009) pursues this similarity as well.
At a more general level, the relationship between ontogeny and phylogeny is one of the great themes in biology; see Gould (1977). The similarity is nicely captured in German, where issues in both ontogeny and phylogeny are referred to as issues of Entwicklungsgeschichte (lit. ‘history of development’).

46 Cedric Boeckx
species, a fact that is most likely the result of the faculty having emerged in a small group that spread (Homo sapiens). In light of the extremely recent emergence of the FL, one ought to welcome a hypothesis that minimizes the role of the environment (read: the need for several adaptive steps), and, more generally, one that minimizes what had to evolve. Just as in the context of Plato’s Problem, the individual in which the FL emerged must be given a head start: the key evolutionary event must be assumed to have been small, and many cognitive structures available to our ancestors must have been recruited (with minimal modiﬁcations, to avoid the need for many adaptive steps). In Kirschner and Gerhart’s (2005) terms, the emergence of this biological novelty had to be facilitated. As far as I can see, this is exactly the logic laid out in Hauser, Chomsky, and Fitch (2002), who take the FL to consist of a variety of shared cognitive structures (what they call the Faculty of Language in the Broad sense, FLB), and a minimal amount of genuine novelty/speciﬁcity (their Faculty of Language in the Narrow sense, FLN). My point in this section is that this sort of evolutionary scenario makes a lot of sense once we recognize the similarity between the logic of Darwin’s Problem and that of Plato’s Problem. (I guess one could say that in the context of language, the argument about phylogeny (Darwin’s problem) recapitulates the argument about ontogeny (Plato’s problem).)
3.1.2 (Neo-)Cartesian linguistics meets (neo-)rationalist morphology
It is customary to allude to Theodor Dobzhansky’s well-known dictum that “nothing makes sense in biology except in the light of evolution” whenever questions of origin are raised (Dobzhansky 1973). The exquisite complexity of organisms can only be accounted for, so it seems, by means of natural selection. As Dawkins (1996: 202) puts it, “whenever in nature there is a suﬃciently powerful illusion of good design for some purpose, natural selection is the only known mechanism that can account for it.” Questions of origin pertaining to the mind, the “Citadel itself,” as Darwin called it, are no exception. Indeed, the assumption that natural selection is the “universal acid” (Dennett 1995) is perhaps nowhere as strong as in the study of mental faculties, being the motto (credo?) of evolutionary psychology (witness Pinker 1997, Marcus 2008). But the simplicity of Dobzhansky’s assertion conceals layers of necessary reﬁnements that cannot be ignored. Its meaning very much depends on what it means to make sense of life (including mental life), and what we understand by (Darwinian) evolution.
As Fox-Keller has made clear in her book Making Sense of Life (Keller 2002), the notion of explanation, of “making sense of life,” cannot be uniformly

Darwin’s Problem in the Context of Cartesian Biolinguistics 47
deﬁned across the life sciences.9 As for Darwinian evolution, Gould, more than anyone else, has stressed the richness and complexity of evolutionary theory (see Gould 2002), and stressed the limitations of ultra-Darwinism and its narrowly adaptationist vision. One can, and must, preface any study of origin by “ever since Darwin,” not, I think, by “ever since Dawkins.” And one must bear in mind that Darwin himself was explicit about the fact that “natural selection is . . . not [the] exclusive means of modiﬁcation” (Darwin 1859: 6) There are signs that the tide is changing. The promises of genome sequencing, and of the selﬁsh gene, have not been met, and a growing number of biologists side with Lynch’s (2007) opinion that “many (and probably most) aspects of genomic biology that superﬁcially appear to have adapative roots . . . are almost certainly also products of non-adaptive processes.” Speaking for all evo–devo adherents, Carroll (2005a) points out that the modern synthesis has not given us a theory of form. A theory of form is at the leart of what Kirschner and Gerhart call “Darwin’s Dilemma.” When Darwin proposed his theory of evolution, he relied on two ingredients: variation and selection. Although he could explain selection, he could not explain variation. The forms on which selection operated were taken for granted. Since The Origin of Species, at repeated intervals, and with accelerated pace in recent years, it has been suggested that several factors giving direction to evolution (facilitating variation, biasing selection, etc.) must be taken into account.
As Gould (2002: 347) clearly states,
simple descent does not solve all problems of “clumping” in phenotypic space; we still want to know why certain forms “attract” such big clumps of diversity, and why such large empty spaces exist in conceivable, and not obviously malfunctional, regions of potential morphospace. The functionalist and adaptationist perspective ties this clumping to available environments, and to shaping by natural selection. Structuralists and formalists wonder if some clumping might not record broader principles, at least partly separate from a simple history of descent with adaptation principles of genetics, of development, or of physical laws transcending biological organization.
In this respect Gould (2002: 21) calls for a renewed appreciation for “the enormous importance of structural, historical, and developmental constraints in channeling the pathways of evolution, often in highly positive ways, adding that “the pure functionalism of a strictly Darwinian (and externalist) approach
9 Mayr (2004) says nothing diﬀerent when he calls our attention to two ﬁelds of biology, with radically diﬀerent methodological assumptions: functional biology (biochemistry) and evolutionary biology.

48 Cedric Boeckx
to adaptation no longer suﬃces to explain the channeling of phyletic directions, and the clumping and inhomogenous population of organic morphospace.”
Echoing Gould, Pigliucci (2007) writes that biology is in need of a new research program, one that stresses the fact that natural selection may not be the only organizing principle available to explain the complexity of biological systems. It is not just all tinkering; there is design too.10 Pigliucci (2007) reviews numerous works that provide empirical evidence for non-trivial expansions of the modern synthesis, with such concepts as modularity, evolvability, robustness, epigenetic inheritance, and phenotypic plasticity as key components.
Amundson (2005) points out correctly that many of the themes at the heart of the expanded modern synthesis (a more enlightened version of Darwinian evolution) hark back to all the major theorists of life before Darwin, especially those that are often called the Rationalist Morphologists. All major theories of life before Darwin followed a tradition reaching back to Plato in presenting a fundamentally “internalist” account, based upon intrinsic and predictable patterns set by the nature of living systems for development through time, as the term “evolution” (evolutio, “unfolding”) reveals. As one of the foremost exponents of such internalist accounts, and the person who coined the term “morphology,” Goethe writes (second essay on plant metamorphosis, written in 1790):
In my opinion, the chief concept underlying all observation of life—one from which we must not deviate—is that a creature is self-suﬃcient, that its parts are inevitably interrelated, and that nothing mechanical, as it were, is built up or produced from without, although it is true that the parts aﬀect their environment and are in turn aﬀected by it.
By analogy with Chomsky’s distinction between I(nternalist)-linguistics and E(xternalist)-linguistics introduced in Chomsky (1986), we could call the modern synthesis E-biology and the return to pre-Darwinian concerns, I-biology. As a common thread, internalist accounts deny exclusivity to natural selection as the agent of creativity, viewing “adaptation as secondary tinkering rather than primary structuring” (Gould 2002: 290). Internalists claim a high relative frequency of control by internal factors, emphasizing notions like Unity of Type and Correlation of growth. At the heart of internalist
10 Not only design but perhaps simplicity too; see my remarks on Alon (2007) in Boeckx (2010). Note, incidentally, that the more good design and simplicity one ﬁnds in nature, the less one should be tempted to regard the brain (or any other organ) as a kluge (contra Marcus 2008).

Darwin’s Problem in the Context of Cartesian Biolinguistics 49
frustrations is the linkage between natural selection and contingency. In the words of Kauﬀman (1993: 26):
We have come to think of selection as essentially the only source of order in the biological world. It follows that, in our current view, organisms are largely ad hoc solutions to design problems cobbled together by selection. It follows that most properties which are widespread in organisms are widespread by virtue of common descent from a tinkered-together ancestor, with selective maintenance of useful tinkerings. It follows that we see organisms as overwhelmingly contingent historical accidents, abetted by design. My own aim is not so much to challenge as to broaden the neo-Darwinian tradition. For, despite its resilience, that tradition has surely grown without attempting to integrate the ways in which simple and complex systems may spontaneously exhibit order.
Despite the fact that various biologists have complained that phrases like “adaptation to the edge of chaos,” and “order for free,” repeatedly used by Kauﬀman, Goodwin, and other proponents of Neo-rationalism in biology, lack clear scientiﬁc deﬁnition and operational utility, Gould (2002: 1213) argues that Kauﬀman et al. are groping toward something important, a necessary enrichment or broadening of biology, with important implications.
Of great signiﬁcance is the fact that the concerns that animate the return to the insights of the Rationalist Morphologists are the very same concerns that animate research in (Cartesian) biolinguistics. By using Cartesian Biolinguistics I intend to point to an important distinction within those who conceive of linguistics as a branch of biology (at a suitable level of abstraction). I suspect that most biolinguists make the very same “bet” that Dawkins does,11 and privilege adapation as the sole source or order and complexity. Let us call them neo-Darwinian biolinguists (see Givón 2002, Marcus 2008). By contrast, those that I would call Cartesian biolinguists follow Chomsky in (i) favoring internalist explanations, (ii) seeing design and topology where others would see tinkering, and (iii) focusing on Form over Function.12 Indeed, once the complexity of biology as a whole, and evolutionary biology in particular, is clear, any perceived conﬂict between “Chomsky and Darwin” (Dennett 1995), or any need to reconcile them (Calvin and Bickerton 2000), quickly evaporates.
As a matter of fact, once the richness of evolutionary biology is taken into consideration, it seems to me that one can begin to approach Darwin’s problem with some optimism toward its resolution. I am here relying on the fact that a neo-Darwinian view of the type advocated by Pinker and
11 On Dawkins’s bet, and how it contrasts with the one made by Gould, see Sterelny (2007). 12 For converging remarks pertaining to cognitive science as a whole, see Fodor (2008).

50 Cedric Boeckx
Bloom (1990) still strikes me as hopeless, as Piattelli-Palmarini (1989), and more recently, Uriagereka (1998) and Lorenzo and Longa (2003), have made clear. But it is fair to say that the alternative, neo-rationalist scenario was hard to entertain until the advent of the minimalist program in linguistic theory. As I discussed in Boeckx (2006: ch. 4), the standard Principles-and-Parameters model of the FL focused on the speciﬁcities of the language organ, and made it very unlikely that central linguistic concepts such as c-command, government, empty categories, and cyclicity, just to name a few, may have emerged from any suﬃciently general theory of form. The standard Principles-andParameters architecture, with its richly modular structure, oﬀered a picture of the language faculty that was too complex for structural constraints (of the sort explored by D’Arcy Thompson) to realistically account for its emergence.
Put diﬀerently, the idea that the language faculty was not shaped by adaptive demands, but by physical constraints (“Turing’s thesis,” as Chomsky sometimes calls it)—a recurring theme in Chomsky’s writings (see Jenkins 2000, Otero 1990, for relevant citations)—did not ﬁt snugly in past frameworks. It found its niche only recently, as part of the Minimalist Program for linguistic theory, in the same way that the pre-Darwinians’ speculations about a general theory of biological form seem to be ﬁnding their niche in the extended modern synthesis advocated by a growing number of biologists.
At the risk of oversimplifying, I will say that the core idea behind linguistic minimalism is that all the apparent complexity revealed in the course of pursuing a Cartesian Linguistics program is the result of few very simple computational mechanisms. Although the Minimalist Program as a whole may still be premature, there is little doubt that it oﬀers an extremely useful perspective from a biological point of view, especially in the context of evolution.13 With its emphasis on virtual conceptual necessity, minimalism reduces considerably the burden any evolutionary story has to bear. This is a welcome consequence because, to repeat, according to everyone’s best guess, the human language faculty emerged very, very recently in the species, which makes it hard to seriously entertain an adapatationist, gradualistic story. There is just not enough time for such a complex object to be built step by step.
3.2 The Key Novelty
The hypothesis formulated in Hauser et al. (2002) and reﬁned in Fitch, Hauser, and Chomsky (2005) oﬀers a concrete example of the sort of research program
13 As I argued in Boeckx (2006) (see also Hornstein 2009), linguistic minimalism may also form the basis for a more fruitful research program for addressing “Gall’s/Broca’s problem,” or the question of how the FL is implemented in the brain.

Darwin’s Problem in the Context of Cartesian Biolinguistics 51
that minimalism makes feasible in the study of Darwin’s problem.14 According to Hauser et al., what distinguishes humans from other species is the FLN: the computational system that constitutes Narrow Syntax, speciﬁcally its recursive quality (the ability for unbounded embedding) and the way syntactic expressions maps the syntactic objects it constructs to the conceptual–intentional and sensory–motor systems. They claim that there is evidence that other species possess sensory–motor and at least some conceptual-intentional systems similar to our own (on the sensory-motor side, see also Samuels 2009; for some of the conceptual richness in animals, see Hauser 2000, Carruthers 2006, Cheney and Seyfarth 1990, 2007, among many others). These constitute the FLB. (Hauser et al. 2002 leave open the possibility that once the FLN was in place, its presence led to modiﬁcations of FLB-components. For some evidence that this was the case on the sound side, see Samuels (2009). See also next section.) Hauser et al. (2002: 1574) point out that their hypothesis may have important consequences for how we think about the evolution of cognition:
[The hypothesis that only the FLN is unique to humans] raises the possibility that structural details of [the FLN] may result from preexisting constraints, rather than from direct shaping by natural selection targeted speciﬁcally at communication. Insofar as this proves to be true, such structural details are not, strictly speaking, adaptations at all.
It may be useful to point out that the evolutionary novelty that Hauser, Chomsky, and Fitch dub the FLN need not conﬂict with Darwin’s important claim that novelty is often the result of descent with some signiﬁcant modiﬁcation. Indeed, genuine novelty, in the sense of emergence of completely new processes, is extremely rare in the biological world. Nature, as Jacob famously pronounced, is a tinkerer. But although many have seized on Jacob’s pronouncement to stress the klugy, higgledy-piggledy aspect of evolution, I do not think that this is Jacob’s most important lesson. I think Jacob wanted to emphasize that novelty in the organism’s physiology, anatomy, or behavior arises mostly by the use of conserved processes in new combinations at diﬀerent times and in diﬀerent places and amounts, rather than by the invention of completely new processes. This is exactly what Darwin meant by his
14 As Fitch et al. (2005) observe, although their hypothesis is logically independent from the minimalist program, the latter renders the former far more plausible. This plausibility is nowhere clearer than in the way minimalist proposals, with their focus on elementary operations, provide the basis for comparative, cross-species studies. This new chapter in comparative syntax and the way syntax interfaces with external systems is one of the ways in which our evolutionary narratives will be testable. It is not the case that comparative studies were impossible earlier, but the notions the various disciplines made use of were incommensurable.

52 Cedric Boeckx
term “descent with modiﬁcation” (a term which he preferred to “evolution”). Germans would say that novelty is characterized by Um-bildung (‘reformation’, ‘recombination’), not by Neu-bildung (‘new formation’)—topological variations, not introductions of novel elements. As Gould (1977: 409) clearly stated, “there may be nothing new under the sun, but permutations of the old within complex systems can do wonders.” Novelty in biology arises the same way water arises from combining the right amount of H and of O. Notice that if this characterization of biological novelty is borne in mind, the fact that speciﬁcity is not consistently reﬂected in brain images, genetic disorders, etc. need not lead to a crisis for cognitive sciences, as is sometimes thought (see the literature against modularity). It is just what you expect if novelty arose through recruitment and subsequent diversiﬁcation. It may well be that we are not yet at a stage where we detect diversiﬁcation among recruited parts, a message which I think goes along the lines stressed by Josef Grodzinsky in recent work (see also Marcus 2006).
3.2.1 The lexical envelope as the locus of linguistic speciﬁcity
The literature following Hauser et al. (2002) has focused on their claim that a key property of the FLN is recursion (ignoring the fact that Hauser et al. explicitly mentioned the possibility that the FLN may be empty, as well as their emphasis on the issue of interfaces—the mapping of syntactic expressions onto the right mental components). Here I will show how a speciﬁc characterization of recursion may have important consequences for another seemingly unique and language-speciﬁc property of human cognition, the ability to build a lexicon.
Chomsky (2004a) has identiﬁed Merge as the most basic procedure that could yield recursive structures of the sort that the FL makes use of. In its simplest form, Merge takes two elements · and ‚ and combines them into a set, {·, ‚}. Iterative applications of Merge yield recursive structures {‰, . . . {„, {·, ‚ }} . . . }.15 Here I would like to concentrate on the property that makes elements mergeable. After all, set formation is a very basic computational operation, one that is unlikely to be unique to humans or speciﬁc to language. What is remarkable and unique about the FL (and perhaps derivative systems like our improved number sense) is the fact that Merge is recursive in other words, what makes Merge possible in the ﬁrst place remains available throughout a linguistic computation. Following a suggestion of Chomsky’s
15 Descriptively speaking, one can, following Chomsky (2004a)—in a move whose signiﬁcance has not been fully appreciated in my view—distinguish between “external” Merge, which combines two previously unconnected elements (i.e. two elements selected from the lexicon), and “internal” Merge, which combines two elements, one of which was already contained inside the set to which the other element is merged. Internal Merge yields conﬁgurations manifesting displacement (“movement”).

Darwin’s Problem in the Context of Cartesian Biolinguistics 53
(see Chomsky 2005), I would like to attribute this fact to the idea that lexical items are sticky. They have what Chomsky calls an edge feature. The following passage, from Chomsky (2008: 6) makes this clear:
For a L[exical] I[tem] to be able to enter into a computation, merging with some [syntactic object], it must have some property permitting this operation. A property of an LI is called a feature, so an LI has a feature that permits it to be merged. Call this the edge-feature (EF) of the LI.
As I suggest elsewhere (Boeckx, in progress), we can think of the process of lexicalization as endowing a concept with a certain inertia, a property that makes the lexical item active (i.e. allows it to engage in Merge relations).16
We can represent a lexicalized concept C endowed with an edge feature as: {C} (a concept with a lexical envelope), or +{C}, with the + sign representing the edge property that allows further combination, much like a free electron allows bonding in chemistry.17 We can also think of the lexical envelope as a mapping instruction to the Conceptual–Intentional system to “fetch a concept C” (see Pietroski, to appear). Thus conceived, the process of lexicalization not only makes Merge possible, it also achieves what amounts to a demodularization of concepts. We can in fact think of lexicalization as the mental analog of the hypothetical creation of a truly universal currency, allowing transactions to cross formerly impenetrable boundaries.
I take it that Jerry Fodor is right to think of the mind as consisting at least in part of a variety of modules (the exact number and identity of which are not important for my purposes). I also assume that the modular mind is not a uniquely human attribute, but is in fact quite widely shared with other species. The ethology literature is replete with evidence that throughout the animal kingdom creatures are equipped with specialized behaviors, many of which require a certain amount of highly speciﬁc triggering experience, which crucially transcend the limits of any behaviorist stimulus–response schema. I follow Gallistel, Hauser, Marler, Cheney, and Seyfarth, and many cognitive ethologists in claiming that animals come equipped with learning organs (a.k.a. modules or core knowledge systems). Remarkably, as I will emphasize in the next section, humans appear to be uniquely endowed with the ability to consistently go beyond the limits of these modules and engage in systematic cross-modular combinations (i.e. cross-modular thought). I would like to claim that it is the process of lexicalization that underlies this ability to
16 In Boeckx (in progress) I build on Marantz (2008) and take lexicalization to be a phase transition, induced by the com√bination of a ‘root’ (concept) with a lexical categorizer (Marantz’s “little x”); that is to say, {C} = {x, C }.
17 Hiroki Narita (p.c.) points out that the set-formation capacity that is Merge leads us to expect the existence of “unvalued” features, giving rise to agreement relations (Chomsky 2000a), if we understand “unvalued feature” as a feature whose value has merged with the empty set.

54 Cedric Boeckx
extract concepts from their modular bounds. It is as if the lexical envelope (the edge feature) on the one hand makes the content of a concept opaque to the computational system (a hard atom in Fodor’s 1998 sense), and, on the other, frees this concept from its limited (modular) combinatorial potential (for a similar view, see Pietroski, to appear). Once lexicalized, concepts can be combined freely (via Merge) as expressions like Chomsky’s Colorless green ideas sleep furiously or Lewis Carroll’s “Jabberwocky” attest. Syntactic relations cease to depend on (semantic/conceptual) content; presumably, by the same token, the semantics of “words” cease to be tied to (externalist) notions like reference (which may well be at work inside modules).
I am surrounding the term “word” with quotation marks because I want to emphasize the fact that linguistic words are not merely sound–meanign pairs; they are mergeable items. Word formation (in this sense) is as speciﬁc and unique to the FL as recursion. Accordingly, the oft-made claim that members of other species may acquire words, but may lack the ability to combine them (see Anderson 2004 on Kanzi) must be qualiﬁed. Acquiring words in the context of the present discussion cannot be dissociated from being able to freely combine them. If one insists on designating the sound–meaning associations attested in others species as “words,” then we should say that FL lacks words, but instead possesses lexical items.
My claim in this section is that the edge feature, the catalyst for recursive Merge, is the one key property that had to evolve. I am silent on precisely how it evolved. It may be the result of random mutation, or an exaptation. Perhaps we will never know for sure, but it is something that is now part of our biological endowment (albeit maybe indirectly coded, perhaps as part of brain growth)—what Chomsky (2005) would call a ﬁrst factor component. Other properties standardly attributed to Merge, such as binary branching or the phasal property of certain nodes (the ability of certain nodes to trigger transfer of their complements to the interfaces), may instead be the result of non-genomic, third factor principles.18 In the remainder of this section I will focus on binary branching, and assume that cyclicity (spell-out by phases) is a speciﬁc implementation of the general chunking strategy that pervades the cognitive world, especially when working memory is involved (see Miller 1956, Feigenson and Halberda 2004, Terrace 2005, among many others).19
18 Other properties of the FLN, for example, the mapping instructions to external mental components may have been recruited. Indeed, the sort of mapping instructions discussed by Pylyshyn (2007) in the context of vision may be suﬃcient to carry out the required operations, especially if the mapping is as simple and transparent as Pietroski (to appear), Kayne (1994), and others claim.
19 For material that directly bears on the relevance of subitizing and chunking in language, see ? (?).

Darwin’s Problem in the Context of Cartesian Biolinguistics 55
3.2.2 On the distribution of lexical material in natural languages
Since Kayne (1984) it has been standard to take syntactic representations to be constrained by a binary branching requirement; in modern parlance, Merge can only combine two elements at a time. For Kayne, the binary branching requirement on syntactic structures was imposed to ensure that paths (the set of nodes between two elements establishing a syntactic dependency) be unambiguous (basically, binary branching reduces the number of routes an element might take to connect to another element). Chomsky has also at times suggested that binary branching may be imposed by an overarching requirement of eﬃcient computation (see Chomsky (2004a: 115), and especially Chomsky (2005: 16), where “minimization of search in working memory” is hinted at). I would like to claim in this subsection that the intuition that binary branching may be the result of third-factor considerations is on the right track, and can in fact be strengthened by taking into account results achieved on completely independent grounds in Bejan (2000).
For a number of years Bejan has been studying systems that exhibit binarybranching (bifurcation, pairing, dichotomy) properties, and has hypothesized that all these systems are organized in this way as a result of an optimization process. Speciﬁcally, he has established on the basis of a wide range of examples (from systems of nature to artiﬁcial systems in engineering) that ﬂow systems that connect one root point to a ﬁnite-size area or volume (an inﬁnity of points) display tree-shaped networks. He claims that the shape of the network can be deduced from considerations guaranteeing easiest access (optimal ﬂow). Bejan is careful to stress that exactly what ﬂows is largely irrelevant (it can be electricity, water currents, and, I would add, information such as lexical/conceputal information); what matters is how what ﬂows ﬂows. Bejan is able to show mathematically that the binary branching the networks he studies exhibit is one of constant resistance—that is, one that deﬁnes the path of least resistance for all that points in an area/volume that have to be squeezed through a single exit (one that minimizes entropy generation). The basic intuition is one that Galileo already had, when he investigated ways of deﬁning a beam of constant strength. Galileo concluded that a beam of constant strength is one in which the maximum stress (pressure) is spread as much as possible through the body of the beam. This is equivalent to a binarybranching tree. It is indeed easy to see that a uniformly binary-branching tree is better equipped to provide the least resistance for whatever is ﬂowing from a terminal node to a root note. The maximum resistance is deﬁned by the maximum number of branches meeting at a single point. In a binary branching tree, the maximum number (nmax) is 2. This is less than if the tree exhibits uniform ternary branching ((nmax = 3), or if the tree varies in its

56 Cedric Boeckx
branching conﬁguration (making some points of access more resistant than others). This is equivalent to minimizing the maximum pressure diﬀerence across points.
Bejan notes that dendritic patterns occur spontaneously in nature when ﬂow systems are large and fast enough, because it is Y-shaped ﬂow systems that minimize volume/area-to-point resistance (thermodynamic optimization). By the same reasoning, he is able to predict the shape of snow ﬂakes, the emergence of fractal structures, and even deduce Fermat’s Principle of Least Time/Maupertuis’s Principle of Least Action (Path of Least Time = Path of Easiest/Fastest Access = Path of Least Resistance). A binary-branching tree is thus one that achieves constant minimax stress/resistance across the length of a derivation—a sort of smooth design, where no single point bears more than two relations at any given time.
Nature thus appears to favor slender trees, achieving resistance minimization through growth. It optimizes access by optimizing the internal geometry of the system, achieving a stationary optimal conﬁguration, an optimal space allocation, which patterns according to a scaling law already recognized by Murray (1926), where W = 2d (W = width, d = depth).
I submit that syntax performs its objective (providing instructions to external mental systems; squeezing a phrase structure representation through a spell-out point; a volume/area-to-point situation) in the best possible manner; with binary branching emerging as a balancing act that guarantees equipartition (optimal spacing, or uniform spreading) of terminals.
By focusing on the structure of ﬂow systems, Bejan reveals nature’s urge to optimize. Bejan notes that the tree-shaped networks he studies are astonishing in simplicity and robustness, holding across the inanimate, the animate, and the engineered realms. By bringing optimization considerations to bear on issues such as why tubes and streams bifurcate, Bejan can rationalize the geometry of all these structures. But it is important to stress that the optimization at issue is one without search; it is emphatically not the sort of optimization that ultra-Darwinists like Dawkins advocate (see e.g. Dawkins 1982). For them, optimal structures are the inevitable result of trials-and-errors over a very long period. Ultra-Darwinists study the slow making of the ﬁttest, whereas Bejan studies the spontaneous emergence of the best. (It is interesting to note that the very same optimization considerations led Bejan in recent work to vindicate Galileo’s intuition that animal locomotion is optimal; see Bejan and Marden 2006.)
Bejan explicitly sees his work as consolidating Leibniz’s intuition that of all the possible processes, the only ones that actually occur (spontaneously) are those that involve minimum expenditure of “work (action).” Reinforcing

Darwin’s Problem in the Context of Cartesian Biolinguistics 57
rationalist tenets, Bejan stresses that only explanations of this kind—ones that appeal to nature laws—enable the scientist to make better sense (I would say, perfect sense) of the object of study. Only the appeal to general laws lends a certain sense of inevitability to the explanation, and hence a certain sense of genuine satisfaction, to the explanation—a sense that one has gone beyond explanatory adequacy, an indication that nature can be understood more simply, a sign that it is not chance, but necessity alone that has fashioned organisms.
In sum, Bejan’s work suggests that there is at least one way in which the shape of Merge can be understood as the optimal distribution of terminals (“an optimal distribution of imperfections,” as Bejan puts it), the result of optimization of lexical access. Like traﬃc patterns, Y-shaped phrase structure representations seek to spread out ﬂowing material to avoid bottleneck eﬀects. This need not be coded in the genome. As soon as Merge is available (as soon as edge features/lexical envelopes have emerged), it would follow as a matter of course that Merge will exhibit a binary branching character if the FL is optimally designed.
3.3 The Seat of Humaniqueness and the Ascent of Homo combinans
In this section I would like to return to the process of lexicalization, the key event in the evolution of the FLN, and suggest that it may be the source of Man’s unique abilities (humaniqueness, as Hauser felicitously dubbed it), that Great Leap Forward that gave us our niche. It is commonly assumed that the key evolutionary step that gave us our distinctness is cognitive in nature.20 Accordingly, the quest for humaniqueness amounts to identifying the factors that make human cognition special. In the words of Hauser (2008),
[a]nimals share many of the building blocks that comprise human thought, but paradoxically, there is a great cognitive gap between humans and animals. By looking at key diﬀerences in cognitive abilities, [we hope to] ﬁnd the elements of human cognition that are uniquely human. The challenge is to identify which systems animals and human share, which are unique, and how these systems interact and interface with one another.
The program can be seen as an extension of Hauser et al. (2002), from the FLN to HCN (Human Cognition in the Narrow sense; that which is speciﬁc and
20 Needless to say, it is perfectly possible that our special cognitive features are the result of more basic anatomical changes. Tattersall (1998) even suggests that the emergence of language was the result of epigenetic processes.

58 Cedric Boeckx
unique to human cognition), or (building on Fodor 1975), LOTN (Language of Thought Narrow).
Hauser presents four evolved mechanisms of human thought that give us access to a wide range of information and the ability to ﬁnd creative solutions to new problems based on access to this information:
1. the ability to combine and recombine diﬀerent types of information and knowledge in order to gain new understanding;
2. to apply the same rule or solution to one problem to a diﬀerent and new situation;
3. to create and easily understand symbolic representations of computation and sensory input; and
4. to detach modes of thought from raw sensory and perceptual input.
Details of formulation aside, Hauser’s hypothesis is a very familiar one. The essence of Hauser’s claim really goes back to the Descartes’s fascination with human cognitive ﬂexibility, its ﬂuidity, its detachment from perception, and its unbounded character—in short, its creative character. This is what led the Cartesians to claim that Man has no instinct, by which they meant that Man’s cognitive faculties rise above the hic and nunc. This was clear to Konrad Lorenz as well, who said that “man is a specialist in not being specialized” (Lorenz 1959). As Marc Hauser likes to put it, while other animals display laser-beam-like intelligence (highly precise speciﬁcity), humans intelligence is ﬂoodlight-like (generalized speciﬁcity) in character. Tattersall (1998: 197) calls it “the human noncondition” and writes:
. . . [O]ver millenia now, philosophers and theologians have made something of an industry of debating the human condition. Even if inevitable, it is rather ironic that the very species that apparently so much enjoys agonizing over its own condition is, in fact, the only species that doesn’t have one—or at any rate, whose condition, if any, is most diﬃcult to deﬁne. Whatever condition is, it is surely a lot easier to specify it in the case of an amoeba, or a lizard, or a shrew, or even a chimpanzee, than it is in our own.
Elsewhere (p. 207), Tattersall notes that in our case, “natural selection has gone for ‘ﬂexibility’ instead of speciﬁcity in behavior” (something which one may attempt to relate to Gould’s 1977 discussion of “neoteny”).
To be sure, scientists have found that some animals think in ways that were once considered unique to humans. For example, some animals have episodic memory, or non-linguistic mathematical ability, or the capacity to navigate using landmarks. In sum, animals have a rich mental life, full of modules or what Liz Spelke calls “core knowledge systems.” What Man seems to have in

Darwin’s Problem in the Context of Cartesian Biolinguistics 59
addition is the ability to systematically transcend the boundaries of modular thought and engage in cross-modular concept formation.
I would like to claim that this ability of building bridges across modules is directly related to language, speciﬁcally the ability to lexicalize concepts (uprooting them from their modules) and combine them freely via Merge.
I am by no means the ﬁrst to speculate along these lines. Spelke (2003), Carruthers (2006), Pietroski (2007), Tattersall (1998), Chomsky (2005), and, to some extent, Mithen (1996), all agree with Descartes that language plays a signiﬁcant role in human cognition. Darwin himself appears to be in agreement when he writes in The Descent of Man,
If it could be proved that certain high mental powers, such as the formation of general concepts, self-consciousness, etc., were absolutely peculiar to man, which seems extremely doubtful, it is not improbable that these qualities are merely the incidental results of other highly-advanced intellectual faculties; and these again mainly the result of the continued use of a perfect language. (p. 126)
The emergence of lexical items was the sort of perfect storm that gave Man his niche. Once concepts are dissociated from their conceptual sources by means of a lexical envelope, the mind truly becomes algebraic and stimulus-free.
The creation of the human lexicon, which, if I am correct, goes hand in hand with Merge, is what lies behind the creative aspect of our thought process, which fascinated both Descartes and Chomsky. Edge features are the set of humaniqueness. With language, creativity emerged, understood (as did Arthur Koestler) as “the sudden, interlocking of two previously unrelated skills or matrices of thought,” an almost limitless capacity for imagination, metaphorical extension, etc.21 Note that one need not follow Hauser (2008) in positing four distinct mechanisms to account for humaniqueness. One key event (the emergence of edge features) suﬃces. Going back to Hauser’s four ingredients for human speciﬁcity listed above, we can now claim that by means of lexical envelopes, humans are able to “detach modes of thought from raw sensory and perceptual input,” and lexicalize at will (“create and easily understand symbolic representations of computation and sensory input”). Via Merge, humans have “the ability to combine and recombine diﬀerent types of information and knowledge in order to gain new understanding, and apply the same rule or solution to one problem to a diﬀerent and new situation.”
21 It may be interesting to note that Mithen’s (1996) characterization of the evolution of mind in three stages—primitive general intelligence, specialized intelligence (modules), and cross-modular intelligence—mirrors Fitch’s (2008) three stages of consciousness. Perhaps one may attempt to make precise the link between language and consciousness that Jackendoﬀ (1987, 2007) has tried to establish.

60 Cedric Boeckx
With edge features and Merge, the human mind became capable of true Swiss-army-knife style cognition. Before that the tools at the animal’s disposal were exquisitely tuned to their tasks, but too isolated. Their eﬀects could only be combined sequentially; they could not be seamlessly and smoothly integrated with one another. With language, the human mind developed into a key ring, where all keys (concepts) can be combined and available at once, thanks to the hole (edge feature) that they all share.
One could say that the ability to endow a concept with an edge feature was, to paraphrase Armstrong, a relatively small step for a man, but a giant leap for mind-kind (and mankind). As Dennett (1996: 17) puts it (in agreement with the intuition behind Cartesian dualism), “perhaps the kind of mind you get when you add language to it is so diﬀerent from the kind of mind you can have without language that calling them both minds is a mistake.”
Merge/edge features gave Man a truly general language of thought, a lingua franca, where previously there were only modular, mutually incomprehensible, dialects/(proto-)languages of thoughts. It signiﬁcantly altered Man’s conceptual structures—how humans think the world. By merging lexicalized concepts, Man was able to hold in mind concepts of concepts, representations of representations, and associations of associations. Homo became Homo combinans.
The result of the emergence of the FLN was a creative, cultural explosion well attested in the archeological record (art, symbol, music, notation, feelings of mystery, mastery of diverse materials, true innovation in toolmaking, sheer cleverness), a “quantum leap,” as Tattersall (1998) calls it.
I agree with Tattersall (1998: 171) that “it is very hard to avoid the conclusion that articulate language is quite intimately tied up with all the other mysterious and often unfathomable aspects of modern human behavior.”22 Tattersall (1998: 186, 228) notes further,
Almost all of the unique cognitive attributes that so strongly characterize modern humans—and that undoubtedly also distinguished our fellow Homo sapiens who eliminated the Neanderthals—are tied up in some way with language. Language both permits and requires an ability to produce symbol in the mind, which can be reshuﬄed and organized by the generative capacity that seems to be unique to our species. Thought as we know it depends on the mental manipulation of such symbols, which are arbitrary representations of features belonging to both the internal and outside world.
22 Tattersall is quite careful, as is Chomsky (see Chomsky 2008) to distinguish between language [the FLN] and speech, which he regards as an “adjunct” (p. 186) to language. It is quite possible that this adjunction was a recruitment of old circuits specialized in externalization of the sort we ﬁnd in songbirds. For relevant discussion, see Piattelli-Palmarini and Uriagereka (this volume) on the possible role of FOXP2 in this respect.

Darwin’s Problem in the Context of Cartesian Biolinguistics 61
. . . virtually any component of our ratiocinative capacities you can name—from our sense of humor to our ability to entertain apocalyptic visions—is based on those same mental abilities that permit us to generate language
Through Merge/edge features, we became the symbolic species—a transformative experience that is directly reﬂected in the archeological records. As Camps and Uriagereka (2006) have noted, aspects of Homo sapiens’s toolmaking seem to require the kind of mental computation that are distinctive of the FLN (the discussion in Mithen 1996: 76 can also be interpreted in this way).
Monboddo—one of the forefathers of evolutionary thought—was clearly correct in his belief that language is “necessarily connected with an[y] inquiry into the original nature of Man.” As Tattersall (1998: 58) writes, “universal among modern humans, language is the most evident of all our uniqueness.” Tattersall goes on to note (p. 68) that our closest relatives “do not display ‘generativity,’ the capacity that allows us to assemble words into statements or ideas into products.” It seems to me that edge features are a good candidate for the source of this very generativity, and humaniqueness.
3.4 On the Importance of Distinguishing between Language and the Language Faculty
The scenario I have sketched does not account for all aspects of the FL, but it surely alleviates the explanatory burden that evolutionary hypotheses face in light of the rapid emergence and spread of “Merge-man” (Homo combinans). And perhaps it is just as well that the hypothesis I have sketched does not cover all aspects of what we would call language, because it is not at all clear that uniﬁcation of all aspects of language is desirable. Language is almost surely not a natural object. It is an object of our folk psychology/biology. The FL is the real object of study. The FL, like the hand, the nose, and other properties of our organism, is put to use in countless ways, and it would be foolish to expect any theory to capture all these aspects. The term “language” is a remnant of philological thinking; ignoring the basic distinction between competence and performance is bound to lead to the claim that language is messy or klugy. Instead, if one focuses on the language organ, I think that signs of good design emerge very quickly.
This said, the discussion above has left out important aspects traditionally associated with the FL. For example, I have said very little about the way linguistic expressions are externalized. I have tacitly assumed the correctness of a long-standing intuition (already present in the writings of the Cartesians)

62 Cedric Boeckx
that the FL’s core function is basically that of providing a syntax of thought (Language of Thought in the narrrow sense, LOTN). As Chomsky (2008: 136) remarks,
It may be that there is a basic asymmetry in the contribution to language design of the two interface systems: the primary contribution to the structure of [the] F[aculty of] L[anguage] may be optimization of the C-I [sense] interface.
From this perspective, externalization appears to be an addendum—(quite literally) an afterthought. This may explain why the nature of morphophonology appears to be radically diﬀerent (see Bromberger and Halle 1989, Blevins 2004). This is not to say that morpho-phonology is not part of language. It clearly is. But it is just that the syntax–phonology connection is much looser, and less constitutive of the FL than the mapping from syntax to thought.
I also said nothing regarding the size of the lexicon, or the way words are acquired. Although I have argued that the nature of lexicalization is crucial to the FLN, I do not think that the size of the lexicon or some of the strategies used in its acquisition must be regarded as unique to language (or speciﬁc to humans). Clearly, syntax plays a role in carving the path of acquisition to some extent (as Lila Gleitman and colleagues have revealed over the years; Gleitman et al. (2005); see also Borer 2005), but other aspects, not speciﬁcally linguistic, surely come into play (see Bloom 2000). As for the size of our vocabulary, it is likely to be tied to our ability to imitate—unique to us, but not speciﬁc to language (Hauser 1996, 2000; Tomasello 1999).
Finally, there is another aspect of the FL that I have not touched on and that many would have regarded as central to language (indeed part of the FLN) until recently, and that is parameters—the points of variation pre-deﬁned by Universal Grammar, the open variables in universal linguistic principles. I argue in Boeckx (this volume) that perhaps UG is not as overspeciﬁed as traditionally thought in generative grammar. There is indeed a case to be made that parameters are not constituents of the FLN. Rather, they may be the necessary result of a very impoverished, underspeciﬁed system (that is to say, some parametric eﬀects may be epigenetic). Let me illustrate what I mean with one clear example. Take the fact that phrases are headed (a property known as endocentricity). Under most accounts (but see Kayne 1994), phrases in natural language are assumed to be underspeciﬁed with regard to the directionality of headedness. One can formulate this as a principle with open values, as in “syntactic projections must be [right/left] headed.” From the acquisition standpoint, children know that projections have to be headed, but have to ﬁgure out whether their language is head-initial or head-ﬁnal. They must pick

Darwin’s Problem in the Context of Cartesian Biolinguistics 63
one or the other option. However, note that instead of building the parametric variation into linguistic principles (FLN), we could get the same result from letting it follow (logically) from the fact that each phrase must be linearized, and, since externalization proceeds through a narrow, one-dimensional channel, the physics of speech will force the very choice that the head parameter is supposed to code for.
Generally speaking, I believe that a more minimalist understanding of parameters ought to move underspeciﬁcation from the domain of individual principles to the system as a whole. It is because the system is not richly speciﬁed in the genome, because the FLN is so minimalist, that not every aspect of the FL is ﬁxed once and for all, and therefore variation is expected. This view on variation meshes well with Kirschner and Gerhart’s (2005) theory that variation results from underspeciﬁed parts of system that allow organism to explore the fullness of space pre-deﬁned by their ﬂexibly connected constituents, freeing the genome from registering rigidly the space of variation. The same logic underlies the temporal retardation of development, especially prominent in humans—with absence of early rigidiﬁcation leading to bigger potential (neoteny; see Gould 1977). Less is indeed more.23 Once available, non-genomic parametric dimensions would be made use of, in the same way that colored patterns on wings are used in butterﬂies to reinforce group boundaries, preventing possible interbreeding (a state of artiﬁcal speciation) (see Lukhtanov et al. 2005).
3.5 By Way of a Conclusion
The formulation of concrete minimalist hypotheses, combined with recent developments in genetics that re-establish the balance between genes, environment, and organism (Lewontin’s 2000 triple helix) and allow us to extrapolate back to our ancestors when the fossil record fails (see Kirschner and Gerhart 2005, Wade 2006), should enable us to not only regard the muchpublicized 1866 ban imposed by the Linguistic Society of Paris on any debate concerning the origin of language as passé, but more importantly relieve Lewontin’s (1998) unremitting attack on the plausibility of evolutionary studies of cognitive faculties. I will not review Lewontin’s points in detail here. Suﬃce it to say that he saw the problem of reconstruction in the absence of
23 This view also ﬁts in well with recent attempts to minimize the structure and format of parameters by reducing their size and macroproperties (see Kayne 2005b). It also ﬁts well with recent criticism leveled at (macro-)parametric approaches (see Culicover 1999, Newmeyer 2005, who, unfortunately, made the mistake of throwing away the baby with the bathwater; for further discussion, see Boeckx, in progress).

64 Cedric Boeckx
record as insuperable, and therefore all evolutionary account of cognition as Just-So stories. In his own words (p. 130):
the best lesson . . . is to give up the childish notion that everything that is interesting about nature can be understood. . . . It might be interesting to know how cognition . . . arose and spread and changed, but we cannot know. Tough luck.
It is hard to dismiss the views of giants like Lewontin, but those interested in Darwin’s problem will no doubt ﬁnd comfort in the following assertion made by Darwin himself in The Descent of Man:
It has often and conﬁdently been asserted, that man’s origin can never be known: but ignorance more frequently begets conﬁdence than does knowledge: it is those who know little, and not those who know much, who so positively assert that this or that problem will never be solved by science. (p. 3)
One could indeed point to many episodes where the end of a ﬁeld has been announced right before important advances took this ﬁeld to new heights. Perhaps we will never know for sure whether certain aspects of the FL emerged via adaptation or some other means, but already now it seems we can make some educated guesses about and some progress toward understanding how the FL could have evolved. But there is no doubt that if we are to vindicate Darwin’s view, the task that lies ahead will be supremely interdisciplinary and therefore extremely hard. To address Darwin’s problem, it will be imperative to ask—simultaneously—distinct, but inextricably related questions, which I borrow from Piattelli-Palmarini and Uriagereka (2005): What is language such that it may have evolved? and, What is evolution such that it may have applied to language?
Jackendoﬀ (2002) is right: One’s view on the evolution of language depends on one’s view of language. But it also depends on one’s view on evolution, which too many students of language have taken to be ﬁxed along Dawkinsian lines. The point here is not to revive creationism or promote intelligent design. It is simply an attempt to strike a non-dogmatic balance between Form and Function, between Necessity and Chance. What I ﬁnd most fascinating, from the perspective of a linguist, is that seemingly arcane discussions about Merge and the nature of lexicalization may contribute to our understanding of evolutionary theory. The language organ may become a model organism in the context of an extended modern synthesis in biology—extremely useful because surprisingly simple and optimal in its design.

4
Syntax Facit Saltum Redux: Biolinguistics and the Leap to Syntax
ROBERT C. BERWICK
I have been astonished how rarely an organ can be named, towards which no transitional grade is known to lead. The truth of this remark is indeed shown by that old canon in natural history of Natura non facit saltum. We meet with this admission in the writings of almost every experienced naturalist; or, as Milne Edwards has well expressed it, nature is prodigal in variety, but niggard in innovation. Why, on the theory of Creation, should this be so? . . . Why should not Nature have taken a leap from structure to structure? On the theory of natural selection, we can clearly understand why she should not; for natural selection can act only by taking advantage of slight successive variations; she can never take a leap, but must advance by the shortest and slowest steps. (Darwin 1859: 194)
4.1 Introduction: Language, Biology, and the Evolution–Language Gaps For hundreds of years, long before Darwin’s publication of Origin of Species (1859) and his Descent of Man (1871) that explicitly brought language into the fold of modern evolutionary thinking, the evolution of language has captured the imagination of biologists and linguists alike. Among many evolutionary puzzles, one that stands out is the obvious discontinuity between the human species and all other organisms: language is evidently unique to the human lineage—being careful here to deﬁne language properly as distinct
This research was supported by NSF grant 9217041-ASC and ARPA under the HPCC program. Noam Chomsky, Samuel Epstein, Anna Maria Di Sciullo, Charles Yang, and Morris Halle provided many valuable comments; all remaining errors are our own.

66 Robert Berwick
from general communication, a matter addressed in Berwick and Chomsky (this volume) and considered further below.
Such gaps, or evolutionary novelties, have always posed a challenge to classical Darwinian analysis, since that theory is grounded fundamentally on the notion of gradualism—incrementally ﬁne steps leading from a trait’s precursor, with adaptive, functioning intermediates at every step along the evolutionary path, ultimately culminating in an “organ of extreme complexity and perfection,” as with the vertebrate eye. Add one extra layer of light-sensitive membrane, so the argument goes, and an eye’s photon-trapping improves by a fractional percent—a smooth incline with no jumps or surprises. Indeed Darwin himself devoted considerable eﬀort in Origin to analyzing precisely this problem with this assumption (Chapter VI, “Organs of extreme perfection”), using the evolution of the eye as his make-or-break case study for his gradualist model, the very heart and soul of the theory itself, since, as he himself insists, “If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modiﬁcations, my theory would absolutely break down” (1859: 189):
To suppose that the eye, with all its inimitable contrivances for adjusting the focus to diﬀerent distances, for admitting diﬀerent amounts of light, and for the correction of spherical and chromatic aberration, could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree. Yet reason tells me, that if numerous gradations from a perfect and complex eye to one very imperfect and simple, each grade being useful to its possessor, can be shown to exist; if further, the eye does vary ever so slightly, and the variations be inherited, which is certainly the case; and if any variation or modiﬁcation in the organ be ever useful to an animal under changing conditions of life, then the diﬃculty of believing that a perfect and complex eye could be formed by natural selection, though insuperable by our imagination, can hardly be considered real. (1859: 186)
From this perspective, human language seems to be exactly the kind of unwanted biological surprise Darwin sought to avoid. Indeed, it apparently stands squarely as a counter-example to his entire theory of evolution via descent with modiﬁcation, at least if we are to take Darwin at his word. Perhaps that is why ever since Darwin nearly all researchers seem to abhor even the slightest hint of a non-gradual, non-adaptationist account of human language evolution.
It is perhaps worth recollecting that this picture of evolution-as-minuteaccumulation of small changes was not always so strongly embraced. As the evolutionary theorist Allan Orr notes in a recent review (2005a), in the late nineteenth and early twentieth centuries, Mendelians like William Bateson argued that the “micromutational” view was simply a path of least eﬀort:

Biolinguistics and the Leap to Syntax 67
“By suggesting that the steps through which an adaptive mechanism arises are indeﬁnite and insensible, all further trouble is spared. While it could be said that species arise by an insensible and imperceptible process of variation, there was clearly no use in tiring ourselves by trying to perceive that process. This labor-saving counsel found great favor” (Bateson 1909). The gradualist position became the dominant paradigm in the ﬁeld in the 1930s via the analytical uniﬁcation marrying Mendelism to Darwinism forged by R. A. Fisher, S. Wright, and J. B. S. Haldane, dubbed the “Modern Synthesis”: on this view, micro-mutational particulate events—tiny changes, perhaps at the level of single nucleotides (the DNA “letters” Adenine, Thymine, Guanine, and Cytosine), and correspondingly small steps in allele (gene variant) frequencies— comprise the bulk of evolutionary change. The Modern Synthesis too suggests that a Big-Bang emergence of language might be quite unlikely.
How then can one reconcile evolutionary theory’s Modern Synthesis with the apparent discontinuity, species-speciﬁcity, and distinctive syntactic competence of human language? A familiar line of thinking simply denies that a gap exists at all: other hominids also possess human syntactic abilities. A second position embraces the Modern Synthesis and rejects discontinuity, asserting that all the particular properties of human language have been speciﬁcally selected for as directly adaptive, with small gradual changes leading from non-language using ancestors to the present, a position perhaps most strongly advocated by Pinker and Bloom (1990). Other researchers deny that there are properties proprietary to human syntax, instead grounding them on principles of general purpose cognition, like those found in connectionism (Rummelhart and McClelland 1987). Still other approaches call for development from a proto-language to full language (Bickerton 1990). Perhaps the only proposals made previously that avoid an outright appeal to gradualism are those that involve exaptation in the sense of Gould and Vrba (1982), or genetic draft (Gillespie 1991)—that human language hitchhiked on the back of a related, already adaptively advantageous cognitive subsystem, such as motor-gesture articulation, hierarchical tool-making, or social grooming. While these last approaches remain possibilities within a classical gradualist framework, they all focus on some external aspect of language-as-communication, rather than internal syntax tout court, a matter discussed elsewhere (Berwick and Chomsky, this volume). Berwick and Chomsky note that all recent relevant biological and evolutionary research leads to the conclusion that the process of externalization is secondary, subsequent to conceptualization and the core principles of human syntax. If this conclusion is on the right track, we are left with the same puzzling gap as before.

68 Robert Berwick
Besides this unsettling discontinuity, human language poses a second challenging gap, a classic and also familiar biological one: how to bridge between a genotype and a phenotype, in this case perhaps the most complex behavioral phenotype we know. Linguistics has cast natural language’s intricate and ultimately behavioral “form that shows” (its phenotype) at an abstract level, far removed from language’s computational and biological “inner form” or genotype. Linguistic science’s successful program over the past ﬁfty years has resulted in perhaps the richest description of a human genotype-to-phenotype mapping that we know of—the initial substrate for human language and how language develops in an individual.
However, until recently progress at bridging from the language genotype to phenotype has been diﬃcult. In part this is because we simply do not know much about the human language genotype, and we often run aground by misidentifying the language phenotype with communication, as mentioned above. Further, the gulf separating computation, biology, and language has been equally long-standing—in large measure resulting from the abstraction gap between linguistic and biological description: we do not expect to literally ﬁnd a “passive grammar rule” inside a person’s head. The history of the ﬁeld here from Fodor, Bever, and Garrett’s summary of work from the 1960s (1974) to Berwick and Weinberg (1986), Di Sciullo (2000), Phillips (2003), Reinhart (2006), and many others might be read as one long attempt to ﬁnd moreto-less isomorphic mappings between linguistic rules and representations and computational rules and representations.
In this chapter we show how to resolve both the evolutionary and the genotype–phenotype gaps in a new way, emphasizing that the speciesspeciﬁcity and novelty of human language need not conﬂict with Darwinian thinking—indeed, that modern evolutionary theorizing and discoveries have moved past the “micro-mutational” gradualist view so as to become quite compatible with modern linguistic theory, with both linguistic theory and evolutionary theory contributing theoretical insights to each other. This synergy is not a new development. It is one that itself has evolved over the past two decades: the Principles-and-Parameters (P&P) approach to language (Chomsky 1981) was directly inspired by the biologist Jacob’s (1977) remarks about how the apparent diversity of biological forms might be produced by an underlying parameterization of abstract genetic regulatory switches, a view that Chomsky then imported into linguistic theory. On the biological side, the so-called evo–devo [evolution–development] revolution (see Carroll, Grenier, and Weatherbee 2001; Carroll 2005a; Chomsky 2007c; Müller 2007), along with new results on genomic and regulatory networks, new simulation results on the apparently much larger size of adaptive mutational change

Biolinguistics and the Leap to Syntax 69
(Orr 2002, 2005a), and a more widespread acknowledgment of homeoplasy or horizontal gene transfer (Warren et al. 2008), have moved evolutionary theory well past the Fisher–Wright gradualist, particulate, micro-mutational view. We shall see below that historically developments in both linguistic theory and evolutionary theory are in many respects parallel and mutually reinforcing.
To resolve these two gaps, in this chapter we move beyond the 1980s viewpoint on both the biological and linguistic fronts, showing how the most recent developments in linguistic research, dubbed the Minimalist Program (MP) (Chomsky 1995, 2005, 2007a) can bridge the biology–language divide. The MP demonstrates that despite its apparent surface complexity, language’s core might in fact be much simpler than has previously been supposed. For biologists pursuing clues left by the linguistic phenotype’s fault lines down to the level of the real genotype, this is a promising development. The reﬁnement of our understanding of the linguistic phenotype comes at a particularly apt time, since in the last decade there has been an ever-growing, though still small, range of work on genetics and language, as exempliﬁed in the work of Gopnik and colleagues (Gopnik 1990), and many others since, including the extensive recent ﬁndings on locating speciﬁc genetic variation in language, namely, the mutations in the FOXP2 gene (Marcus and Fisher 2003; Enard et al. 2002). (But see Berwick and Chomsky, this volume, for a critique of naive genetical interpretations of single mutations.) This is because the Minimalist Program is eliminative in exactly the right way, and so can serve as a case study for how a complex behavioral phenotype emerges from the interactions of a much simpler genotype. In particular, the Minimalist Program posits that the human syntactic engine consists of just two components: (1) words and word features; and (2) a single, simple recursive operation, Merge, that glues together words and word complexes into larger units.
This chapter demonstrates how just these two components, without further stipulation, interact to yield many, perhaps all, of the special design features of human language syntax. If this is so, then we have no need for speciﬁc, adaptive accounts of these particular features. By design features we mean familiar properties of human language syntax such as the following:
r digital inﬁnity and recursive generative capacity, the familiar “inﬁnite use of ﬁnite means”: sentences may be arbitrarily long and novel; there are 1-, 2-, . . . word sentences, but there are no 51/2 -word sentences;
r displacement: human languages move phrases from their natural argument positions, as in This student, I want to solve the problem where the subject

70 Robert Berwick
of the verb, solve, namely This student, appears at the front of the sentence instead of in its normal position after the verb; r locality constraints: displacement does not act over unbounded domains— in Who do you wonder Bill thinks solved the problem, who cannot be interpreted as the subject of solve; r restricted grammatical relations: out of a potentially inﬁnite set of logically possible relations that might be deﬁned over conﬁgurations of syntactic structures, only a handful ever seem to play a role in human syntax. For example, human languages often match verbs to objects (in terms of predicate–argument structure); demand agreement between tense/inﬂection and subjects as in the case of subject–verb person–number agreement; or verbs may select either subjects or objects, as in the familiar contrast between John admires honesty/Honesty admires John. Yet most logically possible syntactic rules and relations are unattested—for instance, there is apparently no analog to “object-of,” say subject-object-of, where the subject and object of a sentence must agree.
For the evolutionary biologist seeking to answer why we see this particular distribution of organisms or traits in the natural world and not others—one of the central questions of biology being to reconcile this pattern of variation both present and absent with the apparent commonalities of organisms, just as with language—such a ﬁnding is central. If observed patterns follow from a single, central principle, then there is no need to invoke some special adaptive explanation for any of them. There is no locality “trait” and no grammatical relation trait that must be acquired in an evolutionary piecemeal fashion. One does not need to advance incremental, adaptationist arguments with intermediate steps between some protolanguage and full natural language to explain much, perhaps all, of natural language’s speciﬁc design.
Note that from a logical or communicative standpoint, these particular design properties are otherwise mysterious. For instance, there is no immediately obvious computational or communicative reason why languages ought not to relate subjects and objects. Communicatively, a sentence’s subject, usually an agent, and its object, usually the aﬀected object, form just as natural a class as subject and predicate. Further, as is easy to see from the transitivity of conditional probabilities that can be simply multiplied together, nothing blocks a purely statistical conditional relationship between subject and object. However, it seems that no such connections are to be found in human languages. Indeed this is another clear limitation of the currently popular statistical approach to language description, which otherwise oﬀers no barrier to such unattested relations. Similarly, as pointed out in Berwick and Chomsky (this volume), displacement makes language processing and

Biolinguistics and the Leap to Syntax 71
communication more diﬃcult, not less diﬃcult—yet another argument that language is not designed for communication. The ultimate explanation for language’s design must be, obviously, biological, but on the view here, not at the level of expressiveness or communicative eﬃciency. This chapter oﬀers an alternative, deeper, possibility: the reason why human syntax looks the way it does rather than some other way—why natural languages have an objectof grammatical relation but not a subject-object-of grammatical relation— follows from the fundamental principles of the basic combinatorial syntactic engine itself.1
As to the evolutionary origin of the fundamental combinatory ability itself, Merge, we leave this topic largely unexplored here. Along with the evolutionary theorist G. C. Williams (1996: 77), one might speculate that the hierarchical combinatorial ability possibly appeared just as other evolutionary novelties do: “new structures arise in evolution in one of two ultimate ways, as redundancies or spandrels”—a structure arising as an incidental consequence of some other evolutionary change. Where we part ways with Williams’s classical account is in the nature of evolutionary change itself, as we describe in Section 4.2, below. Berwick and Chomsky (this volume) provide additional details on how a singular event of this kind might arise and spread in a small group, some 50,000 to 100,000 years ago.
This minimalist reformulation also has important consequences for models of language processing, and so ultimately for descriptions of the linguistic phenotype as it is externalized. The most minimal conception of a processor or parser for natural language takes the relation between basic parsing operations and the abstract linguistic system as simply the identity function. As it turns out, this leads to the most eﬃcient processor achievable, left-to-right,
1 We note that it is more diﬃcult than sometimes supposed to give a purely functional communicative justiﬁcation for some of the more patent universal properties of natural-language syntax. For instance, it is sometimes suggested that recursive generative capacity is somehow necessary for communication, thus bridging the gap between protolanguage and recursive human syntax. But is this so? There seem to be existing human languages that evidently possess the ability to form recursive sentences but that apparently do not need to make use of such power: a well-known example is the Australian language Warlpiri, where it has been proposed that a sentence that would be recursively structured in many other languages, such as ‘I think that John is a fool’ is formed via linear concatenation, ‘I ponder it. John is a fool’; or to take a cited example, Yi-rna wita yirripura jaru jukurrpa-warnu wiinyiinypa, literally, ‘little tell-present tense story dreaming hawk,’ translated as ‘I want to tell a little dreaming story about a hawk’ (Nash 1986, Swartz 1988). Evidently then, recursion is not essential to express “the beliefs about the intentional states of others,” quite contrary to what some researchers such as Pinker and Bloom (1990) and more recently Pinker and Jackendoﬀ (2005) have asserted. This seems again to be an example of the confusion between externalization-as-communication and internal syntax. Apparently the same was true of Old English, if the data and linguistic arguments presented in O’Neil (1976) are correct. There is nothing surprising in any of this; it is quite similar in spirit to the observed production/perception limitations on, for example, center-embedded and other diﬃcult-to-process sentences in English.

72 Robert Berwick
real-time, and deterministic to the extent this is possible at all, and at the same time replicates some of human language’s known psychophysical, preferential “blind spots.” For example, in sentence pairs such as John said that the cat died yesterday/John said that the cat will die yesterday, yesterday is (reﬂexively) taken to modify the second verb, the time of the cat’s demise, even though this is semantically defeasible in the second sentence. In this sense, this approach even helps solve the secondary process of externalization (and so communicative eﬃciency, again to the extent that eﬃcient communication is possible at all).
If all this is correct, then current linguistic theory may have now attained a better level of description in order to proceed with evolutionary analysis. In this sense, using familiar Darwinian terms, the syntactic system for human language is indeed, like the eye, an “organ of extreme complexity and perfection.” However, unlike Linnaeus’s and Darwin’s slogan shunning the possibility of discontinuous leaps in species and evolution generally—natura non facit saltum—we advocate a revised motto that turns the original on its head: syntax facit saltum—syntax makes leaps—in this case, because human language’s syntactic phenotype follows from interactions amongst its deeper components, giving it a special character all its own, apparently unique in the biological world.
The remainder of this chapter is organized as follows. Section 4.2 serves as a brief historical review of the recent shift from the “gradualist,” micromutational Modern Synthesis in evolutionary biology to a more ecumenical encompassing the evo–devo revolution and macro-adaptive events. Section 4.3 follows with an outline of parallel shifts in linguistic theory: from highly specialized language-particular rules to more abstract principles that derive large-scale changes in the apparent surface form of syntactic rules. This historical shift has two parts. First, the change from the atomic, languageparticular rules of the Aspects era, the Extended Standard Theory (EST) to a system resembling a genetic regulatory–developmental network, dubbed the Principles-and-Parameters (P&P) theory; and second, the more recent shift to the Minimalist Program. Putting to one side many important details irrelevant for our argument, we outline how sentence derivations work in the MP. By examining the notion of derivation in this system, we demonstrate that all syntactic “design features” in our list above follow ineluctably, including all and only the attested grammatical relations, such as subject-of and objectof. Section 4.4 turns to sentence processing and psychophysical blind spots. It outlines a speciﬁc parsing model for the minimalist system, based on earlier computational models for processing sentences deterministically, strictly left to right. It then shows how reﬂexive processing preferences like the one

Biolinguistics and the Leap to Syntax 73
described above can be accounted for. Section 4.5 concludes with observations on the tension between variation and uniformity in language, summarizing the evolutionary leap to syntax.
4.2 Ever since Darwin: The Rise and Fall of Atomism in Modern Evolutionary Biology
As we noted in the introduction, by the mid-1930s the micro-mutational, atomistic picture of evolution by natural selection held sway, admirably uniﬁed on the one hand by particulate Mendelism and on the other hand by Darwinism, all uniﬁed by the “inﬁnitesimal” mathematical theory established by Fisher, Wright, and Haldane. However, by the late decades of the twentieth century and on into the twenty-ﬁrst, this Modern Synthesis paradigm has undergone substantial revision, due to advances on three fronts: (1) the evo– devo revolution in our understanding of deep homologies in development and underlying uniformity of all organisms; (2) an acknowledgment of a more widespread occurrence of symbiotic evolutionary events and homeoplasy; and (3) the erosion of the Fisher-inspired adaptationism-as-incrementalism model.
First, the evo–devo revolution has demonstrated that quite radical changes can arise from very slight changes in genomic/developmental systems. The evolution of eyes provides a now-classic example, turning Darwin’s view on its head. While perhaps the most famous advocate of the Modern Synthesis, Ernst Mayr, held that it was quite remarkable that 40 to 60 diﬀerent eyes had evolved separately, thus apparently conﬁrming that micro-mutational evolution by natural selection had a stunning ability to attain “organs of perfection” despite radically diﬀerent starting points and diﬀerent contexts, in reality there are very few types of eye, perhaps even monophyletic (evolving only once) in part because of constraints imposed by the physics of light, in part because only one category of proteins, opsin molecules, can perform the necessary functions (Salvini-Plawen and Mayr 1961; Gehring 2005). Indeed, this example almost exactly parallels the discoveries in linguistic theory during the 1980s, that the apparent surface variation among languages as distinct as Japanese, English, Italian, and so forth are all to be accounted for by a richly interacting set of principles, parameterized in just a few ways, that deductively interact to yield the apparent diversity of surface linguistic forms, as described in the next section.
This move from a highly linear model—compatible with a “gradualist, incrementalist” view—to a more highly interconnected set of principles where a slight change in a deep regulatory switch can lead to a quite radical

74 Robert Berwick
surface change, from vertebrate eyes to insect eyes—follows the same logic in both linguistics and biology. The parallel is much more than superﬁcial. Gehring (2005) notes that there is a contrast to be made in the evolution of biosynthetic pathways, for instance for histidine, as proposed by Horowitz (1945), as opposed to the evolution of morphogenetic pathways, as for the vertebrate eye. In the case of biosynthesis, the linear model can proceed backwards: at ﬁrst, histidine must be directly absorbed from the environment. When the supply of histidine is exhausted, then those organisms possessing an enzyme that can carry out the very last step in the pathway to synthesize histidine from its immediate precursor are the Darwinian survivors. This process extends backwards, step by step, linearly and incrementally. We might proﬁtably compare this approach to the one-rule-at-a-time acquisition that was adopted in the Wexler and Culicover (1980) model of acquisition in the similarly atomistic linguistic theory of the time.
However, quite a diﬀerent model seems to be required for eye morphogenesis, one that goes well beyond the incremental, single nucleotide changes invoked by the Modern Synthesis. Here the interplay of genetic and regulatory factors seems to be intercalated, highly interwoven as Gehring remarks. That much already goes well beyond a strictly micromutational, incremental view. The intercalation resembles nothing so much as the logical dependencies in the linguistic P&P theory, as illustrated in Figure 4.1; see p. 82. But there is more. Gehring argues that the original novelty itself—a photoreceptor next to a pigment cell—was a “purely stochastic event,” and further, that there is reasonable evidence from genome analysis that it might in part be due to symbiosis—the wholesale incorporation of many genes into an animal (Eukaryotic) cell by ingestion of a chloroplast from Volvox, a cyanobacter. Needless to say, this completely bypasses the ordinary step-by-step nucleotide changes invoked by the Fisher model, and constitutes the second major discovery that has required re-thinking of the gradualist, atomistic view of evolution:
The eye prototype, which is due to a purely stochastic event that assembles a photoreceptor and a pigment cell into a visual organ, requires the function of at least two classes of genes, a master control gene, Pax6, and the structural genes encoding on rhodopsin, for instance, the top and the bottom of the genetic cascade. Starting from such a prototype increasingly more sophisticated eye types arose by recruiting additional genes into the morphogenetic pathway. At least two mechanisms of recruitment are known that lead to the intercalation of additional genes into the genetic cascade. These mechanisms are gene duplication and enhancer fusion. . . . For the origin of metazoan photoreceptor cells I have put forward two hypotheses: one based on cell diﬀerentiation and a more speculative model based on symbiosis.
(Gehring 2005: 180; emphasis added)

Biolinguistics and the Leap to Syntax 75
The outcome is a richly interconnected set of regulatory elements, just as in the P&P theory. Further, we should note that if the origin of the original novelty assembling photoreceptor and pigment cell is purely stochastic then the “cosmic ray” theory of the origin of Merge, sometimes derided, might deserve more serious consideration, though of course such a view must remain entirely speculative.
Second, the leaps enabled by symbiosis seem much more widespread than has been previously appreciated. As Lynn Margulis—the biologist who did much to establish the ﬁnding that cellular mitochondria with their own genes were once independent organisms ingested symbiotically—has often remarked, “the fastest way to get new genes is to eat them.” To cite yet another recent example here out of many that are quickly accumulating as wholesale genomic analysis accumulates, the sequencing of the duckbill platypus Ornithorhynchus anatinus genome has revealed a substantial number of such horizontal transfers of genes from birds and other species whose most recent common ancestor with the platypus is extremely ancient (Warren et al. 2008). (Perhaps much to the biologists’ relief, the crossover from birds does not seem to include the genes involved in the platypus’s duck bill.)
Of course, at the lowest level, by and large genomic changes must of necessity be particulate in a strong sense: either one DNA letter, one nucleotide, changes or it does not; but the by-and-large caveat has loomed ever larger in importance as more and more evidence accumulates for the transmission of genes from species to species horizontally, presumably by phagocytosis, mosquito-borne viral transmission, or similar processes, without direct selection and the vertical transmission that Darwin insisted upon.
The third biological advance of the past several decades that has eroded the micro-mutational worldview is more theoretical in character: Fisher’s original mathematical arguments for ﬁne-grained adaptive change have required substantial revision. Why is this so? If evolutionary hills have gentle slopes, then inching uphill always works. That follows Fisher chapter and verse: picture each gene that contributes to better eyesight as if it were one of millions upon millions of ﬁne sand grains. Piling up all that sand automatically produces a neatly conical sand pile with just one peak, a smooth mound to climb. In this way, complex adaptations such as the eye can always come about via a sequence of extremely small, additive changes to their individual parts, each change selectively advantageous and so seized on by natural selection.
The key question is whether the biological world really works this way, or rather, how often it works this way. And that question divides into two parts. Theoretically speaking: what works better as the raw material or “step size”

76 Robert Berwick
for adaptation—countless genes each contributing a tiny eﬀect, or a handful of genes of intermediate or large eﬀect? Empirically speaking: how does adaptation really play out in the biological world? Are large mutations really always harmful, as Fisher argued? Do organisms usually tiptoe in the adaptive landscape or take larger strides? Are adaptive landscapes usually smooth sand piles, jagged alpine ranges, or something in between?
Fisher addressed the theoretical question via a mathematical version of the familiar “monkey wrench” argument: a large mutation would be much more likely than a small one to gum up the works of a complex, ﬁnely constructed instrument like a microscope, much as a monkey randomly ﬁddling with the buttons on a computer might likely break it. It is not hard to see why. Once one is at a mountain top, a large step is much more likely to lead to free-fall disaster. But the microscope analogy can easily mislead. Fisher’s example considers a mutation’s potential beneﬁts in a particularly simple setting—precisely where there is just one mountain top, and in an inﬁnite population. But if one is astride K90 with Mt. Everest just oﬀ to the left, then a large step might do better to carry me towards the higher peak than a small one. The more an adaptive landscape resembles the Himalayas, with peaks crowded together—a likely consequence of developmental interactions, which crumple the adaptive landscape—the worse for Fisher’s analogy. Small wonder then that Dawkins’s topographic maps and the gradual evolutionary computer simulations he invokes constantly alter how mountain heights get measured, resorting to a single factor—ﬁrst for eyes, it’s visual resolution; next, for spider webs, it is insect-trapping eﬀectiveness; then, for insect wings, it is aerodynamic lift or temperature-regulating ability. An appropriate move, since hill-climbing is guaranteed to work only if there is exactly one peak and one proxy for ﬁtness that can be optimized, one dimension at a time.
Even assuming a single adaptive peak, Fisher’s microscope analogy focused on only half the evolutionary equation—variation in individuals, essentially the jet fuel that evolution burns—and not the other half—the selective engine that sifts variations and determines which remain written in the book of life. Some ﬁfty years after Fisher, the population biologist Motoo Kimura (1983) noted that most single mutations of small eﬀect do not last: because small changes are only slightly selectively advantageous, they tend to peter out within a few generations (ten or so). Indeed, most mutations, great or small, advantageous or not, go extinct—a fact often brushed aside by pan-adaptationist enthusiasts (see also Berwick and Chomsky, this volume). Kimura calculated that the rate at which a mutation gains a foothold and

Biolinguistics and the Leap to Syntax 77
then sweeps through a population is directly proportional to the joint eﬀect of the probability that the mutation is advantageous and the mutation’s size. Moreover, even if medium-scale changes were less likely to ﬁx in a population than micro-mutations, by deﬁnition a larger change will contribute correspondingly more to an organism’s overall response to natural selection than a small one.
However, Kimura neglected an important point: he calculated this relative gain or loss for just a single mutation, but the acquisition of some (perhaps complex) trait might take several steps. This alters Kimura’s analytical results, as has been more recently studied and thoroughly analyzed by Orr via a combination of mathematical analysis and computational simulations (2002, 2005a), and extended to discrete molecular change in DNA sequence space via other methods by Gillespie (1984). The upshot seems to be that beneﬁcial mutations have exponentially distributed ﬁtness eﬀects—that is,
adaptation is therefore characterized by a pattern of diminishing returns—larger-eﬀect mutations are typically substituted earlier on and smaller-eﬀect ones later, . . . indeed, adaptation seems to be characterized by a ‘Pareto principle’, in which the majority of an eﬀect (increased ﬁtness) is due to a minority of causes (one [nucleotide] substitution).
(Orr 2005a: 122, 125)
Thus, while Kimura did not get the entire story correct, Fisher’s theory must be revised to accommodate the theoretical result that the ﬁrst adaptive step will likely be the largest in eﬀect, with many, many tiny steps coming afterwards. Evidently, adaptive evolution takes much larger strides than had been thought. How then does that connect to linguistic theory? In the next section, we shall see that linguistics followed a somewhat parallel course, abandoning incrementalism and atomistic ﬁne-grained rules, sometimes for the same underlying conceptual reasons.
4.3 Ever since Aspects: The Rise and Fall of Incrementalism in Linguistic Theory
Over the past ﬁfty years, linguistic science has moved steadily from less abstract, naturalistic surface descriptions to more abstract, deeper descriptions—rule systems, or generative grammars. The Minimalist Program can be regarded as the logical endpoint of this evolutionary trajectory. While the need to move away from mere sentence memorization seems clear, the rules that linguists have proposed have sometimes seemed, at least to some, even farther removed from biology or behavior than the sentences they were

78 Robert Berwick
meant to replace. Given our reductionist aim, it is relevant to understand how the Minimalist Program arose out of historical developments of the ﬁeld, partly as a drive towards a descriptive level even farther removed from surface behavior. We therefore begin with a brief review of this historical evolution, dividing this history into two parts: from the Extended Standard Theory of Chomsky’s Aspects of the Theory of Syntax (1965) to the P&P model; and from P&P to the Minimalist Program.
Before setting out, it perhaps should ﬁrst be noted that the ‘abstraction problem’ described in the introduction is not unfamiliar territory to biologists. We might compare the formal computations of generative grammar to Mendel’s Laws as understood around 1900—abstract computations whose physical basis were but dimly understood, yet clearly tied to biology. In this context one might do well to recall Beadle’s comments about Mendel, as cited by Jenkins (2000):
There was no evidence for Mendel’s hypothesis other than his computations and his wildly unconventional application of algebra to botany, which made it diﬃcult for his listeners to understand that these computations were the evidence.
(Beadle and Beadle 1966)
In fact, as we suggest here, the a-biological and a-computational character sometimes (rightly) attributed to generative grammar resulted not because its rules were abstract, but rather because rules were not abstract enough. Indeed, this very fact was duly noted by the leading psycholinguistic text of that day: Fodor, Bever, and Garrett’s Psychology of Language (1974: 368), which summarized the state of psycholinguistic play up to about 1970: “there exist no suggestions for how a generative grammar might be concretely employed as a sentence recognizer in a psychologically plausible system.”
In retrospect, the reason for this dilemma seems clear. In the initial decade or two of investigation in the era of modern generative grammar, linguistic knowledge was formulated as a large set of language-particular, speciﬁc rules, such as the rules of English question formation, passive formation, or topicalization (the rule that fronts a focused phrase, as in, these students I want to solve the problem). Such rules are still quite close to the external, observable behavior—sentences—they were meant to abstract away from.
4.3.1 All transformations great and small: From Aspects to Principles and Parameters
By 1965, the time of Chomsky’s Aspects of the Theory of Syntax, each transformational rule consisted of two components: a structural description, generally corresponding to a surface-oriented pattern description of the conditions

Biolinguistics and the Leap to Syntax 79

under which a particular rule could apply (an ‘IF’ condition), and a structural change marking out how the rule aﬀected the syntactic structure under construction (a ‘THEN’ action). For example, an English passive rule might be formulated as follows, mapping Sue will eat the ice cream into The ice cream will be+en eat by Sue, where we have distinguished pattern-matched elements with numbers beneath:

Structural description (IF condition):

Noun phrase Auxiliary Verb Main Verb

1

2

3

Sue

will

eat

Noun Phrase 4
the ice cream

Structural change (THEN):

Noun phrase Auxiliary Verb

4

2

The ice cream

will

be+en Main Verb 3
be+en eat

by + Noun Phrase 1
by Sue

In the Aspects model a further housekeeping rule would next apply, hopping the en aﬃx onto eat to form eaten.
This somewhat belabored passive-rule example underscores the nonreductionist and atomistic, particulate, ﬂavor of earlier transformational generative grammar: the type and grain size of structural descriptions and changes do not mesh well with the known biological descriptions of, for example, observable language breakdowns. Disruption does not seem to occur at the level of individual transformational rules, nor even as structural descriptions and changes gone awry generally.
The same seems to holds true for many of the biological/psycholinguistic interpretations of such an approach: as Fodor, Bever, and Garrett remarked, individual transformational rules did not seem to be engaged in sentence processing, and the same problems emerge when considering language learnability, development, or evolution. Indeed, the biological/evolutionary picture emerging from the Aspects-type (Extended Standard Theory or EST) grammars naturally reﬂects this representational granularity. For any given language, in the EST framework, a grammar would typically consist of many dozens, even hundreds, of ordered, language-speciﬁc transformations, along with a set of constraints on transformations, as developed by Ross (1967), among others. This “particulate” character of such rules was quite in keeping with a micro-mutational view: if the atoms of grammar are the bits and pieces of individual transformations, structural descriptions and changes, then it is quite natural to embrace a concomitant granularity for thinking about language learning, language change, and language

80 Robert Berwick
evolution. It is quite another matter as to the empirical reality of this granularity.
Consider by way of example the Wexler and Culicover (1980) model of the late 1970s, establishing that EST grammars are learnable from simple positive example sentences. In the Wexler and Culicover model, what was acquired at each step in the learning iteration was a single rule with a highly speciﬁc, structural description/structural change, as driven by an error-detection principle. This atomistic behavior was quite natural, since it mirrored the granularity of the EST theory, namely, some sequence of transformations mapping from a base D-structure to a surface S-structure.
But equally, this extreme particulate view was embraced in other contexts, notably by Pinker and Bloom (1990), who argued that language evolution must follow a similar course: all of the speciﬁc design features of language must have arisen by incremental, adaptive evolutionary change. What Pinker and Bloom did in part is to run together two distinct levels of representation: what of necessity must be true, barring horizontal transfer—that genomic evolution lies, ultimately, at the level of single DNA letters or nucleotides— with what need not, and we shall argue, is not true—that language evolution, a distal behavior, proceeds apace at the level of the granularity set by the EST linguistic theory, with all evolutionary change incremental and adaptive. Such a position can lead to diﬃculties. It argues in eﬀect that each and every property of syntactic design that we see must have some measurable outcome on ﬁtness—perhaps quite literally, the number of viable oﬀspring. As Lightfoot (1991a) notes, this entails what might be dubbed the “Subjacency and Sex Problem”—there are important syntactic constraints, like the one called “subjacency,” a restriction on the distance that a phrase can be displaced—that have no obvious eﬀect on the number of oﬀspring one might have, absent special pleading: “subjacency has many virtues, but . . . it could not have increased the chances of having fruitful sex.” Further, this stance seemingly entails connecting all such “inner” constraints once again somehow to external communication—a delicate, probably incorrect link, as we have seen. Indeed, Pinker and Bloom do not advance any speciﬁc evolutionary modeling details at all about which syntactic structures are to be linked to particular aspects of ﬁtness so as to construct a proper evolutionary model. Fortunately, we do not have to resort to this level of detail. As we have outlined in the previous section, such incrementalism and pan-adaptationism is now known to be far from secure quite generally in evolutionary biology, even assuming the conventional Fisher model. If one in addition takes into account our more recent, nuanced understanding of the rapid evolution that can occur due to developmental changes as well as the widespread possibility of

Biolinguistics and the Leap to Syntax 81
homeoplasy, then this micro-mutational view of language evolution fares even worse.
Given such rule diversity and complexity, by the mid-1960s the quasibiological problems with surface-oriented rules—problems of learnability and parsability among others—were well known: how could such particular structural conditions and changes be learned by children, since the evidence that linguists used to induce them was so hard to come by? The lack of rule restrictiveness led to attempts to generalize over rules, for example, to bring under a single umbrella such diverse phenomena as topicalization and question formation, each as instances of a single, more general, Move whphrase operation. By combining this abstraction with the rule Move Noun phrase, by the end of the 1970s linguists had arrived at a replacement for nearly all structural changes or displacements, a single movement operation dubbed Move alpha. On the rule application side, corresponding attempts were made to establish generalizations about constraints on rule application, thereby replacing structural descriptions—for example, that noun phrases could be displaced only to positions where they might have appeared anyway as argument to predicates, as in our passive example.
Inspired in part by a lecture given by Jacob at MIT’s Endicott House in 1974 on the topic of biological variation as determined by a system of genetic switches (see Berwick and Chomsky, this volume), Chomsky had already begun to attempt to unify this system of constraints along the same lines—a concrete example of linguistics inspired by biology. By the 1980s, the end result was a system of approximately 25 to 30 interacting principles, the so-called Principles-and-Parameters (P&P) or Government-andBinding approach (Chomsky 1981). Figure 4.1 sketches P&P’s general picture of sentence formation, shaped like an inverted Y. This model engages two additional representational levels to generate sentences: ﬁrst, D-structure, a canonical way to represent predicate–argument thematic relations and basic sentence forms—essentially, “who did what to whom,” as in the guy ate the ice cream where the guy is the consumer and ice cream is the item consumed; and second, S-structure, essentially a way to represent argument relations after displacement—like the movement of the object to subject position in the former passive rule—has taken place. After the application of transformations (movement), S-structure splits, feeding sound (phonological form, PF) and logical form (LF) representations to yield (sound, meaning) pairs.
Overall then, on the Principles-and-Parameters view, sentences are derived beginning with a canonical thematic representation that conforms to the basic tree structure for a particular language, and then mapped to S-structure via

82 Robert Berwick
D-structure

Inherent-case assignment
Basic hierarchical structure (X-bar theory)

S-structure

Transformations (movement)
Case filter
Subjacency Theta criterion Binding theory Empty category principle

Phonological form (sound)

Logical form (meaning)

Figure 4.1 A conceptual picture of the traditional transformational generativegrammar framework (applying equally to the Extended Standard Theory, Government–Binding, and Principles-and-Parameters, approaches). Thin arrows denote constraints that possible sentence forms must satisfy, like the case ﬁlter. We do not describe all the depicted constraints in this chapter

a (possibly empty) sequence of displacement operations. For instance, one could start with the guy ate the ice cream in hierarchical form with a thematic or D-structure; via displacement of the ice cream this initial representation can be mapped to the topicalized form, ice cream, the guy ate. To use another conceptual picture that has been developed for computer implementation, in the P&P approach, sentence generation can be viewed as starting at D-structure and then running a gauntlet through a set of constraint boxes placed at D-structure and S-structure, as shown in Figure 4.1. A sentence is completely well formed if it passes all the constraints and emerges at the two interfaces of phonological form and logical form as one or more sound–meaning pairs.
Akin to atomic theory, this small set of constraints may be recombined in diﬀerent ways to yield the distinctive syntactic properties of diverse natural languages, just as a handful of elements recombine to yield many diﬀerent molecular types, or, anticipating the evo–devo revolution, the way that the same regulatory genes might readjust their timing and combinations to yield “endless forms most beautiful,” in Darwin’s famous closing lines, the title of Carroll’s (2005a) book on evo–devo. For example, one of the principles, X-

Biolinguistics and the Leap to Syntax 83
bar theory, constrains the basic D-structure tree shapes for phrases—whether phrases can appear in function–argument form, as in English verb–object or preposition–object combinations, for example, eat ice cream or with a spoon, or alternatively, in argument–function form, as in Japanese object–verb or postposition–object combinations, such as ice cream-o tabeta or spoon-ni.
The X-bar module constrains just a small part of the ultimate surface form of sentences and must conspire with other principles to yield the surface complexity that one actually sees. In order to replicate the passive rule, at least three other general principles constraining displacement and S-structure come into play. One such constraint is the so-called theta criterion: if one pictures a verb as a predicate taking some number of arguments—its thematic roles, such as drink requiring something to be drunk—then at the end of a derivation, all of the verb’s arguments must have been discharged or realized in the sentence, and every possible argument in the sentence must have received some thematic role. A second constraint is the Case ﬁlter: any pronounceable noun phrase, such as the guy, must possess a special feature dubbed Case, assigned by a verb, preposition, or tense/inﬂection.
Now the former passive rule follows as a theorem from these more basic principles. Starting from the D-structure was eaten ice cream, since eaten does not assign Case (analogously to an adjectival form, like tired or happy), the object ice cream must move to a position where it does get case—namely, the position of the subject, where ice cream can receive case from the inﬂected verb was. We thus derive the surface form the ice cream was eaten. The thematic association between eat and ice cream as the material eaten is retained by a bit of representational machinery: we insert a phonologically empty (unpronounced) element, a trace, into the position left behind by ice cream and link it to ice cream as well. In a similar fashion one can show that approximately thirty such constraints suﬃce to replace much of syntax’s formerly rule-based core. The end result is a system very close to Jacob’s system of genetic regulatory “switches,” with variation among languages restricted to the choice of parameters such as function–argument or argument–function form, possibly restricted further to choices of lexical variation. The language phenotype now looks rather diﬀerent; it has far less to do with the surface appearance of structure descriptions and structural change, but is constituted entirely of the parameters and their range of variation, a rather diﬀerent picture. This P&P approach to language variation was quite fruitful, and led to models of language acquisition, change, and parsing that diﬀered substantially from the Aspects view, more closely mirroring the possibilities of radical surface diﬀerences given just a few underlying changes. For example, in the domain of language acquisition and change, Niyogi and Berwick (1996)

