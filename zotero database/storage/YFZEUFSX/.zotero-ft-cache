Topics in Cognitive Science 7 (2015) 12–35 Copyright © 2014 Cognitive Science Society, Inc. All rights reserved. ISSN:1756-8757 print / 1756-8765 online DOI: 10.1111/tops.12120
Signers and Co-speech Gesturers Adopt Similar Strategies for Portraying Viewpoint in Narratives
David Quinto-Pozos,a Fey Parrillb
aDepartment of Linguistics, University of Texas at Austin bDepartment of Cognitive Science, Case Western Reserve University Received 28 February 2013; received in revised form 4 March 2014; accepted 2 May 2014
Abstract Gestural viewpoint research suggests that several dimensions determine which perspective a
narrator takes, including properties of the event described. Events can evoke gestures from the point of view of a character (CVPT), an observer (OVPT), or both perspectives. CVPT and OVPT gestures have been compared to constructed action (CA) and classiﬁers (CL) in signed languages. We ask how CA and CL, as represented in ASL productions, compare to previous results for CVPT and OVPT from English-speaking co-speech gesturers. Ten ASL signers described cartoon stimuli from Parrill (2010). Events shown by Parrill to elicit a particular gestural strategy (CVPT, OVPT, both) were coded for signers’ instances of CA and CL. CA was divided into three categories: CA-torso, CA-affect, and CA-handling. Signers used CA-handling the most when gesturers used CVPT exclusively. Additionally, signers used CL the most when gesturers used OVPT exclusively and CL the least when gesturers used CVPT exclusively.
Keywords: Constructed action; Classiﬁers; Character viewpoint; Observer viewpoint; Gesture; Embodiment
1. Introduction
Comparisons between signed languages and co-speech gesture (those gestures that cooccur with speech: for a discussion of different types of co-speech gesture, see Kendon, 2004; McNeill, 1992) have led to important ﬁndings about language structure, language processing, and other aspects of cognition. Various studies have described similarities and differences between signers’ and co-speech gesturers’ productions (e.g., Arik, 2009; Shaw, 2013; Sevcikova, 2014). Researchers have noted similarities in how these groups
Correspondence should be sent to David Quinto-Pozos, Department of Linguistics, University of Texas at Austin, 305 E. 23rd Street STOP B5100, Austin, TX 78712. E-mail: davidqp@austin.utexas.edu

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

13

of communicators depict the handling of objects. They have also noted similarities in the strategies adopted by signers and non-signers for describing objects (including animate beings and inanimate items), scenes and spatial conﬁgurations, and events (e.g., Casey & Emmorey, 2009; Emmorey & Falgier, 1999; Emmorey, Tversky, & Taylor, 2000; Perniss, 2007a,b; Perniss, Zwitserlood, & O€ zy€urek, 2011). A common ﬁnding among these latter studies is that both signers and gesturers can choose to describe events from the point of view of an observer outside the scene or a character inside the scene.
In the present study, we combine elements of these approaches by comparing signed language and co-speech gesture productions of deaf signers of American Sign Language (ASL) and co-speech gestures produced by hearing users of American English. Our focus is on the strategies employed by communicators for describing dynamic scenes that contain animate and inanimate objects that interact. We examine whether signers and co-speech gesturers employ similar strategies for communicating information about the content of the scenes. To the extent that we identify similarities across communicators, we wonder whether the strategies could be attributable to cultural norms of gestural communication (e.g., a system of gesticulation that is common to people in ASL- and English-using contexts) or cognitive universals of all signers and co-speech gesturers. Regarding the former, it is possible that signers and gesturers produce some similar behaviors because of similar experiences with certain kinds of events. Alternatively, perhaps signers and non-signers produce some similar communicative forms if the same gestural bases underlie communication in both modalities. These two possibilities are not mutually exclusive, and examination of either topic holds the potential to help us understand fundamental facts about human cognition.
The remainder of the introduction provides background information about previous work in these areas of inquiry. First, we introduce key concepts and terms that have appeared in the co-speech gesture literature for more than 20 years. We follow that with work on signed language and viewpoint. Then, we describe ways in which researchers have compared the strategies used by co-speech gesturers to those employed by signers. Finally, we address the question of what may be contributing to similarities that are observed.

1.1. CVPT and OVPT in co-speech gesture

In co-speech gesture research, two primary perspectival options (internal/external) have been referred to as character viewpoint gesture (CVPT) and observer viewpoint gesture (OVPT), respectively (McNeill, 1992).1 CVPT gestures portray the actions of a character via the speaker’s movements and displays of affect, whereas OVPT gestures depict entire characters (or objects) and their movements in a smaller scale in front of the speaker. Parrill (2010) showed that for English co-speech gesturers who describe scenes from viewed cartoons, certain events (i.e., actions within cartoon scenes) evoke gestures from the point of view of a character (CVPT), others from the point of view of an observer (OVPT), and some from both perspectives. In this study, CVPT gestures were used for depicting how characters handled objects (e.g., Bugs Bunny holding a baseball bat and

14

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

preparing to hit a ball), for communicating emotional affect of a character (e.g., Bugs Bunny’s expression of surprise), or for showing how a character may have moved with his or her torso (e.g., a backward torso tilt to show how the character leaned while preparing to catch a baseball ﬂying through the sky). On the other hand, OVPT gestures were used to show an object or character’s path through space (e.g., Bugs Bunny running through a baseball diamond). Other events elicited both kinds of gesture from different speakers: Some speakers produced CVPT and others produced OVPT. Events that elicited both CVPT and OVPT gestures also contain features of handling and movement trajectories, but the choice of gesture type (CVPT or OVPT) tends to depend on discourse processes and whether the reported information is given or new (also see Debreslioska, O€ zy€urek, Gullberg, & Perniss, 2013).

1.2. CA and CL in signed language

As suggested, signed language users can also choose to describe the actions of characters using different visual perspectives that are similar to the CVPT and OVPT gestures of co-speech gesturers. O€ zy€urek and Perniss (2011) refer to the different perspectives in signed language as event space projections, and they outline features of character perspective and observer perspective that are strikingly similar to those described for CVPT and OVPT (also see Perniss, 2007b). The authors also point out that such perspectives have been labeled in various ways in the sign literature, including viewer and diagrammatic space (Emmorey & Falgier, 1999; Emmorey et al., 2000), and surrogate and depictive space (Liddell, 2003), among others. This general typology of viewpoint or perspective is echoed in other writings about signed languages (e.g., see Cormier, QuintoPozos, Schembri, & Sevcikova, 2012; Perniss, 2007a, 2012),
O€ zy€urek and Perniss (2011) note that, when engaging character perspective, a signer “assumes the role of the character in the event, such that at least the character’s head and torso are mapped to the signer’s body, and the size of the projected space is life-sized” (p. 87). This description largely coincides with what other authors refer to as constructed action (CA), or a signer’s use of various parts of her body (e.g., hands, head, torso, eyegaze) to depict the postures, actions, thoughts, and/or expressions of a character or animate being (Liddell & Metzger, 1998; Metzger, 1995). Constructed action has been claimed to have obligatory qualities (i.e., signers normally feel that they need to engage its use for depicting the actions of characters; Quinto-Pozos, 2007a,b), it is used across different registers of language (Quinto-Pozos & Mehta, 2010), and it appears in narratives across sign languages (e.g., Aarons & Morgan, 2003 for South African Sign Language).
With respect to observer perspective, O€ zy€urek and Perniss (2011, p. 87) suggest that this occurs when “event space is projected onto sign space from an external vantage point. The signer is not part of the represented event, and the event space is reduced in size, projected onto the area of space in front of the signer’s body.” Such projections require the use of signed language classiﬁers, devices that include verbs of motion and location, verbs of handling, and predicates of visual-geometric description (Schembri, 2003). Among the different types of classiﬁers are those that represent an object in its

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

15

entirety and serve verbal and pronominal functions (also referred to by other labels such as whole entity classiﬁers, CLASS classiﬁers, semantic classiﬁers, static size and shape speciﬁer [SASSes]).2 Classiﬁers are used extensively in spatial descriptions that involve the motion and/or location of objects, and most signed languages studied thus far possess this type of sign (see Schembri, 2003).
The use of character and observer perspective has also been described in terms of the types of classiﬁers that can be used within each perspective choice (Perniss, 2007a,b, 2012; O€ zy€urek & Perniss, 2011). Character perspective often involves the use of handling classiﬁers, whereas entity classiﬁers are needed for observer perspective; this is referred to as aligned perspective and classiﬁer combination. However, signers can also produce entity classiﬁers during character perspective and handling classiﬁers during observer perspective; these would be non-aligned combinations. A typical example of a non-aligned combination would be the simultaneous depiction of a character’s affect and head/torso postures with a classiﬁer production that depicts the character’s movement through space. Dudis (2004) refers to such constructions as examples of body partitioning, which are common in the use of depiction in signed languages. The other non-aligned combination (i.e., handling classiﬁers during observer perspective) is perhaps more infrequent.
As noted above, character perspective can be treated as equivalent to constructed action (CA). Thus, CA can be produced simultaneously with classiﬁers, resulting in the simultaneous production of multiple perspectives. Aarons and Morgan (2003) describe a signer depicting a parachutist (through the eyes of the character, via CA) and other objects in the sky (through the use of classiﬁers). This is also possible in co-speech gesture. Gestures that simultaneously show multiple points of view (e.g., one hand shows a character’s path through space, while the body shows the character’s running action) are called Dual Viewpoint gestures (McNeill, 1992). However, Parrill (2009) has suggested that these gestures are rare.

1.3. Comparing CVPT to CA and OVPT to CL

Various authors have noted similarities between a signer’s full-sized portrayal of the perspective of a character (i.e., CA) and CVPT co-speech gestures, and smaller scaled depictions of the motion and/or location of an object or character (i.e., entity CLs in sign) and OVPT co-speech gestures (Cormier et al., 2012; O€ zy€urek & Perniss, 2011; Perniss, 2012).3 These comparisons are striking because of the suggested similarities despite the fact that the burden of communication lies in the visual-gestural modality for signers, whereas users of spoken language can take advantage of both oral-auditory and visualgestural channels.
Recent studies have pointed out similarities between the manual and bodily productions of signers and users of spoken languages. As one example, Shaw (2013), in an analysis of two groups of friends (hearing/English, deaf/ASL) during their respective game-night interactions, suggests that signers and co-speech gesturers adopt similar gestural strategies of communication that are context-driven. For example, the researcher notes that both speakers and signers used more “depictive” forms during certain types of

16

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

discourse, such as longer stretches of explicative discourse. Those representational forms often took the form of enacting the manner in which a character would conduct an act (e.g., handling a card). Shaw argues for a uniﬁed theory of gesture use that captures the productions of co-speech gesturers and signers alike without placing the gesture of signed language users along a continuum that does not allow for the simultaneous use of language and gesture for signed language users (e.g., McNeill, 1992). With respect to the topic of handling, Sevcikova (2014), in an experiment designed to investigate the production and perception of handling by signers (British Sign Language [BSL] users), cospeech gesturers (English users), and non-signers in a pantomime condition, demonstrated that co-speech gesturers and signers encode the size of an object in similar ways via handshape. For both types of communicators, handling handshapes were produced in discrete ways (rather than in analog ways for matching a target stimulus), although they could also employ gradient distinctions to focus on speciﬁc properties of an object or for purposes of emphasis. These works suggest that there are multiple ways in which cospeech gesturers and signers utilize the manual modality similarly—and this is true across cultures.

1.3.1. Accounting for similarities and differences Are similarities across co-speech gesturers and signers reﬂecting cultural norms of ges-
tural communication, or do they reveal cognitive universals common across signers and co-speech gesturers? In this section, we highlight previous studies that have suggested that culture and ambient spoken language have an inﬂuence on the type of gesticulation that is produced by co-speech gesturers. Further, we provide background information about a model of language production that could account for how humans use their bodies for communication. We will return to these points in our discussion of the results of our study.
With respect to a possible role of the ambient culture and language, it is the case that languages (grammar and lexical items) can impose certain constraints on their users, which will lead to differences in gesture behavior. For example, work by Kita and O€ zy€urek (2003) has shown that linguistic properties of Japanese, Turkish, and English have effects on which semantic features appear in gesture. Thus, while conceptualizations of those groups of language users may be similar, the grammar of the languages they speak impacts their behavior. Arik (2009) also reveals differences between Turkish speakers and users of English and various signed language in how they describe spatial scenes. With regard to the topic of the present work, Brown (2008) has shown that language can have an effect on viewpoint in gesture: In her study, speakers of Japanese used more CVPT gesture than did speakers of English.
Consideration should also be given to potential cognitive universals that are based in embodied communication. As noted by Emmorey and colleagues (e.g., Emmorey et al., 2000), similarities we see across signers and gesturers may be the result of the way human beings conceptualize space for the purposes of communication. A recent model of language production, the Gestures as Simulated Action (GSA) model (Hostetter & Alibali, 2008, 2010), provides some detail on the processes that might be involved. While the

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

17

model focuses on spoken languages, the same principles can be used to explain how similarity in conceptualization might give rise to similarity in communicative behaviors across signers and gesturers. A full discussion of this model is beyond the scope of this paper, though we will say a few words about it here (and see Parrill, 2010). According to the GSA, gestures occur because during language use, we create simulations (partial reconstructions) of any imagistic and motoric content associated with what we are talking about. There is mounting evidence that visual and motor simulations are in fact part of language use, as reviewed in Hostetter and Alibali (2008). How could such simulations lead to gesture or sign? As an example, if talking about an agent performing an action (say, a person climbing up a ladder), a motor program for the arm motion and handshape associated with climbing will be activated. This may result in a gesture or sign showing these actions. If taking the perspective of the agent, the language user may show this action via CVPT or CA. A mental image of a person climbing upward will also be generated, and properties of the mental image can appear in gesture or sign as well. For instance, the visual trajectory from the mental image may be re-represented as a motor action tracing that upward trajectory. In this case, the language user is taking an observer’s point of view and may use OVPT or a CL. There is no reason to think that such simulations are radically different for signers and spoken language users. For this reason, we should also observe parallels in behavior, such as those discussed above.

1.4. Our extensions

Some previous studies comparing signed languages and gesture have shown groups of each kind of language user the same stimuli and compared the behaviors of each group (e.g., Brentari, Nadolske, & Wolford, 2012b; Casey & Emmorey, 2009; Emmorey et al., 2000; Quinto-Pozos & Parrill, 2008). This study takes a slightly different approach, but one that has important theoretical beneﬁts. We attempt to determine whether signers and gesturers behave in similar ways for speciﬁc kinds of events. Namely, we isolate events that are known to evoke CVPT or OVPT in gesturers and ask what signers do for those events. By doing so, we hope to both support the parallels between CA and CVPT and CL and OVPT, but also to reveal important differences in how CA and CL are deployed by signers.

2. Method
2.1. Research questions and predictions
The starting point for this study is a 2010 corpus analysis of 23 English-speaking gesturers describing three short cartoon clips. In this study, Parrill found that co-speech gesturers reliably used CVPT gestures when describing events that depict a character handling an object, displaying affect, or using the torso prominently (e.g., shrugging). OVPT gestures were used when depicting a character’s path through space. In addition,

18

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

some events elicited either CVPT or OVPT gestures (referred to as the “Both” category of events by Parrill), namely those that combine properties of both kinds of event (e.g., handling while moving through space, as when running while carrying an object). It is unclear what motivated a speaker to use one or the other type of gesture for this latter class of event, but Parrill suggests that “discourse factors (such as the narrator’s focus, the status of the information as given or new, the structure of the narrative)” (p. 661) might provide an explanation. For the study reported here, we collected data from deaf signers of ASL in order to determine what signers do when co-speech gesturers use CVPT, OVPT, or a mix of both viewpoints.
Various predictions can be made based on earlier writings on this topic (Cormier et al., 2012; Quinto-Pozos & Parrill, 2008). If CA is indeed similar to CVPT for gesturers, we would expect it to be used most frequently for the set of events that elicited only CVPT. We might also predict that signers would use CL strategies in addition to CA, because it has been claimed that signers can use multiple strategies simultaneously for communicating information about a scene (e.g., Dudis, 2004; Quinto-Pozos, 2007a,b). Similarly, we would expect signers to use CL most frequently for those events where non-signers used OVPT in the Parrill study. From the Dudis and Quinto-Pozos writings, we would also expect signers to use some CA for those events. For the set of events that elicited both CVPT and OVPT, we predict that signers would also use both strategies, but the frequency of use of CA or CL in those situations is difﬁcult to predict. Our predictions are show in Table 1.

2.2. Participants and materials

Data from 10 adult signers of ASL (Mage 28.7, range 19–42; six females) are provided here (a total of 20 signers participated in the study, although 10 of them produced other narratives that were not included in this analysis). Three were native signers (i.e., exposed to ASL since infancy), whereas seven were non-native signers (exposure to ASL ranged from ages 4 to 12). Signers viewed the same three short cartoon clips used in the Parrill (2010) study.

2.3. Procedure

For each data collection session, two signers sat side-by-side at approximately a 45degree angle in order to allow them to interact with each other while also being captured

Table 1 Predicted strategies for signers
Gesturers Used (Parrill, 2010)
CVPT OVPT CVPT or OVPT

Signers Predicted to Use CA
Most frequently Least frequently Unsure

Signers Predicted to Use CL
Least frequently Most frequently Unsure

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

19

by a single video camera. A signer watched a video clip on a laptop computer and then explained to his/her conversational partner what the video clip was about. After watching and describing all three clips, the other participant in the pair followed the same procedure with a separate set of clips. Data from the partners are not included in this analysis. After the explanation of all clips by both signers, a short quiz (in paper format) was given to each participant. The goal of including the quiz in the data collection was to have the signers describe the clips in detail and to encourage the conversational partner to be engaged as the signer was explaining the clips. The signers were informed about the quiz at the beginning of the data collection session.
As noted, our intent was to determine what signers do for events where gesturers exclusively use CVPT, OVPT, or use a mix of both viewpoints. In the Parrill study, multiple English speakers watched the clips and divided them into events based on actions of the characters and cinematographic features like “camera” angle (see Parrill, 2010, for more detail and reliability). We used that same division of events. While it is possible that the division of the clips into events would have differed for ASL users, using the same events used previously allowed us to compare the co-speech gesturer and signer data. There were 18 CVPT events, 20 OVPT events, and 4 events where co-speech gesturers chose one or the other of the two strategies. See Tables A1, A2, and A3 (reproduced from Parrill, 2010) in the Appendix for details.

2.4. Coding

The coding of the data focused on two broad categories: documenting the use of constructed action (CA) and classiﬁers (CL) by each signer throughout the various events enumerated by Parrill (2010). The primary coder was a nationally certiﬁed ASL-English interpreter who was trained in the coding parameters.
With respect to constructed action, the coder documented instances of CA that involved the display of affect (i.e., emotion and emotive facial expressions), movement of the torso, or handling that was intended to reenact actions of a character in the stimuli clips. As such, each occurrence of CA-affect, CA-torso, and CA-handling was recorded separately, which differs from the Parrill (2010) coding where CVPT was coded as a single category. For the sign data, the coder did not scrutinize the speciﬁc handshapes that were used for handling, only that handling was either being represented in the narrated event or not. In addition, the coder was instructed to not count repetition of CA within an event (e.g., if the signer were to produce handling CA for an event, followed by signs and classiﬁers and then a repetition of the earlier handling). In addition, because the Parrill coding did not include analysis of the head or gaze alone, we did not document examples of CA as reﬂected through head movements alone (e.g., head movements that reﬂect eyegaze changes of the character).
For the classiﬁer signs, the coder documented each time the signer used his/her hand to depict an entity in its entirety (i.e., the use of an ASL classiﬁer) or the path of a character/object. Like the constructed action coding, the coder was instructed to not count repetition of either of these within a single event.

20

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

2.5. Reliability

Approximately 25% of the narratives were randomly selected to be coded for reliability purposes. The coding was completed by two reliability coders who sign ASL (one deaf signer and one hearing signer), and they were trained in the coding scheme used by the primary coder. They were not aware of the patterns of co-speech gesture according to event type as reported in Parrill (2010). When the reliability data were compared to the data from the primary coder, any discrepancies in judgments across coders were scrutinized by the ﬁrst author, which included revisiting the original video data for conﬁrmation of accurate coding for such disagreements. After this was completed, agreement with the originally coded data set was calculated to surpass 90%.

3. Results
We begin with a few representative examples to illustrate signer behavior. We then describe overall patterns, followed by statistical analysis of the main patterns of interest.
3.1. Examples of signed events
Portions of one of the narratives as told by an ASL signer are described in this section in order to provide visual examples of the different types of depictive devices that were used. The narrative of interest here focuses on Bugs Bunny serving as a pitcher in a baseball park. Bugs Bunny pitches a ball that is hit by an unseen batter; the ﬂy ball leaves the park and Bugs chases after it, ﬁrst on foot and then in a bus.
Fig. 1 depicts the signer displaying a single token of CA-affect (aspects of surprise), which was counted for the event that describes Bugs Bunny’s surprise reaction to the batter hitting a ﬂy ball (Event # R3 in Table A1). Fig. 2 depicts a signer displaying two simultaneous examples of CA for depicting Bugs Bunny reading a newspaper in the bus: CA-torso because the torso is leaning back (as if resting comfortably in a seat) and CAhandling because the character is purportedly holding a newspaper with two hands (these are Event # R7 in the Parrill, 2010, data).
Fig. 3 depicts the signer displaying a single token of CL (V-handshape with the right hand to represent an upright and walking person; the left hand serves as a ground object such as the surface beside where the person is walking), which was counted for the event that describes Bugs Bunny getting off the bus and running into the building (Event # R9 in Table A2).
Lastly, Fig. 4 depicts three frames of the signer pointing to the ball ﬂying through the air (Event # R4 in Table A2). This was originally coded as a single token of the signer depicting the path of an object (i.e., the ball), which would necessarily fall into the CL category even though the CL did not represent the object itself. For example, in the case of indicating a ball ﬂying through space, the signer might use an extended index ﬁnger

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

21

Fig. 1. Signer depicting Bugs Bunny’s surprise reaction to a ﬂy ball being hit (CA-affect).
(commonly referred to as a 1-handshape or an ASL G-handshape) to trace the path, and that 1-handshape is typically not used for depicting a round object moving across the sky unless the index ﬁnger is oriented horizontally so that the length of the ﬁnger appears to be following the path of the ball through space (not just the tip of the ﬁnger showing the path while the ﬁnger is oriented vertically, as in Fig. 4). Some authors might suggest that such an articulation does represent a classiﬁer form (e.g., see Zwitserlood, 2003, p. 176 for a description of a classiﬁer articulated with the ﬁngertip or Zwitserlood, 2012, p. 178 for a discussion of a default classiﬁer.), even though the handshape does not represent an object in its entirety. Such cases (n = 20 for all signers and all events) were ﬂagged for further discussion and analysis by the coder and the researcher. As noted earlier, the type of path gesture captured in Fig. 4 was quite rare in the sign corpus, and it was not included in the statistical analyses below.
Sometimes, signers would also produce simultaneous examples of CA and CL, some of which would represent non-aligned perspective and classiﬁer use (O€ zy€urek & Perniss, 2011; Perniss, 2007a,b, 2012). For example, signers often produced a classiﬁer for Bugs Bunny moving forward in the signing space to depict him running through the baseball diamond while also displaying a facial expression that communicated that Bugs was running quickly. As noted, Dudis (2004) would refer to such constructions as body partitioning since the signer’s hand depicts the entire entity (i.e., Bugs Bunny), while other parts of the upper body show how those body parts moved.

22

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

Fig. 2. Signer depicting Bugs Bunny reading a newspaper (CA-torso & CA-handling).

Fig. 3. Signer depicting Bugs Bunny getting off the bus and walking toward building (CL).

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

23

(A)

(B)

(C)

Fig. 4. Signer depicting the path/trajectory of the ﬂy ball (CL/path gesture); three frames.
3.2. Descriptive statistics
Based on our method of coding, the total number of depictive devices in this dataset is 508 (324 CA [64%], 184 CL [36%]). The Parrill (2010) dataset netted 506 gestures (233 CVPT [46%], 273 OVPT [54%]). The main difference with the basic distribution between the two datasets is that ours included noticeably more CA than CL, whereas the two analogous depiction strategies (CVPT, OVPT) were closer to being equally distributed in the Parrill dataset. If our coding had not counted simultaneous instances of CA (i.e., either affect, torso, or handling) separately, the number of depictive devices would instead have equaled 360 (176 CA [49%], 184 CL [51%]), a result that would have mirrored the Parrill dataset to a greater degree.
One other notable difference between the two datasets is that 23 co-speech gesturers were included in Parrill’s study, whereas only 10 signers were used in this dataset. This means that the signers, on average, produced more than double the manual depictive devices than the co-speech gesturers did. It is important to keep in mind that the speech data accompanying the manual gestures (for the co-speech gesturers) were not reported in the Parrill (2010) study, so some of the communicative information is not included there. Likewise, we do not provide information about the lexical and grammatical information that complements the CA and CL data that we report here, thus not providing a complete picture of the communicative situations.
Parrill included manual gestures that represented both the character in its entirety and the path of the character through space as OVPT gestures. When we coded our data, we identiﬁed 20 examples of a signer indicating a character’s path but not representing the character in its entirety (see Fig. 4), and they only appeared for the events where Parrill’s subjects produced OVPT gestures exclusively. Because of the relative rarity of this type of depictive device in this sign dataset (<4% of the dataset), we do not include those 20 tokens within the statistical analyses included here. We report their existence and offer a speculation about why they appear so infrequently below. As such, our statistical analyses are based on 488 tokens of depictive devices. The distribution of these devices is shown in Fig. 5, which shows a notable use of CA by signers for events that elicited CVPT by

24

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

Fig. 5. Distribution of signers’ depictive devices by event category. Note. “CA” = combined CA-handling, CA-torso, and CA-affect.
gesturers. It is also notable that both CA and CL were used throughout all events, which was not the case for CVPT and OVPT in the Parrill (2010) results. Even though there were marginally fewer CVPT events (n = 18) than OVPT events (n = 20), there were slightly more depictive devices used for CVPT events (n = 218) versus the total number in the OVPT events (n = 191). Since there were only four events that constituted the Both (i.e., “either”) category, it is not surprising that only a total of 79 depictive devices were used across all those events.
CA (66% of the corpus of 488 items) can be further divided in the tripartite fashion detailed earlier. There were 150 instances of CA-affect (31%), 113 instances of CA-torso (23%), and 61 instances of CA-handling (13%). Fig. 6 shows the mean use, in percentage, of each type of depiction by event category. The means represent proportions of the use of each depictive device (e.g., CL, CA-affect, CA-torso, and CA-handling) per signer over all events within a category (e.g., gesturers use CVPT, OVPT, or either CVPT/ OVPT). If, for example, a signer would use each depictive device over all events within a category, he or she would have a mean 100% use. That participant’s mean would then be averaged with those of the other nine participants to obtain the data shown in Table 2. Another example is that CL was used by the 10 signers approximately 25% of the time for the events where the gesturers used CVPT exclusively.
Fig. 6 suggests that signers commonly used both CA and CL, even when co-speech gesturers used CVPT or OVPT exclusively. Multiple depictive devices (e.g., CL or a type of CA) for a single event could occur simultaneously or in sequence. Signers appear to use CL the least for those events that evoked only CVPT gesture (approximately 25%, on average, compared to approximately 50% range for the other two categories of events). In addition, CA-handling is nearly non-existent for those events that evoked only OVPT

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

25

Fig. 6. Signers’ mean percentage of CA and CL use by event category.

Table 2 Differences in use of depiction across event category using the t-test statistic

Parrill (2010) Event Type of Depiction Used by Signers

Category

Comparisons

CL

CA-affect

CA-torso

CA-handling

CVPT versus OVPT t = À4.76 p < .0001 t = 1.78 p = .1785 t = 1.90 p = .1406 t = 5.02 p < .0001 CVPT versus both t = 4.86 p < .0001 t = 2.19 p = .0737 t = 3.18 p < .01 t = À1.51 p = .2856 OVPT versus both t = 2.10 p = .0912 t = 3.27 p < .01 t = 4.36 p < .0001 t = 3.62 p < .01

Note. For each comparison, df = 396.

gesture, whereas it is commonly used in the other two categories (approximately 1%, on average, compared to 14% and 29% in the other two categories). The other types of CA (CA-affect and CA-torso) also show some differences across event categories, but not to the extent that CA-handling does. Also notable is the frequent use of CL and CA depictive devices for events that evoked both CVPT and OVPT gesture—especially the more frequent use of CA-affect and CA-torso.
3.3. Statistical analyses
To determine whether the differences we observed across event category were statistically signiﬁcant, we used a generalized linear mixed model with event category and event item as ﬁxed factors, and participant as a random effect variable. For each type of depiction, a Tukey–Kramer adjustment for multiple comparisons (across event categories) was calculated. Results of these analyses are shown in Table 2.

26

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

The comparisons shown in Table 2 can be summarized in the following ways:
1. When we compare the events that elicited CVPT gesture to those that elicited OVPT gesture for the Parrill (2010) results (top row of data), we see a signiﬁcant difference in the use of CL (more for those events that evoked OVPT) and in CA-handling (more for those events that evoked CVPT); however, there were no differences in the use of CA-affect or CA-torso across those two categories of events.
2. When we compare the events that elicited CVPT gesture to those that elicited both kinds of gesture (middle row of data), we see a signiﬁcant difference in the use of CL (more for those events that evoked both types of gesture) and in CA-torso (more for those events that evoked both types of gesture), yet there were no differences in the use of CA-affect and CA-handling across those two categories of events.
3. When we compare the events that elicited OVPT gesture to those that elicited both types of gesture (bottom row of data), we see a signiﬁcant difference in the use of all types of CA (more for those events that evoked both types of gesture), but not in the use of CL.

4. Discussion
4.1. Multiple strategies for signers
When comparing the communicative strategies that co-speech gesturers and signers use for depicting how characters act and move through space, several points can be made. For the most part, signers use multiple strategies when co-speech gesturers in the Parrill (2010) study primarily used one strategy. Other studies have reported similar ﬁndings about the use of multiple simultaneously produced communicative strategies by signers. Dudis (2004) describes body partitioning, or the simultaneous use of manual and non-manual articulators to depict various aspects of a scene, and Perniss (2012) describes aligned and non-aligned constructions, such as an entity classiﬁer that accompanies character perspective. The use of constructed action that is complemented by classiﬁers has also been described with data from South African Sign Language (Aarons & Morgan, 2003) and ASL (Quinto-Pozos, 2007a,b).4 It seems that signers are particularly adept at using multiple body parts to depict two viewpoints simultaneously within the signing space, and this fact is borne out by these data. Again, while co-speech gesturers can depict multiple simultaneous perspectives in gesture alone (dual viewpoint gestures), this is infrequent. Whether English speakers perform similar feats by encoding information associated with one point of view in speech and with another point of view in gesture is an open question. In other work, we have explored the extent to which English speakers represent different perspectives in gesture and speech, using syntactic subject as an index of perspective in speech. We ﬁnd that this multiplicity of perspectives is not common (Quinto-Pozos, Parrill, Stec, Kashmiri, & Rimehaug, 2014 ). In cases where it did occur, narrators tended to use the passive voice in speech to keep the main character focused while talking about an action

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

27

performed on the character. Gesture was used to show the action performed on the character. These cases may have occurred because the action performed on the character was best represented via CVPT, but the desire to keep the character as the subject of the utterance (for purposes of discourse cohesion) led to the use of the passive voice. This interaction between the affordances of the manual modality and linguistic structure is reminiscent of Perniss’s discussion of aligned and prototypically and non-prototypically aligned constructions (Perniss, 2007a,b).

4.2. Similarities between signers and gesturers

Even though signers seem to use more strategies (CA and CL throughout all events or the simultaneous use of both strategies), on average, for depicting an event than cospeech gesturers do, the parallels between CA and CVPT and CL and OVPT that have been noted in various places are supported by our data. As predicted (Table 1), this dataset reveals that signers use CL the least in describing those events for which co-speech gesturers use CVPT exclusively. Those events can involve characters handling objects (e.g., holding a newspaper), displaying emotional affect (e.g., surprise), or using their torsos in a particular manner (e.g., leaning against a door). In describing those events, signers use various types of CA, especially CA handling, more than CL as a strategy for communicating the actions of a character. Also as predicted, signers use CL the most when describing those events for which co-speech gesturers use OVPT exclusively. Those events can generally be characterized by the communication of information about character trajectories through space (e.g., running outside of a baseball stadium).
Unlike the predictions that were conﬁrmed for the use of CLs by signers, the use of CA is not as straightforward. Signers did use the least amount of CA-handling for the OVPT category of events, which follows the predictions made in Table 1. But they still continued to use CA-affect and CA-torso during those events. In fact, CA-affect and CAtorso are used throughout all categories of events, with the most frequent use appearing in the both category of events (Those events for which some gesturers used CVPT and others used OVPT.) This suggests that uses of affect and the torso by signers are common and important ways to engage in the retelling or narration of a set of events. Other authors have also suggested that torso shift is a common element of constructed action (e.g., Brentari et al., 2012b), even with the co-occurrence of classiﬁers to capitalize on the use of diagrammatic space (e.g., Dudis, 2004). The complexities of torso shift require future focus, especially to tease apart whether torso shift reliably occurs less in some language registers than others (e.g., see Quinto-Pozos & Mehta, 2010).
Perhaps the most striking difference of CA use across categories comes from CA-handling. The CVPT category of events elicited the most use of CA handling (29% of the time, on average) by signers, whereas handling was less prominent in the other two categories of events (OVPT and the both category, 1% and 14%, respectively). For the OVPT category of events, the result is expected because the gestures that normally appear in that category are those that depict an entity in its entirety, rather than only the hand of a being that interacts with the world around her.

28

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

Based on the data presented in Parrill (2010), handling can be said to be a common characteristic of the use of CVPT by co-speech gesturers, and the data contained herein suggest that handling is often used by signers when describing the same types of events. However, the fact that signers depict handling when co-speech gesturers do the same does not imply that they do it in the exact same way. Brentari, Coppola, Mazzoni, and GoldinMeadow (2012a) have shown that signers and non-signers take advantage of different hand conﬁgurations to depict handling. We did not analyze the precise handshapes used by either signers or gesturers, but this would be a fruitful avenue for further study, allowing us to compare our ﬁndings to those of Brentari and colleagues’ on the selected ﬁnger complexity of handshapes across signers and non-signers (co-speech gesturers).

4.3. More of (just about) everything for the both category

The category of events that elicited CVPT from some gesturers and OVPT from others in the Parrill (2010) study is notable because it evoked more uses of some types of representational devices in comparison with the other categories. In particular, when compared to the OVPT category of events, the Both category elicited signiﬁcantly more of each type of CA, and there was no statistical difference between CL use across the OVPT and Both categories. However, when compared to the CVPT category of events, the Both category elicited more CL and also CA-torso. What is it about these events that results in greater use of some representational devices? As noted in Parrill (2010), this category of events can be described as characters engaging in displays of affect, torso movement, and handling while also traversing some trajectory (e.g., Bugs Bunny pulling himself up the ﬂagpole). Signers appear to portray this by using more CA and more CL (see the introduction for a discussion of the simultaneous communication of multiple perspectives in signed language) than in the categories that elicited mostly one or the other (CVPT and OVPT). Additionally, not only did the group of signers include both main types of depictive devices (CA and CL), more of them did so in comparison to the other categories.

4.4. Relative rarity of path gestures that match OVPT path gestures

Signers produced a few depictive devices that pointed out an object’s path without depicting the object in its entirety. We refer to these devices as path gestures since they mirror similar devices used by hearing co-speech gesturers. These path gestures appeared minimally in this dataset (20/508, ~4%), and they were initially coded as CL gestures, though they were not included in the statistical analyses that were conducted. As suggested, they most closely mirror one type of co-speech gesturers’ OVPT path gestures. As noted earlier, OVPT gestures depict an entity in its entirety as it traverses a path or simply the path that is taken by the entity. The form of these signed gestures was generally a 1-handshape that moved through the space as if tracing a character’s or object’s trajectory (see Fig. 4), but seemingly not referring to the object itself. This function differs from the 1-handshape serving as a CL. In ASL, the 1-handshape in a vertical orientation is commonly used to refer to an upright person, though the same handshape can also

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

29

be used to refer to an entity (animate or inanimate) that is oriented horizontally and either static (e.g., a pen on a table) or moving in some direction (e.g., a skunk or a cat, moving forward as seen in the data from the present study). In these latter cases where the horizontally oriented 1-handshape moves in a direction to indicate the trajectory of an entity, the ﬁnger tip leads the path with the remainder of the index ﬁnger following behind the same path; for these cases we have interpreted the entire ﬁnger as representing the entity. These latter examples of vertically and horizontally oriented index ﬁnger (i.e., the entity CL forms) differ noticeably from the 20 path gestures of the variety that are depicted in Fig. 4, where the tip of the index ﬁnger points to the moving object and serves as a referential locus or a tracing of the movement through space, not as an entity in and of itself.
One may wonder if there is a reliable way to distinguish the two functions of the 1-handshape that we have highlighted here. The ASL phonological parameter of movement—in particular, hand-internal movement—may be salient in determining which of the two functions is being depicted with the extended index ﬁnger. It appears that one such test of the distinction between the two functions could focus on the joints of the ﬁnger: If the joints can be engaged to give the appearance of a ﬁnger wiggle, it is likely that the 1-handshape is functioning as an entity CL (in this case, the ﬁnger wiggle, combined with a path movement of the hand/arm, would signal movement of the person through space). Alternatively, the 1-handshape that serves as a point to the path/trajectory of an entity should not allow for ﬁnger wiggling (i.e., joint engagement). Emmorey (1999) described distinctions between deictic pointing gestures and ASL indexical pronouns, although her examples focused on points to single locations (not a path in space).5 In that writing, Emmorey noted that movement might provide clues to the distinction between the gesture and the ASL indexical point. Her account seems to reference path movement, whereas ours focuses on hand-internal movement, but in both cases there is a suggestion that the phonological parameter of movement is salient in distinguishing such forms. It may be the case that these forms appear relatively minimally in the data because signers most often indicate the path of an object along with the CL that represents the object, rather than by using the deictic path gesture described here. Data from other narratives are needed in order to determine if it could be predicted where such path gestures would occur.

4.5. Implications of this work

We are not the ﬁrst to comment on the strong parallels between the way signers and gesturers communicate information about point of view. As noted earlier, some writers have suggested that similarities across signers and gesturers are likely the result of how human beings conceptualize space for communication (Emmorey et al., 2000). Similar conceptualizations of space across all language users could presumably result in similar forms to represent objects and actions within that space. Recent models of language have provided detail about how conceptualization might result in the appearance of certain features in co-speech gesture, and this model can be extended for co-sign gesture and other aspects of signed languages. Researchers working within simulation-based frameworks suggest that gestures schematically reﬂect underlying visual and motor imagery (Hostetter & Alibali, 2008, 2010;

30

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

Parrill, 2010). If signers and gesturers generate similar mental and motor imagery, we should expect to see similar behaviors despite the very different constraints on how the manual modality is used by these two language groups. Conceptual universals could lead to two kinds of parallels in behavior. First, conceptual similarities can underlie the gestural bases of some signs. As Cormier et al. (2012) have argued, some lexical signs appear to have come from earlier gestural forms that represented, in iconic ways, aspects of the objects in focus (see also the work of Wilcox, e.g., 2004). The same gestural base that signers take advantage of for the creation of some signs is used by gesturers when they speak. The iconic representations are conceptually similar, but in the case of signed languages, the iconic has also become symbolic. (The analog in spoken language would be cases of sound symbolism: Buzz is both iconic and symbolic.) Second, conceptual universals can result in parallel behaviors that are non-linguistic (gestural). In this account, the parallel exists between signers and gesturers because of similar motor or mental imagery being activated in real time, but there is no additional layer of linguistic convention. As we noted earlier, differences across ASL and English may arise because of modality, and/or because of grammar, while similarities may arise because of gestural substrates of signed languages, and/or because of similarities in conceptual structure.
The data from the present study do not allow us to rule on whether similarities across signers are due to shared cultural bases (e.g., for ASL and English within a larger North American culture), shared conceptualizations that underlie communication of all peoples throughout the world, or grammaticalization of iconic gesture into signs and conventional communicative strategies in signed languages. However, further cross-linguistic data collection would shed some light on these questions.

5. Conclusions
In this study, we compared the strategies signers and co-speech gesturers used to communicate information about viewpoint in descriptions of the same video clips. We predicted that signers would use CA most frequently for those events that evoked CVPT gesture, and use CA the least frequently for those events that evoked OVPT exclusively. Likewise, we predicted that signers would use the most CL when gesturers use OVPT exclusively and the least CL with non-signers use CVPT exclusively.
Our predictions were supported for CL: Signers used the most CL when non-signers used OVPT, and they used the least CL when non-signers used CVPT. However, the CA cases were more complicated. For CA-handling, our predictions were correct: Signers used the most CA-handling when non-signers used CVPT and the least CA-handling when non-signers used OVPT. However, CA-affect and CA-torso results were not completely in accord with our predictions. Clearly, more work is needed to determine why CA-affect and CA-torso appear throughout all types of events—in spite of differences in CL use and CA-handling use.
One set of events was particularly interesting: those that elicited both types of viewpoints from non-signers. That category of events elicited the most frequent use of all depictive

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

31

devices, with the exception of CA-handling. Clearly, those events contained material that signers felt needed to be communicated via constructed action and classiﬁers. More work is needed to determine what characteristics of a scene would encourage this type of language use.
Signers also produced a comparatively small set of pointing forms that served to trace the path of an entity or object through space. These gestures were initially coded as part of the CL category of depiction devices, and they compare most readily with a type of OVPT gesture as produced by co-speech gesturers. However, their appearance in the sign data was so minimal that they were not included in the statistical analyses comparing different types of CA and CL productions. A question for future work concerns why these path gestures appear so infrequently in the signer’s narratives.
One particularly noticeable difference between signers and co-speech gesturers is that signers used multiple strategies when co-speech gesturers in the Parrill (2010) study primarily used one strategy. There were many instances in which the signers engaged the use of CLs while they were demonstrated bodily aspects of a character with their own torso, head, arms, etc. (i.e., the use of CA). Additionally, CA characterizations and CL depictions tended to be more detailed than examples of CVPT and OVPT use by co-speech gesturers. This highlights similarities across all communicators, but also notable differences that are likely inﬂuenced by the modality of the linguistic signal.
Our overall results support the claim that while signers use multiple strategies where non-signers may pick one exclusively, there are clear correspondences between signers and non-signers in the strategies that they use to communicate of viewpoint.

Acknowledgments
This article has beneﬁted from the insights and comments of three anonymous reviewers and the editors of this special volume, and we are grateful for their input. We thank the participants for their willingness to participate in this study. We also thank Lynn Hou, Cameron Larson, and Elena Liskova for their assistance with data coding.

Notes
1. Helpful reviews of these terms can be found in Parrill (2009) and Stec (2012). 2. Various taxonomies of signed language classiﬁers exist. See Schembri (2003) for a
detailed discussion of labels and categories of classiﬁers. 3. See Cormier et al. (2012) for examples where both linguistic and gestural features
could apply to instances of CA and CL. Essentially, the authors argue that the interaction between gesture and linguistic processes in sign reveals the inﬂuential gestural bases of signed languages. 4. See Morgan (2002) for a discussion of simultaneity and the use of different sign spaces in deaf children’s acquisition of BSL.

32

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

5. Vermeerbergen and Demey (2007) also discuss pointing signs in the context of distinguishing between different functions of a point in Flemish Sign Language. Unlike the present work, these authors are addressing two-handed signs in which the non-dominant hand is pointing to a location (or sweep of a location), while the dominant hand produces additional material.

References
Aarons, D., & Morgan, R. (2003). Classiﬁer predicates and the creation of multiple perspectives in South African Sign Language. Sign Language Studies, 3(2), 125–156.
Arik, E. (2009). Spatial language: Insights from sign and spoken languages. Unpublished doctoral dissertation. Purdue University.
Brentari, D., Coppola, M., Mazzoni, L., & Goldin-Meadow, S. (2012a). When does a system become phonological? Handshape production in gesturers, signers, and homesigners. Natural Language and Linguistic Theory, 30, 1–31.
Brentari, D., Nadolske, M., & Wolford, G. (2012b). Can experience with co-speech gesture inﬂuence the prosody of a sign language? Sign language prosodic cues in bimodal bilinguals. Bilingualism: Language and Cognition, 15, 402–412.
Brown, A. (2008). Gesture viewpoint in Japanese and English. Gesture, 8(2), 256–276. Casey, S., & Emmorey, K. (2009). Co-speech gesture in bimodal bilinguals. Language and Cognitive
Processes, 24(2), 290–312. Cormier, K., Quinto-Pozos, D., Schembri, A., & Sevcikova, Z. (2012). Lexicalisation and de-lexicalisation
processes in sign languages: Comparing depicting constructions and viewpoint gestures. Language and Communication, 32, 329–348. Debreslioska, S., O€ zy€urek, A., Gullberg, M., & Perniss, P. (2013). Gestural viewpoint signals referent accessibility, Discourse Processes, 50, 7 431–456. DOI: 10.1080/0163853X.2013.824286. Dudis, P. (2004). Body-partitioning and real-space blends. Cognitive Linguistics, 15(4), 223–238. Emmorey, K. (1999). Do signers gesture? In R. Campbell, & L. Messing (Eds.), Gesture, speech, and sign (pp. 133–159). New York: Oxford University Press. Emmorey, K., & Falgier, B. (1999). Talking about space with space: Describing environments in ASL. In E. Winston (Ed.), Storytelling and conversation, discourse in deaf communities (pp. 3–26). Washington, DC: Gallaudet University Press. Emmorey, K., Tversky, B., & Taylor, H. A. (2000). Using space to describe space: Perspective in speech, sign, and gesture. Journal of Spatial Cognition and Computation, 2, 157–180. Hostetter, A. B., & Alibali, M. W. (2008). Visible embodiment: Gesture as simulated action. Psychonomic Bulletin & Review, 15(3), 495–514. Hostetter, A. B., & Alibali, M. W. (2010). Language, gesture, action! A test of the Gesture as Simulated Action framework. Journal of Memory & Language, 63, 245–257. Kendon, A. (2004). Gesture: Visible action as utterance. Cambridge, UK: Cambridge University Press. Kita, S., & O€ zy€urek, A. (2003). What does cross-linguistic variation in semantic coordination of speech and gesture reveal? Evidence for an interface representation of spatial thinking and speaking. Journal of Memory & Language, 48(1), 16–32. Liddell, S. (2003). Grammar, gesture, and meaning in American Sign Language. Cambridge, UK: Cambridge University Press. Liddell, S. K., & Metzger, M. (1998). Gesture in sign language discourse. Journal of Pragmatics, 30, 657–697. McNeill, D. (1992). Hand and mind: What gestures reveal about thought. Chicago, IL: University of Chicago Press.

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

33

Metzger, M. (1995). Constructed dialogue and constructed action in American Sign Language. In C. Lucas (Ed.), Sociolinguistics in deaf communities (pp. 255–271). Washington, DC: Gallaudet University Press.
Morgan, G. (2002). Children’s encoding of simultaneity in British Sign Language narratives. Sign Language & Linguistics, 5(2), 131–165.
O€ zy€urek, A., & Perniss, P. (2011). Event representation in sign language: A crosslinguistic perspective. In J. Bohnemeyer & E. Pederson (Eds.), Event representation in language: Encoding events at the languagecognition interface. (pp. 84–107), Cambridge, UK: Cambridge University Press.
Parrill, F. (2009). Dual viewpoint gestures. Gesture, 9(3), 271–289. Parrill, F. (2010). Viewpoint in speech-gesture integration: Linguistic structure, discourse structure, and event
structure. Language and Cognitive Processes, 25(5), 650–668. Perniss, P. M. (2007a). Locative functions of simultaneous perspective constructions in German Sign
Language narratives. In M. Vermeerbergen, L. Leeson, & O. Crasborn (Eds.), Simultaneity in signed languages, form and function (pp. 27–54). Amsterdam: John Benjamins. Perniss, P. M. (2007b). Achieving spatial coherence in German Sign Language narratives: The use of classiﬁers and perspective. Lingua, 117, 1315–1338. Perniss, P. M. (2012). Use of sign space. In R. Pfau, M. Steinbach, & B. Woll (Eds.), Sign language: An international handbook (pp. 412–431). Berlin: Mouton de Gruyter. Perniss, P., Zwitserlood, I., & O€ zy€urek, A. (2011). Does space structure spatial language? Linguistic encoding of space in sign languages. In L. Carlson, C. Holscher, & T. Shipley (Eds.), Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (pp. 1595–1600). Austin, TX: Cognitive Science Society. Quinto-Pozos, D. (2007a). Can constructed action be considered obligatory? Lingua, 117(7), 1285–1314. Quinto-Pozos, D. (2007b). Why does constructed action seem obligatory? An analysis of classiﬁers and the lack of articulator-referent correspondence. Sign Language Studies, 7, 458–506. Quinto-Pozos, D., & Mehta, S. (2010). Register variation in mimetic gestural complements to signed language. Journal of Pragmatics, 42, 557–584. Quinto-Pozos, D., & Parrill, F. (2008). Enactment as a communicative strategy: A comparison between English co-speech gesture and American Sign Language. Comparison of Signed and Spoken Languages. Bamberg, Germany. Quinto-Pozos, D., Parrill, F., Stec, K, & Rimehaug, S. (2014). Linguistic, gestural, and cinematographic viewpoint: An analysis of ASL and English. International Society for Gesture Studies, University of California San Diego, San Diego, California. Schembri, A. (2003). Rethinking “classiﬁers” in signed languages. In Perspectives on classiﬁer constructions in sign languages, ed.K. Emmorey (pp. 3 –34). Mahwah, NJ: Erlbaum. Sevcikova, Z. (2014). Categorical versus gradient properties of handling handshapes in British Sign Language (BSL): Evidence from handling handshape perception and production by deaf BSL signers and hearing speakers. Unpublished doctoral dissertation. University College London. Shaw, E. (2013). Gesture in multiparty interaction: A study of embodied discourse in spoken English and American Sign Language. Unpublished doctoral dissertation. Georgetown University. Stec, K. (2012). Meaningful shifts: A review of viewpoint markers in co-speech gesture and sign language. Gesture, 12(3), 327–360. Vermeerbergen, M., & Demey, E. (2007). Sign + gesture = speech + gesture? Comparing aspects of simultaneity in Flemish Sign Language to instances of concurrent speech and gesture. In M. Vermeerbergen, L. Leeson, & O. Crassborn (Eds.), Simultaneity in signed languages form and function (pp. 257–282). Amsterdam: John Benjamins. Wilcox, S. (2004). Gesture and language: Cross-linguistic and historical data from signed languages. Gesture, 4(1), 43–75. Zwitserlood, I. (2003). Classifying hand conﬁgurations in Nederlandse Gebarentaal (Sign Language of the Netherlands). Doctoral dissertation. Utrecht, The Netherlands: LOT Trans 10. Zwitserlood, I. (2012). Classiﬁers. In R. Pfau, M. Steinbach, & B. Woll (Eds.), Sign language, an interntational handbook. Berlin: Walter de Gruyter.

34
Appendix

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

Table A1 Events evoking only C-VPT gestures
Event #
K2 K4 K5 K6 K7 K11 K13 R2 R3 R7 R13 R16 R17 R18 S1 S3 S10 S11

Event Description
Woman complains about dog (taps foot) Kitten is on dog’s back Dog puts kitten on shelf Dog covers kitten with bowl Dog agrees w/woman (nods) Dog is laughing Dog is alarmed Rabbit throws ball Rabbit looks alarmed Rabbit reads paper Rabbit hooks self onto ﬂagpole Rabbit throws mitt up Mitt catches ball Mitt falls back onto hand Skunk holds cat and kisses Skunk shrugs Cat closes door Cat leans against door, sighs with relief

Table A2 Events evoking only O-VPT gestures
Event #
K1 K3 K8 K14 K15 R4 R5 R6 R8, 12, 15 R9 R10 R11 S2 S5 S6 S7 S8 S9

Description
Dog walks up to house Dog comes into room Kitten is going into room and dog follows Ball (with kitten on top) rolls toward woman Ball (with kitten on top) rolls into woman’s leg Ball ﬂies out of stadium Rabbit chases ball out of stadium Rabbit takes bus to building Ball ﬂies overhead Bus arrives at building, rabbit gets off Rabbit takes elevator to top of building Rabbit gets off elevator, runs across roof Cat escapes Cat runs, banking turn on wall Skunk follows, hopping Cat goes up stairs Skunk follows up stairs Cat goes into room

D. Quinto-Pozos, F. Parrill / Topics in Cognitive Science 7 (2015)

35

Table A3 Events evoking both C- and O-VPT gestures
Event #
K10 K12 R14 S4

Description
Kitten is playing w/ball (tumbling on & off) Kitten scrambles on ball, ball starts to roll Rabbit pulls self up ﬂagpole Skunk hops after cat

Copyright of Topics in Cognitive Science is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.

