THE ORIGINS OF GRAMMAR Language in the Light of Evolution II

Language in the Light of Evolution
This work consists of two closely linked but self-contained volumes in which James Hurford explores the biological evolution of language and communication and considers what this reveals about language and the language faculty. In the ﬁrst book the author looks at the evolutionary origins of meaning ending at the point where humanity’s distant ancestors were about to acquire modern language. In the second he considers how humans ﬁrst began to communicate propositions to each other and how the grammars developed that enable communication and underlie all languages.
Volume I The Origins of Meaning
Volume II The Origins of Grammar

THE ORIGINS OF GRAMMAR
James R. Hurford
1

3
Great Clarendon Street, Oxford ox2 6dp Oxford University Press is a department of the University of Oxford. It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide in Oxford New York
Auckland Cape Town Dar es Salaam Hong Kong Karachi Kuala Lumpur Madrid Melbourne Mexico City Nairobi
New Delhi Shanghai Taipei Toronto With ofﬁces in
Argentina Austria Brazil Chile Czech Republic France Greece Guatemala Hungary Italy Japan Poland Portugal Singapore South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press in the UK and in certain other countries
Published in the United States by Oxford University Press Inc., New York
© James R. Hurford 2012
The moral rights of the author have been asserted Database right Oxford University Press (maker)
First published by Oxford University Press 2012
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press, or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above
You must not circulate this book in any other binding or cover and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data Data available
Library of Congress Cataloging in Publication Data Data available
Typeset by SPI Publisher Services, Pondicherry, India Printed in Great Britain on acid-free paper by
CPI Antony Rowe, Chippenham, Wiltshire
ISBN 978–0–19–920787–9
1 3 5 7 9 10 8 6 4 2

Contents

Detailed Contents

vi

Preface

x

Acknowledgements

xiv

Part I Pre-Grammar

Introduction to Part I: Twin Evolutionary Platforms—Animal Song

and Human Symbols

1

1 Animal Syntax? Implications for Language as Behaviour

3

2 First Shared Lexicon

100

Part II What Evolved

Introduction to Part II: Some Linguistics—How to Study Syntax

and What Evolved

173

3 Syntax in the Light of Evolution

175

4 What Evolved: Language Learning Capacity

259

5 What Evolved: Languages

371

Part III What Happened

Introduction to Part III: What Happened—the Evolution of Syntax

481

6 The Pre-existing Platform

483

7 Gene–Language Coevolution

539

8 One Word, Two Words, . . .

585

9 Grammaticalization

640

Sendoff

676

Bibliography

677

Index

767

Detailed Contents

Preface

x

Acknowledgements

xiv

Part One Pre-Grammar

Introduction to Part I: Twin Evolutionary Platforms—Animal

Song and Human Symbols

1

1. Animal Syntax? Implications for Language as Behaviour

3

1.1. Wild animals have no semantically compositional syntax

6

1.1.1. Bees and ants evolve simple innate compositional systems 6

1.1.2. Combining territorial and sexual messages

12

1.1.3. Combinatorial, but not compositional, monkey

and bird calls

14

1.2. Non-compositional syntax in animals: its possible relevance

18

1.3. Formal Language Theory for the birds, and matters arising

24

1.3.1. Simplest syntax: birdsong examples

34

1.3.2. Iteration, competence, performance, and numbers

45

1.3.3. Hierarchically structured behaviour

56

1.3.4. Overt behaviour and neural mechanisms

72

1.3.5. Training animals on syntactic ‘languages’

85

1.4. Summary, and the way forward

96

2. First Shared Lexicon

100

2.1. Continuity from primate calls

101

2.1.1. Referentiality and glimmerings of learning

101

2.1.2. Monkey–ape–human brain data

104

2.1.3. Manual gesture and lateralization

114

2.1.4. Fitness out of the here and now

117

2.2. Sound symbolism, synaesthesia, and arbitrariness

121

2.2.1. Synaesthetic sound symbolism

122

2.2.2. Conventional sound symbolism

128

2.3. Or monogenesis?

133

detailed contents

vii

2.4. Social convergence on conventionalized common symbols

137

2.5. The objective pull: public use affects private concepts

153

2.6. Public labels as tools helping thought

163

Part Two What Evolved

Introduction to Part II: Some Linguistics—How to Study

Syntax, and What Evolved

173

3. Syntax in the Light of Evolution

175

3.1. Preamble: the syntax can of worms

175

3.2. Language in its discourse context

180

3.3. Speech evolved ﬁrst

190

3.4. Message packaging—sentence-like units

197

3.5. Competence-plus

207

3.5.1. Regular production

207

3.5.2. Intuition

208

3.5.3. Gradience

223

3.5.4. Working memory

233

3.6. Individual differences in competence-plus

242

3.7. Numerical constraints on competence-plus

251

4. What Evolved: Language Learning Capacity

259

4.1. Massive storage

261

4.2. Hierarchical structure

270

4.2.1. What is sentence structure?

271

4.2.2. Sentence structure and meaning—examples

278

4.3. Word-internal structure

291

4.4. Syntactic categories

297

4.4.1. Distributional criteria and the proliferation of categories 299

4.4.2. Categories are primitive, too—contra radicalism

304

4.4.3. Multiple default inheritance hierarchies

309

4.4.4. Features

314

4.4.5. Are phrasal categories primitives?

319

4.4.6. Functional categories—grammatical words

326

4.4.7. Neural correlates of syntactic categories

329

4.5. Grammatical relations

336

4.6. Long-range dependencies

339

4.7. Constructions, complex items with variables

348

viii

detailed contents

4.8. Island constraints

362

4.9. Wrapping up

369

5. What Evolved: Languages

371

5.1. Widespread features of languages

372

5.2. Growth rings—layering

374

5.3. Linguists on complexity

378

5.4. Pirahã

389

5.5. Riau Indonesian

401

5.6. Creoles and pidgins

415

5.6.1. Identifying creoles and pidgins

415

5.6.2. Substrates and superstrates

420

5.6.3. Properties of pidgins and creoles

421

5.7. Basic Variety

442

5.8. New sign languages

445

5.8.1. Nicaraguan Sign Language

448

5.8.2. Al-Sayyid Bedouin Sign Language

456

5.9. Social correlates of complexity

460

5.9.1. Shared knowledge and a less autonomous code

462

5.9.2. Child and adult learning and morphological complexity 470

5.9.3. Historico-geographic inﬂuences on languages

476

Part Three What Happened

Introduction to Part III: What Happened—The Evolution

of Syntax

481

6. The Pre-existing Platform

483

6.1. Setting: in Africa

483

6.2. General issues about evolutionary ‘platforms’

486

6.3. Pre-human semantics and pragmatics

489

6.4. Massive storage

493

6.5. Hierarchical structure

495

6.5.1. Kanzi doesn’t get NP coordinations

495

6.5.2. Hierarchical structure in non-linguistic activities

498

6.5.3. Hierarchical structure in the thoughts expressed

507

6.6. Fast processing of auditory input

510

6.7. Syntactic categories and knowledge representation

515

6.8. Constructions and long-range dependencies

519

detailed contents

ix

6.8.1. Constructions and plans for action

520

6.8.2. Syntax, navigation, and space

527

7. Gene–Language Coevolution

539

7.1. Fast biological adaptation to culture

539

7.2. Phenotype changes—big brains

544

7.3. Genotype changes—selection or drift?

549

7.4. The unique symbolic niche

560

7.4.1. Relaxation of constraints

560

7.4.2. Niche construction and positive selection

563

7.4.3. Metarepresentation and semantic ascent

569

7.5. Learning and innateness

576

8. One Word, Two Words, . . .

585

8.1. Syntax evolved gradually

585

8.2. One-word utterances express propositions

596

8.3. Shades of protolanguage

605

8.4. Packaging in sentence-like units

612

8.5. Synthetic and analytic routes to syntax

621

9. Grammaticalization

640

9.1. Setting: in and out of Africa

640

9.2. Introducing grammaticalization

645

9.3. Topics give rise to nouns

648

9.4. Topics give rise to Subjects

656

9.5. Emergence of more speciﬁc word classes

661

9.6. Morphologization

667

9.7. Cognitive and social requirements for grammaticalization

670

Sendoff

676

Bibliography

677

Index

767

Preface
This book takes up the thread of a previous book, The Origins of Meaning. That was the easy bit. That book traced the basic precursors in animal behaviour of the kinds of meanings conveyed in human language. The ﬁrst half of that book explored animals’ private conceptual representations of the world around them; I argued for a form of prelinguistic representation that can be called proto-propositions. The second half of the book explored the beginnings of communication among animals. Animal communication starts with them merely doing things to each other dyadically, for example threatening and submitting. In some animal behaviour we also see the evolutionary seeds of triadically referring to other things, in joint attention to objects in the world. In the light of evolutionary theory, I also explored the social and cognitive conditions that were necessary to get a public signalling system up and running. Thus, at the point where this book begins, some of the deepest foundations of modern human language have been laid down.
The earlier book left off at a stage where our non-human animal ancestors were on the brink of a capacity for fully modern human language. Stepping over that brink took our species into a dramatic cascade of consequences that gave us our present extraordinarily developed abilities. We modern humans have a capacity for learning many thousands of arbitrary connections between forms and meanings (i.e. words and constructions); for expressing a virtually unlimited range of propositions about the real world; for conjuring up abstract, imaginary, and ﬁctitious worlds in language; and for conveying many layers of subtle nuance and degrees of irony about what we say. All of these feats are performed and interpreted at breakneck speed, and there are thousands of different roughly equivalent ways of achieving this (i.e. different languages) which any normal human child can acquire in little over half a dozen years. Simplifying massively for this preface, the core of the starting discussion here will be about what must have been the earliest stages of humans communicating propositions to each other using learned arbitrary symbols, and beginning to put these symbols into structured sequences. These are the origins of grammar.
The later evolution of grammar into the fantastically convoluted structures seen in modern languages follows on from this core. In tracking the possible

preface

xi

sequence of these rapid and enormous evolutionary developments as discussed here, it will not be necessary to know the earlier book to take up the thread of the story in this book. The essential background will be sketched wherever necessary.
It was convenient to organize the present book in three parts. The whole book does tell a cohesive story, but each part is nevertheless to some degree selfcontained. Depending on the depth and breadth of your interests in language evolution, it could be sufﬁcient to read just one part.
Part I, Pre-Grammar, explores one possible and one necessary basis for human linguistic syntax. Chapter 1 surveys syntactically structured, but semantically non-compositional, communicative behaviour in non-human animals, such as birds and whales, suggesting even at this point some methodological conclusions about how to approach human syntax. Chapter 2 discusses the likely routes by which shared vocabularies of learned symbols could have evolved, and the effects on human thought.
Part II, What Evolved, gets down to real grammar. Following the now accepted wisdom that ‘evolution of language’ is an ambiguous expression, meaning either the biological evolution of the language faculty or the cultural evolution of particular languages, two chapters in this part ﬂesh out respectively what seems to be the nature of the human language faculty (Chapter 4) and the nature of human languages (Chapter 5). Before this, in Chapter 3, a theoretical background is developed, informed by evolutionary considerations, trying to chart a reasonable path through the jungle of controversy that has surrounded syntactic theorizing over the last half-century. This part contains much material that will be familiar to linguists, even though they may not agree with my take on it. So Part II, I believe, can serve a useful tutorial function for non-linguists interested in language evolution (many of whom need to know more about the details of language).
Finally, Part III, What Happened, tells a story of how it is likely that the human language faculty and human languages evolved from simple beginnings, such as those surveyed in Part I, to their present complex state. The emphases developed in Part II provide a springboard for this story, but it should be possible to follow the story without reading that previous part. If you feel that some assumptions in the story of Part III need bolstering, you should ﬁnd the necessary bolsters in Part II.
In all three parts, linguists can obviously skip the more ‘tutorial’ passages throughout, except where they feel impelled to disagree. But I hope they will withhold disagreement until they have seen the whole broad picture.
Years ago I conceived of a relatively short book which would discuss the whole sweep of language structure, from phonetics to pragmatics, in the light

xii

preface

of evolution, with each chapter dealing with one ‘component’: pragmatics, semantics, syntax, phonology, and phonetics. I still teach a very condensed course like that, on the origins and evolution of language. The book project ‘just growed’. The time has passed for simple potted versions. I managed to say something about the evolution of semantics and pragmatics in The Origins of Meaning, and this book, The Origins of Grammar takes a hefty stab at the evolution of syntax. That leaves the origins of speech—phonetics and phonology. I have some ideas about that, but it’s clear that a third book would take too long for patient publishers to commit to now. Besides, (1) there is already a lot of solid and interesting material out there,1 and (2) the ﬁeld is probably maturing to the point where original contributions belong in journal articles rather than big books. So don’t hold your breath for a trilogy. I’m also aware that I may have given morphology short shrift in this book; see Carstairs-McCarthy (2010) for some stimulating ideas about that.
The content and style of this book are born of a marriage of conviction and doubt. I am convinced of the continuity and gradualness of evolutionary change, and have been able to see these properties where discreteness and abruptness were assumed before. On the empirical facts needed to sustain a detailed story of continuity and gradualness, my insecurity in the disciplines involved has compelled me always to interrogate these disciplines for backup for any factual generalizations I make. This is a thousandfold easier than in the past, because of the instant availability of online primary sources. There can now be far fewer excuses for not knowing about some relevant counterexample or counterargument to one’s own ideas. So this book, like the last, brings together a broad range of other people’s work. I have quoted many authors verbatim, because I see no reason to paraphrase them and perhaps traduce them. Only a tiny fraction of the primary research described is mine, but the broad synthesis is mine. I hope you like the broad synthesis.
While you read, remember this, of course: we never know it all, whatever it is. Human knowledge is vast in its range and impressive in its detail and accuracy. Modern knowledge has pushed into regions undreamt of before. But all knowledge expressed in language is still idealization and simpliﬁcation. Science proceeds by exploring the limits of idealizations accepted for the convenience of an era. The language of the science of language is especially reﬂexive, and evolves, like language. What seemed yesterday to be The Truth turns out to have been a simpliﬁcation useful for seeing beyond it. There is progress, with

1 For example Lieberman (1984); de Boer (2001); Oudeyer (2006); Lieberman (2007); MacNeilage (2008); Fitch (2010).

preface

xiii

thesis, antithesis and synthesis. As we start to label and talk about what we glimpse beyond today’s simpliﬁcations and idealizations, we move onward to the next level of idealizations and simpliﬁcations. The evolution of language and the evolution of knowledge run in the same direction. We know more and know it more accurately, but, as public knowledge is couched in language, we will never know ‘it’ all or know ‘it’ exactly right.

Acknowledgements
I have wobbled on the shoulders of many giants. Naming and not naming are both invidious, but I risk mentioning these among the living: Bernd Heine, Bill Croft, Chuck Fillmore, Derek Bickerton, Dick Hudson, Haj Ross, Joan Bybee, Mike Tomasello, Peter Culicover, Pieter Seuren, Ray Jackendoff, Talmy Givón, Terry Deacon. They in their turn have built upon, or reacted against, the thinking of older giants. More generally, I am appreciative of the hard work of linguists of all persuasions, and of the biologists and psychologists, of all sub-branches, who made this subject. Geoff Sampson, Maggie Tallerman, and Bernard Comrie read the whole book and made recommendations which I was (mostly) glad to follow. They saved me from many inaccuracies and confusions. I am grateful for their impressive dedication.
And, lastly, all of the following have also been of substantial help in one way or another in building this book, and I thank them heartily: Adele Abrahamsen, Giorgos Argyropoulos, Kate Arnold, the ghost of Liz Bates, Christina Behme, Erin Brown, Andrew Carstairs-McCarthy, Chris Collins, Karen Corrigan, Sue N. Davis, Dan Dediu, Jan Terje Faarlund, Nicolas Fay, Julia Fischer, Simon Fisher, Tecumseh Fitch, Bruno Galantucci, Tim Gentner, David Gil, Nik Gisborne, Patricia Greenﬁeld, Susan Goldin-Meadow, Tao Gong, Stefan Hoeﬂer, David Houston, Melissa Hughes, Eve, Rosie, and Sue Hurford, Simon Kirby, Karola Kreitmair, Albertine Leitão, Stefan Leitner, Anthony Leonardo, Gary Marcus, Anna Martowicz, Miriam Meyerhoff, Lisa Mikesell, Katie Overy, Katie Pollard, Ljiljana Progovac, Geoff Pullum, Frank Quinn, Andrew Ranicki, Katharina Riebel, Graham Ritchie, Constance Scharff, Tom Schoenemann, John Schumann, Peter Slater, Andrew Smith, Kenny Smith, Mark Steedman, Eörs Szathmáry, Omri Tal, Mónica Tamariz, Carrie Theisen, Dietmar Todt, Hartmut Traunmüller, Graeme Trousdale, Robert Truswell, Neal Wallace, Stephanie White, Jelle Zuidema. Blame me, not them, for the ﬂaws.

Part One: Pre-Grammar
Introduction to Part I: Twin Evolutionary Platforms—Animal Song and Human Symbols
Before complex expressions with symbolic meaning could get off the ground, there had to be some facility for producing the complex expressions themselves, even if these were not yet semantically interpreted. Birds sing in combinations of notes, but the individual notes don’t mean anything. A very complex series of notes, such as a nightingale’s, only conveys a message of sexual attractiveness or a threat to rival male birds. So birdsong has syntax, but no compositional semantics. It is the same with complex whale songs. Despite this major difference from human language, we can learn some good lessons from closer study of birds’ and whales’ songs. They show a control of phrasal structure, often quite complex. The songs also suggest that quantitative constraints on the length and phrasal complexity of songs cannot be naturally separated from their structure. This foreshadows a conclusion about how the human language faculty evolved as a composite of permanent mental structure and inherent limits on its use in real-time performance.
Also before complex expressions with symbolic meaning could get off the ground, there had to be some facility for learning and using simple symbols, arbitrary pairings of form and meaning. I argue, contrary to views often expressed, for some continuity between ape cries and human vocalized words. There was a transition from innate involuntary vocalizations to learned voluntary ones. This was a biological development to greater behavioural plasticity in response to a changing environment. The biological change in the make-up of individuals was accompanied by the development in social groups of shared conventions relating signals to their meanings. One pathway by which this growth of shared social norms happened capitalized on sound symbolism and synaesthesia. Later, initially iconic form-meaning mappings became stylized to arbitrary conventions by processes which it is possible to investigate with

2

the origins of grammar

modern experiments. With the growth of a learned lexicon, the meanings denoted by the developed symbols were sharpened, and previously unthinkable thoughts became accessible.
Thus, the two chapters in this part survey the situation before any semblance of modern grammar was present, exploring the possibility of non-human antecedents for control of complex syntax and of unitary symbols, protowords. These two chapters deal respectively with pre-human semantically uninterpreted syntax and early human pre-syntactic use of symbols.

chapter 1
Animal Syntax? Implications for Language as Behaviour
The chapter heading poses a question, and I will answer it mostly negatively. Some wild communicative behaviour is reasonably described as having syntactic organization. But only some wild animal syntax provides a possible evolutionary basis for complex human syntax, and then only by analogy rather than homology. That is, we can ﬁnd some hierarchical phrase-like syntactic organization in species distantly related to humans (e.g. birds), but not in our closer relatives (e.g. apes). The chapter is not, however, a wild goose chase. It serves (I hope) a positive end by clarifying the object of our search. Non-linguists ﬁnd linguists’ discourse about syntax quite impenetrable, and the chapter tries to explain some theoretical points that cannot be ignored when considering any evolutionary story of the origins of human syntax. Using a survey of animal syntactic abilities as a vehicle, it will introduce and discuss some basic analytic tools applicable to both human and non-human capacities. These include such topics as semantic compositionality (as opposed to mere combinatorial structure), the competence/performance distinction, the hierarchical structuring of behaviour and the relation of overt behaviour to neural mechanisms. A special tool originating in linguistics, Formal Language Theory (FLT), will come in for particular scrutiny. This body of theory is one of the most disciplined and mathematical areas of syntactic theorizing. FLT is on ﬁrmer formal ground than descriptive syntactic theories, giving an accumulation of solid results which will stand the test of time. These are formal, not empirical, results, in effect mathematical proofs. Some may question the empirical applicability of Formal Language Theory to human language. It does

4

the origins of grammar

give us a precise yardstick by which to compare the syntax of animal songs and human language. It will become clear to what extent any syntactic ability at all can be attributed to songbirds and some whales.
The ﬁrst section below will, after a survey of candidates among animals, reinforce the point that animals in the wild indeed do not have any signiﬁcant semantically interpreted syntax. The second and third sections will examine how much, or how little, the non-semantic syntactic abilities of animals can tell us. I will illustrate with facts about the songs of birds and whales,1 fascinating in themselves. To make the proper comparison between these songs and human syntax, it is necessary to introduce some key concepts underlying the analysis of syntax in humans. Discussing these key concepts in a context away from the common presuppositions of linguistics allows us to reconsider their appropriateness to human language, and to suggest some re-orientation of them. So some of this chapter is theoretical and terminological ground-clearing, spiced up with interesting data from animals.
Syntax, at its most basic, is putting things together. Of course, ‘putting things together’ is a metaphor, but a signiﬁcantly insightful one. Syntactic spoken language is not literally a putting together in the sense in which bricks are put together to make a wall, or fruit and sugar are put together to make jam. Speech is serial behaviour, but serial behaviours differ in the complexity of control of putting things together. Breathing is basic and can be described as putting certain routines of muscular contraction together in a prolonged sequence. Walking is a bit more complex, and the way strides are put together involves more volition and sensory feedback from the surroundings. All animals put actions together in serial behaviour. Indeed that is a deﬁning characteristic of animals, who seem to have some ‘anima’,2 dictating the order of their movements. In all animals many action sequences are instinctive, somehow programmed into the genome, without any shaping by the environment in the individual’s lifetime. Quite a lot of animals also learn motor sequences, used for practical purposes of survival. With most sequences of actions carried out by animals, the environment provides constant feedback about the state reached and prompts the animal for its next step. For example a gorilla picks, packages, and eats nettles in a somewhat complex way (Byrne and Byrne 1991; Byrne 1995). All through this systematic behaviour the animal is getting feedback in

1 Birdsong and whale songs are typical enough to make my general points, and space prohibits discussion of gibbons and other singing species.
2 The etymology of animal reﬂects a narrowing of Aristotle’s concept of anima or ψ υ χ η, which he saw as the essence of all living things, including plants. Aristotle’s anima is often translated as soul, but he did not regard it as a non-physical substance.

a n i m a l sy n ta x? language as behaviour

5

the form of the current state of the nettles, whether they are (1) still growing undisturbed in the earth, (2) with stalk held tightly in the gorilla’s right hand, (3) stripped of their leaves, held in the left hand, or (4) leaves folded into a package and ready to pop into the mouth. There are millions of such examples, wherever an animal is dealing systematically with its environment. Much rarer are learned routines of serial behaviour not scaffolded throughout the sequence by feedback from the environment. During the singing of a nightingale’s song, there are no external landmarks guiding it to its next note. All the landmarks are within, held in the animal’s memorized plan of the whole complex routine. Most, and maybe all, such complex ‘unguided’ routines are communicative, giving information to conspeciﬁcs. Although all complex serial behaviour has a kind of ‘syntax’ or ‘grammar’, I will restrict the term ‘syntax’ in the rest of this work to complex, unguided communicative routines. No doubt, a specialized facility for syntax in this narrow sense evolved out of a facility for serial behaviour more generally.
A fancier term for ‘putting things together’ is combinatorial. Music has combinatorial syntax, because it involves putting notes together in strictly deﬁned ways. Different musical traditions are roughly like different languages, in the sense that they deﬁne different rules for combining their elementary constituents—notes for music, and words for language. Dances, the tango, the waltz, the Scottish country dance Strip-the-Willow, each have their own syntax: ways of putting the elementary moves together into an approved sequence. The syntax of such human activities tends to be normative, hence the use of ‘approved’ here. But behaviour can be syntactically organized without the inﬂuence of any norms made explicit in the social group, as we will see in this chapter when discussing the structured songs of birds and whales. (This does not, of course, mean that syntactic organization cannot be inﬂuenced by the behaviour of others, through learning.)
Peter Marler (1998) distinguishes between phonological syntax and lexical syntax. In its broad sense of putting things together, syntax applies to phonology. Phonology puts phonemes together to make structured syllables. Each language has its own phonological syntax, or sound pattern. The units put together in phonology don’t mean anything. The English /p/ phoneme, on its own, carries no meaning. Nor does any other phoneme. And it follows that the syllables put together out of phonemes can’t mean anything that is any function of the meanings of the phonemes, because they have no meanings. Cat does not mean what it means because of any meanings inherent in its three phonemes /k/, /a/, and /t/. Phonological syntax is the systematic putting of meaningless things together into larger units. Birdsong, whale song and gibbon song all exhibit phonological syntax, and I will discuss two of these in the third section

6

the origins of grammar

below. It is possible that some phonological syntactic ability developed in our species independent of meaning, which is why I devote space to these complex non-human songs.
Lexical syntax, or lexicoding, as Marler calls it, is the kind of putting things together where the elements mean something, and the whole assembly means something which is a reﬂection of the meanings of the parts. This is compositionality. Complex meanings are expressed by putting together smaller meaningful units. As Marler summarizes it, ‘Natural lexicoding appears to be a purely human phenomenon. The only animals that do anything remotely similar have been tutored by humans’ (Marler 1998, p. 11). In order to be clear that this is indeed the case, the ﬁrst section of this chapter will look at some challenges to Marler’s assertion that have surfaced since he wrote. With some tiny reservations, Marler’s assertion stands. (Marler mentioned animals tutored by humans. We will come to them in a later chapter.)
I will weave into the second and third sections of this chapter an introduction to Formal Language Theory. On its own, such an introduction might seem both dry and unmotivated. But the Formal Language Theory approach to repertoires of complex meaningless songs3 turns out to give a useful way of classifying the overt characteristics of song repertoires. The approach also draws out some differences and similarities between these animal songs and human languages that push us to question some of our common assumptions about human language.

1.1 Wild animals have no semantically compositional syntax
This section describes some non-starters as candidates for evolutionary analogues or homologues of human semantically compositional syntax. In brief, no close analogues or homologues are to be found in wild animal communication systems. But surveying cases that show, or might appear to show, some compositionality can clarify what exactly we are looking for.
1.1.1 Bees and ants evolve simple innate compositional systems
Insects are only very distantly related to humans. But even some insects put elements together in a semantically composed signal. Parts of the signal are
3 The songs are not wholly meaningless, of course, or the animals would not sing them. I mean that the songs do not convey referential meanings by combining the meanings of their elementary parts. One way of putting this is to say that the songs have pragmatic, but not semantic, signiﬁcance.

a n i m a l sy n ta x? language as behaviour

7

combined to express a message which is a function of the meanings of the parts. These communication systems are (1) extremely simple, comprising only two meaningful elements, (2) extremely limited in the domain to which they apply—location of food or a good hive site, and (3) innate. These simple systems are adaptive, enhancing the survival chances of the animals. How far can nature go in engineering a genetically ﬁxed semantically compositional system? The insect systems seem to be the limit. There are no more complex innate systems in nature. Without learning, a semantically compositional system cannot evolve beyond the narrowest limits we see in a few insects. So we have an important conclusion here already. Highly complex semantically compositional systems need to be learned. Now I’ll brieﬂy survey what we know about the unlearned insect systems. In their way, they are impressive, but impressive in a totally different way from the wonders of human language, which has evidently taken a different evolutionary course.
The honeybee, Apis mellifera, provides a well known example of animal communication. Surprisingly, for an animal genetically so far distant from us, bees use a simple, but arguably semantically compositional, system.4 They signal the location of food relative to the hive by a vector with two components, a distance component and a direction component. Distance is signalled in analogue fashion by the duration of the ‘waggle’ dance—the longer the dance, the farther away is the food. And direction is signalled by the angle to the vertical of the waggle dance: this corresponds to the angle relative to the sun’s position in which the food lies. Thus a fairly precise location is described in terms of two components and each component is signalled by a separate aspect of the overall signal. The receiving bees may possibly be said in some sense to ‘compose’ the location from its elements, direction and distance.
The question arises, however, whether this description is our own anthropomorphic account of their behaviour. The bee observing the dance no doubt registers somehow the two components of the signal, and responds systematically to both, by ﬂying a certain distance in a certain direction. And then, of course, it gets to roughly the right place. But it does not follow that the bee has in its brain any representation of the place it is going to before it actually gets there. If I give you precise latitude and longitude speciﬁcations of a place, you can consult a map and know what place I am talking about.

4 The summary of bee communication given here is basic and omits many fascinating details of the variety between species, and the scope of their responses to different environmental conditions. For a highly informative and readable account, see Lindauer (1961). Other signiﬁcant works are von Frisch (1923a, 1923b, 1967, 1974); Riley et al. (2005).

8

the origins of grammar

Or, more familiarly, if I say ‘You know, the pub two hundred yards south of here’, you will identify what I mean, and we can talk about it, without either of us necessarily ﬂying off there. There is some evidence that bees can do this as well.5 Gould (1986) showed that bees could ﬁnd their way directly to a feeder station when released at a novel site away from the hive, and construed this as evidence that the bees were computing the new route by reference to a cognitive map. The term ‘cognitive map’ requires some unpacking. For Gould, it was consistent with ‘landmark map’, and his bees could be taken to be ﬁnding their way by reference to familiar landmarks. It is accepted that bees use landmarks in their navigation. On the basis of more carefully controlled experiments, Dyer (1991) argues, however, that the bees acquire ‘route-based memories’ but not cognitive maps. Dyer released his experimental bees in a site, a quarry, from where they could not see landmarks visible from the hive. On release from the quarry, they tended to ﬂy off on a compass bearing close to that on which they would have ﬂown from the hive, that is in a wrong direction. Dyer concludes that his ‘results suggest that honey bees do not have the “mental maps” posited by Gould (1986), or any other mechanism to compute novel short cuts between familiar sites that are not in view of each other’ (p. 245). Nevertheless, it is clear that signalling bees do base their performances on a computation of several factors. ‘Fully experienced bees orient their dances on cloudy days by drawing upon an accurate memory of the sun’s entire course relative to familiar features of the terrain’ (Dyer and Dickinson 1994, p. 4471). More recently, and using hi-tech radar equipment, Menzel et al. (2005) were able to track the entire ﬂights of bees. They concluded:

Several operations must be at the disposal of the animal: (i) associations of headings

and distance measures toward the hive with a large number of landmarks all around

the hive that are recognized from different directions; (ii) shift of motivation (ﬂight to

hive or feeder); (iii) reference to the outbound vector components of the route from hive

to feeder; and (iv) addition and subtraction of the heading and distance components for

at least two conditions, those that would lead directly back to the hive and those that

lead from the hive to the feeder. It is difﬁcult to imagine that these operations can be

done without reference to vectors that relate locations to each other and, thus, make

up a map.

(Menzel et al. 2005, p. 3045)

5 Reznikova (2007) cites Dyer (1991): ‘In the experiments of Dyer (1991), bees left the hive when the returning scout indicated that the food was beside a lake. However they did not leave the hive when they were informed that food was near the middle of the lake. Thus, honey bees appear to interpret the meaning of the dance—possibly by identifying the potential location of food, and then decide whether it is worth making the journey’. Unfortunately, this passage is not actually to be found in the cited article by Dyer, so the lake story must have come from somewhere else.

a n i m a l sy n ta x? language as behaviour

9

All these navigational experiments involve observing the ﬂights taken by bees, and are not directly about what is signalled in the honeybee waggle dance. Thus the compositional nature of the dance signal itself is not directly investigated. But the evidence for quite rich navigational abilities makes it seem unlikely that the response to the dance by bees already familiar with the landscape is entirely robot-like, following two instructions simultaneously, ﬂying for a certain distance in a certain direction. On the other hand, inexperienced bees, who have not become familiar with the local topology, can do nothing but follow the two components of the message conveyed by the waggle dance, insofar as the landscape allows them. On the evidence, the processing of the signal by experienced bees seems likely to be somewhat analogous to what happens when a human understands a phrase such as two hundred yards south-west of here, even when a straight-line walk to that location is not possible, because of the street layout. The human, if he already knows the locality, can make a mental journey by putting the two elements of meaning together, and perhaps never take the actual physical journey. The bee is almost as clever (in this very limited domain), but not quite. Von Frisch (1967) reviews experiments in which bees had to go around an obstacle such as a large ridge to get to their food, thus making a two-leg trip with an angle in it. On returning, their dance signalled the real compass direction of the food (which was not a direction of either leg of their ﬂight) and the actual distance ﬂown, around the obstacle. This shows impressive ability to integrate two ﬂown angles, and the distances ﬂown at those angles, into a single angle. But the location signalled was technically false, being further away from the hive (in a straight line) than the actual food source. One can see this as a simple evolutionary solution to the problem of signalling location over a restricted communication channel. The bee receiving the signal goes in the direction signalled, as best she can, for the distance signalled. Signalling a complex two-leg journey would be more of a challenge.6 This is a case where the bees’ private computational capacity, allowing them to do complex path integration, outstrips what they can communicate publicly. The given message is a simple synopsis of their more complex experience.
In later experiments, it was found that bees could be tricked into believing that they had ﬂown longer distances than they actually had. Srinivasan et al. (2000) trained bees to ﬂy, on either their outward or their inward ﬂight, through a tube painted with many closely-packed colours. After ﬂying through

6 Even humans asking for directions in a strange town ﬁnd it hard to remember oral instructions with more than about three legs.

10

the origins of grammar

such a tube, bees signalled distances much longer than the actual distances ﬂown. Following this up, De Marco and Menzel (2005) made bees take a 90◦ detour through a painted tube to get to their food. Once these bees had arrived at the food source they took a diagonal shortcut back to the hive, presumably relying on landmarks. The experimenters watched the signalling behaviour of the returning bees. They found that the bees signalled the direction of the shortcut route to the food, ﬁgured out from their return journey, but the perceived long distance ﬂown through the tube on their outward journey. On this evidence, bees can separate out two factors of their experience, the length (sometimes misperceived) of their outward ﬂight, and the direction of their return ﬂight. And they code these separate aspects of their experience into the waggle dance. This is compositional coding, but of course in an extremely limited domain, and is not learned behaviour.
Bees have an accurate sense of time and anticipate the movement of the sun across the sky as the day proceeds (Lindauer 1961; Dyer and Dickinson 1996; Dickinson and Dyer 1996). Bees who have received a message in the morning about the direction of food can be kept in the hive for a few hours, and when they are released later in the afternoon they compensate for the movement of the sun during the time they were cooped up. For example, if the waggle dance at noon signals due south, and the bees are released immediately, they ﬂy off directly towards the sun;7 but if after receiving that same signal at noon they are not released until 3.00 p.m., they don’t ﬂy directly towards the sun, but about 45◦ to the left of it. Thus the code is interpreted with some contextual ‘pragmatic’ input, namely the time elapsed since reception of the message. This is a lesson that simply having a code is not enough for practical communication. The information conveyed in a code is supplemented, even in such a simple system as honeybee dancing, by contextual information.8 (Fascinatingly, Lindauer also reports experiments in which bees who had been accustomed to the movement of the sun in one global hemisphere (i.e. left-toright in the northern and right-to-left in the southern) were shifted overnight to the other hemisphere. The originally transported bees did not adapt, but their descendants, after 43 days, did make the correct new adjustment, interpreting the direction aspect of the dance in the new appropriate way. See Lindauer (1961, pp. 116–26) and Kalmus (1956).

7 in the northern hemisphere. 8 Humans who leave a message on a door saying ‘Back in an hour’ seem oblivious of the importance to the receiver of such contextual information about when the message was written.

a n i m a l sy n ta x? language as behaviour

11

Some species of ants, socially organized like honeybees, also show evidence of semantically compositional signalling (Reznikova and Ryabko 1986; Reznikova 2007). It seems that ants communicate by contact with each other with their antennae. In controlled experiments, scout ants reported the location of food to teams of forager ants, who reliably followed the directions given by the scout through a series of T-junctions in a maze. There was individual variation: not all ants were very good at transmitting such information. In the case of the effective ant signallers, the evidence for compositional signalling is indirect. That is, the research has not ‘decoded’ the signals given by the ants into their component meaningful parts, as von Frisch did with the honeybees. Rather, the experimenters carefully controlled the amount of information, measured in bits as deﬁned by Information Theory (Shannon and Weaver 1963). Each turn taken at a T-junction in the maze counted as one bit of information. In complex cases, it was possible for the food to be located at a point six turns into the maze from the entrance. Not surprisingly, a correlation was found between the complexity of the message in bits (i.e. number of turns in the maze), and the time taken by ants to convey it.9 More signiﬁcantly, where there were regular patterns in the message to be conveyed, such as a succession of turns in the same direction (e.g. Right-Right-Right-RightRight, or Left-Left-Left-Left-Left), the time taken to convey such messages was shorter than in the case of less regularly structured messages, such as Right-Left-Left-Right-Left. This, as the authors point out, is evidence of data compression.
One way in which data compression can be achieved is with some kind of compositional coding, where one element of the code systematically denotes the way in which the data is to be compressed. For example, we can imagine (although we don’t know exactly) that a message such as Right-RightRight-Right-Right was compressed by the signalling ant into the equivalent of ‘All-Right’ or ‘Only-Right’. A less regularly structured message could not be compressed in this way, assuming obvious intuitions about what is ‘regular structuring’. We must remember that the natural environment of ants in the wild is unlikely to present them with routes so neatly deﬁned as a series of T-junctions in a lab maze. But the correlation between regularity in the message, measured in information bits, and duration of the signalling episode needs some explanation. The data raise the possibility that these ants have a semantically compositional (albeit very simple) code.

9 Three species of ant showed such behaviour in these experiments, Formica pratensis, F. sanguinea and F. rufa. (There are over 11,000 species of ant.)

12

the origins of grammar

However, the data also support another interpretation, which is that the ant signals are entirely holophrastic. That is, the ants may just have the equivalent of a lexicon, a lookup table in which each separate mapping from a meaning to a form is stored, with no general rules for constructing the signals from meaningful subparts. (This presupposes that the number of conveyable messages is ﬁnite, and presumably small.) The observed correlation between short signals and repetitively structured messages (e.g. Right-Right-Right-Right-Right) may come about through some tendency to associate such meanings with short signals, holophrastically. Information Theory tells us that more efﬁcient communication is achieved if the most frequent messages are coded as the shortest signals. This fact is illustrated by several well-known phenomena, including Zipf’s Law inversely correlating word frequency with word length, and Morse Code, in which the commonest English letter, E, is signalled by the shortest possible dot-dash sequence, namely a single dot. The messages to be conveyed by the ants in these experiments did not vary signiﬁcantly in frequency, so Information Theoretic efﬁciency of coding is probably not a driving force here. But there might be something salient about such repetitively structured meanings to ant brains which makes them assign them shorter signals. The fact of signal compression in itself does not necessarily imply compositionality in the code. Morse Code, for example, is not semantically compositional in its mappings from dots and dashes to letters: the letters of the alphabet are not treated as bundles of features, with each feature signalled by something in the code. Incidentally, humans ﬁnd it easier to remember sequences of digits, such as telephone numbers, if they contain repetitions; 666 1000 is much easier to remember than 657 3925.
These several species of bees and ants may have converged in their evolution on a common principle for efﬁcient information transmission, applying it in very limited ways, and in very narrow domains. These insect encoding and decoding systems are probably wholly innate. (This is not to deny that bees, at least, can learn to apply the messages of the signals appropriately in the context of their local landscape.) We are interested in syntactic systems with a much more signiﬁcant element of learning and with much wider expressive range.

1.1.2 Combining territorial and sexual messages
Birds’ songs typically express either a courtship or a territorial message— ‘Welcome, ladies’, or ‘Keep away, gents’. Can these two messages be combined into a single composite song? If so, could this ability to compose songs be a remote beginning of more complex semantically compositional syntax?

a n i m a l sy n ta x? language as behaviour

13

Chafﬁnches, unlike ants and bees, learn their songs to some extent. The characteristic chafﬁnch song is quite complex, as we will see later. It can be divided into two main parts, an initial ‘trill’ and a ﬁnal ‘ﬂourish’. The whole signal serves a dual function, acting both as a territorial challenge to other males and a way of attracting females. Using experimentally manipulated playback calls in the ﬁeld, Leitão and Riebel (2003, p. 164) found that ‘Males showed the closest approach to songs with a relatively short ﬂourish. . . . These were the songs found less attractive by females tested previously (Riebel and Slater 1998) with the same stimuli’. In other words, if the ﬂourish part of the song is short, males will tend to come a lot closer to other males than if the song has a longer ﬂourish. It would be an oversimpliﬁcation to say that the trill is a territorial challenge to rival males while the ﬂourish functions to attract females, but undoubtedly the two parts of the song do tend somewhat to emphasize these respective functions.
Dual function calls that serve both a territorial and a courtship function are common in nature. But it is not so common that different features of the call can be teased apart and analysed as serving the different functions. Another example is the coqui frog, named after the two parts of its simple call, a low note followed by a higher note (the reverse of a cuckoo call, and higher pitched overall). Here again, it seems that a separate meaning can be assigned to each separate part of the call, each serving a different function. ‘Acoustic playback experiments with calling males in their natural habitat and twochoice orientation experiments with females indicate that males and females of the neotropical tree frog Eleutherodactylus coqui respond to different notes in the two-note call of the male’ (Narins and Capranica 1976, p. 378). ‘In the Puerto Rican “Co Qui” treefrog, Eleutherodactylus coqui, the duration of the ﬁrst note “Co”, is critical in eliciting male territorial behavior, while the spectral content of the second note, “Qui”, is crucial in eliciting positive phonotaxic responses from females’ (Feng et al. 1990). The low ‘Co’ part of the call tends to serve a territorial function, while the higher ‘Qui’ part of the call tends to serve a courtship function.
Are these chafﬁnch and frog calls candidates for semantic compositionality, with the meaning of the whole call being formed by a combination of the meanings of its parts? No. The two meanings, territorial challenge and courtship invitation, are incompatible, and directed at different receivers. In the coqui frog, in fact, the male and female brains are tuned differently to be sensitive to the different parts of the call (Narins and Capranica 1976), so it is possible that neither male nor female actually hears the whole call, let alone puts its parts together. The parts of the chafﬁnch call cannot be combined in the way that distance and direction, for example, can be combined to yield location. The

14

the origins of grammar

closest to a compositional interpretation would be that the whole call conveys a conjunction of the meanings of the components.

1.1.3 Combinatorial, but not compositional, monkey and bird calls
Monkeys are more closely related to us than the insects, birds, and frogs that we have considered so far. Can we see any signs of semantically composed messages in monkeys? Klaus Zuberbühler is a leading investigator of this question. My conclusion from his work, surveyed below, is that some monkey communication is at the margins of semantic compositionality, expressing nothing even as complex as hit Bill. Likewise, there is no ﬁrm evidence of semantic compositionality in bird calls.
Arnold and Zuberbühler (2006) describe a call system used by putty-nosed monkeys in which different call elements are strung together. These monkeys only have two elementary (i.e. unitary) signals in their repertoire, labelled ‘pyow’ and ‘hack’. They also have the ability to combine the elementary ‘pyow’ and ‘hack’ signals into longer sequences. This combinatorial power gives ways of expressing more than two meanings. So ‘pyow’ roughly means leopard, ‘hack’ roughly means eagle, and ‘pyow-hack’ seems to mean let’s go, and so on. Note that the meaning let’s go is not a function, in any natural sense, of leopard and eagle. This, then, is a (very small) combinatorial system, but it is not obviously semantically compositional, because in the case of the ‘pyow-hack’ the meaning of the whole is not a function of the meanings of the parts.
Arnold and Zuberbühler write, very carefully, ‘Our ﬁndings indicate that non-human primates can combine calls into higher-order sequences that have a particular meaning’. There are two ways to interpret the data. One interpretation is that the internally represented meaning of ‘pyow-hack’ in the receiving monkey’s mind has nothing to do with eagles or leopards, and that it invokes instead some separate notion of imminent travel. In this case the ‘particular meaning’ that the researchers mention is not a function of the meanings of the basic calls combined, and so the ‘pyow-hack’ call of the putty-nosed monkeys is not semantically compositional. This would be a case of animals overcoming the limits of their repertoire of individual calls by combining them, but not in any way reﬂecting the composition of the meanings expressed.
The other interpretation of the data, perhaps more plausible, is that ‘pyowhack’ conjures up in the receiver’s mind both concepts, eagle and leopard, and the monkey takes appropriate action. In this case, the call is, in the simplest sense, compositional, expressing a conjunction of the meanings of its parts, that is eagle & leopard. In a later paper (Arnold and Zuberbühler 2008),

a n i m a l sy n ta x? language as behaviour

15

somewhat extended data is described, with responses to longer series of pyows and hacks. Series combining pyows and hacks again elicited travel. Here the authors use the title ‘Meaningful call combinations in a non-human primate’. This is again careful: the call combinations are meaningful, but whether they are interpreted compositionally remains an open question.10
A similar point can be made about another case carefully observed, and carefully discussed, by Klaus Zuberbühler (2002). This is more problematic, because part of the story involves the responses of one species, Diana monkeys, to the alarm calls of another species, Campbell’s monkeys. Campbell’s monkeys have speciﬁc alarm calls for leopards and eagles, and Diana monkeys respond to these by giving their own different alarm calls for these predators. There is some interest in ﬁrst discussing the signiﬁcance of the calls to the Campbell’s monkeys alone. Zuberbühler writes ‘In addition to the two alarm calls, male Campbell’s monkeys possess another type of loud call, a brief and low-pitched “boom” vocalization. . . . This call type is given in pairs separated by some seconds of silence and typically precedes an alarm call series by about 25 s. Boom-introduced alarm call series are given to a number of disturbances, such as a falling tree or large breaking branch, the far-away alarm calls of a neighbouring group, or a distant predator. Common to these contexts is the lack of direct threat in each, unlike when callers are surprised by a close predator’ (2002, p. 294). The responses of Campbell’s monkeys to these boomintroduced calls are not described, but if they are like the responses of the Diana monkeys (to the Campbell’s calls), the Campbell’s monkeys show little or no alarm on hearing a pair of booms followed about 25 seconds later by what sounds like a regular alarm call. The booms could be interpreted as in some sense negating, or qualifying, the normal meaning of the alarm call, just as the English expressions maybe or not-to-worry-about might modify a shout of ‘Police coming!’ This is the strongest interpretation one can put on the facts. The 20-second delay between the booms and the alarm call is problematic, as it does not suggest composition of a unitary message. One would expect a unitary communicative utterance consisting of several parts to be produced with little or no delay between the parts (unlike the slow stately progress of whale songs.) The contexts in which the boom-introduced calls occur, as Zuberbühler describes them, can possibly be thought of as semantically composite, for example something like threat + distant, but

10 Another interesting fact is that in these studies female receiving monkeys only responded to the calls of ‘their own’ males, so this is not a case of a group-wide code. Also, Anderson (2008a, p. 800) has an identical take to mine on the ‘pyow-hack’ data.

16

the origins of grammar

the do-nothing responses cannot be seen as any obvious function of the panic reactions induced by the plain alarm calls.11
More recently, a team including Zuberbühler (Ouattara et al. 2009) have found more complex behaviour among wild Campbell’s monkeys. Besides the ‘boom’ (B) call, they distinguished ﬁve different types of ‘hack’, which they labelled ‘krak’ (K), ‘hok’ (H), ‘krak-oo’ (K+), ‘hok-oo’ (H+) and ‘wak-oo’ (W+). Their observations are worth reporting at length as they are the most complex yet seen in wild primates, and have some syntax, though it is not semantically compositional.
The different call sequences were not randomly assembled but ordered in speciﬁc ways, with entire sequences serving as units to build more complicated sequences. As mentioned, pairs of booms alone instigate group movements toward the calling male, while K+ series functioned as general alarm calls. If combined, the resulting sequence carried an entirely different meaning, by referring to falling wood. In all cases, the booms preceded the K+ series. We also found that another sequence, the H+ series, could be added to boom-K+ sequences, something that callers did when detecting a neighboring group. H+ series were never given by themselves. . . .
These call combinations were not random, but the product of a number of principles, which governed how semantic content was obtained. We found ﬁve main principles that governed these relationships. First, callers produced sequences composed of calls that already carried narrow meanings (e.g., K = leopard; H = crowned eagle). In these instances, sequence and call meanings were identical. Second, callers produced meaningful sequences, but used calls with unspeciﬁc meanings (e.g., K+ = predator). Third, callers combined two meaningful sequences into a more complex one with a different meaning (e.g., B + K+ = falling wood). Fourth, callers added meaningless calls to an already meaningful sequence and, in doing so, changed its meaning (e.g., B + K++ H+ = neighbors). Fifth, callers added meaningful calls to an already meaningful sequence and, in doing so, reﬁned its meaning (e.g. K + K+ = leopard; W + K+ = crowned eagle). We also found regularities in terms of call order. Boom calls, indicative of a nonpredation context, always preceded any other call types. H and K calls, indicators of crowned eagles or leopards, were always produced early in the sequence and were relatively more numerous if the level of threat was high.
(Ouattara et al. 2009, p. 22029)
These monkeys do produce systematically formed call-sequences, so, like birds, they have some combinatorial syntax. The sequences are meaningful, apparently referential, but the meanings of the sequences are not functions of the

11 For sure, one can always think of some function getting from one concept to another, but it won’t necessarily be a very natural function. This assumes, of course (what else can we assume?) that what is a ‘natural’ function for a monkey is also at least somewhat natural for us human investigators.

a n i m a l sy n ta x? language as behaviour

17

meanings of the parts, so the syntax is not semantically compositional. What could be happening here is that there is a felt need to express more meanings than can (for some reason) be expressed by an inventory of four one-unit calls ‘boom’, ‘krak’, ‘hok’, and ‘wak’. The monkeys cannot produce any further oneunit distinct calls, so they resort to making new signals by concatenating what they have. The meanings expressed are all of the same level of concreteness— leopard, eagle, neighbours, tree-falling—and not in any hierarchical relation with each other, so a compositional system would not be appropriate. This is pure speculation, and not very convincing, at that, but maybe other similar examples will be found that shed some light on this case. It seems unlikely that Campbell’s monkeys are the only species with such behaviour. We need more empirical ﬁeld research.
Moving on to birds, the dominant consensus in the birdsong literature is that songs are meaningful in the sense that they function to attract mates or defend territory. The great variety in some birdsong repertoires is interpreted as impressive display, or versatile echoing of rival songs. Very few authors claim any compositional semantics for birdsong. Exceptions to this general trend are Hailman et al. (1985), writing about the black-capped chickadee, and Smith (1972), on its close relative, the Carolina chickadee.
These preliminary discoveries of S. T. Smith obviously do not specify referents of notetypes completely, but they do suggest that the locomotory signals have something to do with such acts as take-off, landing, ﬂight, perching, and change of direction.
(Hailman et al. 1985, p. 221)
S. T. Smith (1972) went on to make preliminary identiﬁcation of note-types with speciﬁc ‘messages’ about locomotion, and noted that the combination of these notes in calls encoded a combination of their separate messages. She also pointed out that notetypes are commonly repeated within a call, which suggests that the repetitions encode intensive aspects of the basic message of note-types. (Hailman et al. 1985, p. 191)
Hailman et al. have no hesitation in writing about the ‘referents’ of the various note-types, of which there are just four in the black-capped chickadee. The last quotation above is a clear statement of compositionality, but it has not, to my knowledge, resurfaced in the literature. At most, the kind of compositionality involved expresses a conjunction of the meanings of the basic notes. For example, if note ‘A’ signals something to do with take-off, and ‘B’ signals something to do with change of direction, then the sequence AB might signal something to do with take-off and with change of direction. This is like the wellknown child example ‘Mommy sock’, meaning something to do with Mummy and with a sock. It is the simplest form of compositionality. As Hailman et al. (1985) concede: ‘Unlike written words made recombinantly from their

18

the origins of grammar

component letters, calls strung into bouts have no evident higher level of structure such as the grammar of human sentences’ (p. 221).
In sum, there is no compelling evidence for any semantically compositional learned signalling in wild animals. Even if the problematic cases that have been mentioned are held to be strictly compositional, they are of limited scope, and provide only a slight platform upon which the impressive human capacity for compositionality might have evolved.

1.2 Non-compositional syntax in animals: its possible relevance
Some wild animals do produce syntactically complex behaviour, in semantically uninterpreted ‘songs’. In such songs, although they are structurally somewhat complex, the meaning of a whole signal is not in any way composed as a function of the meanings of its elementary parts. How might signals which don’t express any complex meaning be relevant to the evolution of human language? A number of writers, dating back several centuries, have seen in this behaviour the beginnings of human syntax. For these authors, the link lies in the sheer syntactic complexity of the songs. In this section and the next I survey these animal systems, and extract some general lessons about how to conceive of such pure syntactic abilities within biological organisms.
One evolutionary possibility is that after the chimp/human split the ancestors of humans developed somewhat syntactically complex songs like birds or gibbons, initially with no systematic combining of the meanings of the elements to convey some perhaps complex message (even if the elements had some meanings, which they might not have had). This is in fact a venerable idea. Rousseau and Darwin believed it, and so did Otto Jespersen, a renowned early twentieth-century linguist. These all saw music, in some form, as a pre-existing kind of syntactically complex expressive behaviour from which referentially meaningful language later evolved. The function of such complex songs was purely for display, to attract sex partners, they suggested (Darwin 1871; Jespersen 1922). The idea was of a separate syntactic ability, used for composing seductively complex songs—that is songs which were seductive purely by virtue of their complexity, and not by virtue of any semantic content, because they had none (apart from ‘come mate with me’). For birdsong,
The evidence from the laboratory data is highly consistent and shows that, when females are exposed to large repertoires, they display higher levels of sexual arousal

a n i m a l sy n ta x? language as behaviour

19

than when they hear small repertoires (e.g. Catchpole et al. 1986; Lampe and

Saetre 1995; Searcy and Marler 1981) Field data however are not as straightfor-

ward. . . . [However] in the great reed warbler Acrocephalus arundinaceus. . . cuckolded

males had smaller song repertoires than their cuckolders (Hasselquist, Bensch, and

T. von Schantz, 1996).

(Gil and Slater 2000, p. 319)

The hypothesis of an early-evolved syntactic, speciﬁcally musical, ability, predating the exaptation of syntactically structured songs for propositional semantic purposes by humans, is explicitly argued by Fitch (2005, p. 16). ‘The many similarities between music and language mean that, as an evolutionary intermediate, music really would be halfway to language, and would provide a suitable intermediate scaffold for the evolution of intentionally meaningful speech’.12 Mithen (2005, 2009) has argued for a closely related view, involving coevolution of the human musical and linguistic capacities; see also MolnarSzakacs and Overy (2006) who emphasize a common neural substrate for music and language, and similar hierarchical structure. Fitch points out that the function of such song need not be for sexual attraction, but could also have a role in promoting group cohesion, or could be used by mothers to calm their young. Cross and Woodruff (2009, pp. 77–8) also stress the functions of music in ‘the management of social relationships, particularly in situations of social uncertainty’. For birds with extremely large repertoires, such as the nightingale, it has been pointed out that sexual attraction is an implausible function, as females are unlikely to spend time listening to over a hundred songs, just to be impressed by the male’s versatility. In this case, a territory-marking function may be more likely, but the question remains whether rival males need to be told in so many different ways to get lost.
Music has complex syntax, but the meaning of a whole tune is not put together from the elementary meanings of each note or phrase; and music certainly does not refer to outside objects or events (though it may iconically evoke them). It is possible that some purely syntactic capacity, possibly used for display, or to enhance group cohesion, or to claim territory, evolved in parallel with private, somewhat complex, conceptual mental representations. (Here ‘syntactic’ simply means ‘exploiting combinatorial possibilities, given a set of elementary forms’.) Then, according to this hypothesis, at some later stage the conceptual and syntactic abilities got combined to give complex semantically compositional syntax. The syntax-from-song hypothesis has been seriously argued by serious people, so I will give it a fair hearing in this chapter. I do not think that pre-existing complex song can be the whole story of how

12 See also an essay by Fitch at http://languagelog.ldc.upenn.edu/nll/?p=1136.

20

the origins of grammar

humans got complex syntax. But it may be some part of the story. How large that part is cannot be argued, given present evidence.
Command of a range of different complex songs may have served a mnemonic function when they ﬁnally began to carry some semantic content. Sometimes you have to repeat a sentence to yourself before you really understand what it means. The ability to repeat it before fully understanding it involves some capacity for holding a (somewhat) meaningless, but nevertheless structured, string in your head.
One intriguing similarity between the songs of many bird species and human utterances in conversation is that they are of roughly the same duration, between two and about ten seconds. A bird will sing one song from its repertoire, lasting, say, about ﬁve seconds, and then wait for a similar period, during which a territorial rival may sing its responding song, often identical or similar (Todt and Hultsch 1998, p. 488). Thus a kind of discourse exists with the same temporal dimensions as human casual conversation. (But whalesong is an entirely different matter, with individual songs lasting up to half an hour; this conceivably is connected to the greater distances over which songs transmitted through water can carry.)
A striking difference between bird repertoires and human languages illustrates the unproductivity of bird syntax: ‘The composition of vocal repertoires reveals a basic principle in most songbirds: The sizes of element-type repertoires are larger than the sizes of their song-type repertoires’ (Hultsch et al. 1999, p. 91). This assertion is surprising to a linguist if one equates elementtypes with words and song-types with sentences. This fact is also stated by Todt (2004, p. 202) and Bhattacharya et al. (2007, p. 2), and is borne out by the examples I will discuss here. Podos et al. (1992) devised a method to put identiﬁcation of song-types, and hence song repertoire sizes, on a ﬁrmer objective footing. They introduced a concept of ‘minimal unit of production’, MUP for short. An MUP is typically an individual note, but can be a sequence of notes if these notes always occur together in the same order. Then one can count the MUP repertoire size and the song repertoire size of any bird. Using this method, Peters et al. (2000) quantiﬁed the MUP repertoire and song repertoire sizes of ﬁve geographically separate groups of song sparrows. In all cases the MUP repertoire sizes were greater than the song repertoire sizes, by factors of about six or seven.
Much depends, of course, on how you count song-types. Hailman et al. (1985) studied chickadee (Parus atricapillus) ‘calls’ (most of which more recent researchers would classify as ‘songs’). They counted 362 different ‘call-types’ composed from a basic vocabulary of four notes. This includes one-note, that is non-combinatorial, calls, and calls with different numbers of repetitions of

a n i m a l sy n ta x? language as behaviour

21

the component notes, which other researchers would classify as belonging to the same song-type. Counting only songs in which notes are combined and counting repetitions of the same note as one, the number of distinct songs comprising over 99 per cent of the repertoire comes, by my reckoning, to just four, the same as the basic vocabulary. A spectacular example of a bird’s failure to exploit syntactic combinatorial possibilities is provided by the brown thrasher (Toxostoma rufum). This bird is reported as being at the extreme of vocal virtuosity, having ‘a repertoire of several thousand different types of song’ (Brenowitz and Kroodsma 1996, p. 287). The original students of this bird’s repertoire (Kroodsma and Parker 1977) report that each distinct song type is in fact a repetition of a distinct syllable type. There is not, apparently, any combination of one syllable type with another in the same song. So this bird has an estimated vocabulary in the thousands, and its song repertoire is in fact no larger than its vocabulary. This extreme example illustrates a general point that whatever syntax can be found in bird repertoires, they do not take advantage of its combinatorial possibilities. An analogy from English orthography would be a repertoire of, say, ﬁve words which happen to use all 26 letters of the alphabet. Given so many letters, and some possibility of combining them, why restrict the combinations to less than the number of letters? Why not make up and use more words? In human languages, the inventory of phonemes is always orders of magnitude smaller than the vocabulary size; and the vocabulary size is always orders of magnitude smaller than the number of possible sentences. Birdsong is thus strikingly different in this respect.
Conceivably, an ability for complex song provided an evolutionary basis for human phonological syntax, but no basis, or only a slight basis, for the semantically interpreted syntax of whole sentences. ‘[P]honology (sound structure), the rules for ordering sounds, and perhaps the prosody (in the sense that it involves control of frequency, timing, and amplitude) are the levels at which birdsong can be most usefully compared with language’ (Doupe and Kuhl 1999, p. 573). MacNeilage (2008, pp. 303–8) also ﬁnds suggestive parallels between the serial organization of birdsong and human phonological syntax. A complementary part of the story, and perhaps the whole story, of how we got complex semantically compositional syntax is that it evolved on a platform of complex conceptual representations, plus some natural principles of the communication of information. These last complementary ideas are not for now but for later chapters.13

13 The evolutionary contribution of pre-existing song-like syntax to modern semantically interpreted syntax is bound up with a debate between advocates of two different possible routes to modern syntax, an ‘analytic’ route and a ‘synthetic’ route. This debate will be the topic of a later chapter.

22

the origins of grammar

Pure uninterpreted syntax is not found in communication systems in the recent human lineage. The closest species to us displaying such asemantic song are gibbons. The absence of any complex songlike behaviour in great apes is not necessarily a problem. Complex song occurs many times in nature, in subsets of classes and families. Many, but not all, bird species have complex song. Among oscine birds, chafﬁnches have complex songs, but crows do not. Some hummingbirds have complex song (Ficken et al. 2000), while others don’t. Among whales and dolphins, humpback whales have the most complex song. Among primates, only gibbons have complex songs. Complex song, it appears has evolved separately several times. So it could have evolved separately in humans after the chimp/human split.
Learned vocal behaviour also cross-cuts phylogenetic classiﬁcations, and so has probably also evolved independently several times. There is a close correlation between complexity of song and the degree to which the song is learned. If we can class human speech with song, humans have ‘songs’ that are both complex and learned. Despite the great genetic distance between songbirds and humans, and despite the large differences in their brain structure (e.g. birds do not have a many-layered cortex like mammals), there are signiﬁcant similarities in the neural circuitry used for the production and learning of vocalizations. Jarvis (2004a, 2004b, 2007) argues for a hypothesis that ‘vocal learning birds—songbirds, parrots, and hummingbirds—and humans have comparable specialized forebrain regions that are not found in their close vocal non-learning relatives’ Jarvis (2007, p. 35). To argue this, Jarvis has to depend on a number of hypothesized functional equivalences of parts among the anatomically different brains involved (of parrots, songbirds, hummingbirds, and humans). He gives a long list of evidence that lesions in equivalent places in these brains produce functionally similar deﬁcits in the respective species (2007). In similar vein, Doupe and Kuhl (1999, p. 567) summarize a broad survey thus: ‘Although some features of birdsong and speech are clearly not analogous, such as the capacity of language for meaning, abstraction, and ﬂexible associations, there are striking similarities in how sensory experience is internalized and used to shape vocal outputs, and how learning is enhanced during a critical period of development. Similar neural mechanisms may therefore be involved’. They also cite lesion and stimulation studies which bring out the similarities among learners, and their differences from non-learners. The relevant areas are areas of higher control:

Both songbirds and humans have high-level forebrain areas that control the preexisting hierarchical pathways for vocal motor control . . . , whereas nonlearners do not. There are no neocortical sites in monkeys from which vocalization can be elicited

a n i m a l sy n ta x? language as behaviour

23

by stimulation nor whose ablation affects calls (Ploog 1981). In striking contrast, in humans the entire perisylvian cortical area as well as posterior parieto-temporal cortex is critical for speech production, as shown by both stimulation and lesion studies.
(Doupe and Kuhl 1999, p. 599)

This again suggests convergent evolution by somewhat different kinds of brain onto a common working solution to the problem of vocal learning.
Complex signals of wild animals are only partly learned, or not at all; in all species, there is a very hefty innate component. Without assigning percentages to innate and learned components, it is clear that the parallel between human language and animal songs is not strong on this point. Commitment to a nativist and syntactocentric view of language can lead to an emphasis on parallels between birdsong and language:

Certainly, little or no overlap occurs in the details of the development of speech in

children and of song in birds. Equally obvious, however, is the remarkable similarity

of these two processes at only a modest level of abstraction. . . . We should have little

hesitation in seeing both processes as essentially similar, as the working out of a species’

developmental program in biologically guided maturation. In other words, nestlings

and babies both grow up in a speciﬁc way, determined in its essence by the fact that

they are birds and humans, respectively.

(Anderson 2004, p. 165)

What this view underemphasizes is the massive functional (semantic) difference between birdsong and language, accompanied by an equally great difference in structural complexity, differences that Anderson elsewhere acknowledges. Additionally, a remarkable difference between nestlings and babies growing up and learning their language is the fact that birds do not learn their song incrementally through a process of discourse with their parents (or other group members). Birds store the patterns they hear as nestlings, and then only later, sometimes as much as eight months later, start to produce their own songs.14
In birdsong, there is also some evidence of voluntary control. ‘We found that chafﬁnches (Fringilla coelebs) in noisier areas (i.e., close to waterfalls and torrents) sang longer bouts of the same song type before switching to a new type, suggesting that they use increased serial redundancy to get the message across in noisy conditions’ (Brumm and Slater 2006a, p. 475). In another study, Brumm and Slater (2006b) found that zebra ﬁnches sang louder when the receiving female was further away, and draw a superﬁcial parallel with humans raising their voices. However, they suggest that ‘this behaviour can be

14 See Fehér et al. (2009) for an interesting recent study in which zebra ﬁnches developed a wild song type, over three or four generations, by iterated learning starting from birds who had had no model to imitate.

24

the origins of grammar

accounted for by simple proximate mechanisms rather than by the cognitive abilities that have been thought necessary in humans’ (p. 699).
To recap, it is worth looking at complex song in species not closely related to humans because of the possibility of a parallel independent evolution adapting to similar functions, and involving similar brain mechanisms. If this happened, then some time after the human/chimp split, our ancestors developed a capacity for complex musical or song-like behaviour that was later recruited for the expression of complex meanings. Perhaps it did happen. Some empirical light could be shed on this question by testing the susceptibility of apes and monkeys to various sequences with music-like structure.

1.3 Formal Language Theory for the birds, and matters arising
So far, I have only mentioned that birdsong and whalesong can be syntactically ‘complex’. But how complex is ‘complex’? In the rest of this chapter, we get to grips with a way of comparing meaningless syntax across species. It will emerge that despite big quantitative differences between animal song and human language, the more complex animal songs do have some similarities with language. Apart from the obvious lack of compositional, and referential, semantics, these songs are not qualitatively, but only quantitatively, different in their basic combinatorial structure.15
If we are seriously to compare human syntax and the complex songs of animals, we need some common scale by which to measure each of them. Formal Language Theory provides a scale which is in some ways suitable. The crossdisciplinary exercise of applying this body of theory to animal songs will reveal some of the serious theoretical issues that arise when applying the tools of one trade to data from another. One conclusion will be that applying this scale shows that human languages are not just head and shoulders above animal songs in syntactic complexity, but (to continue the metaphor) head, shoulders, trunk, and legs above them. The familiar assertion of a huge gap between humans and non-humans is thus reinforced. But it is good to have a nonimpressionistic way of justifying this common assertion, and Formal Language Theory provides a tool for this. The other main conclusion to arise from this exercise is that certain issues which have been contentious in theorizing about
15 This is not to deny that certain semantico-syntactic, or pragmatico-syntactic features of human language are absent from animal song (see Chapters 3 and 4). I assume that these features were superimposed on any basic syntactic structure if and when it was recruited for expressing complex meanings.

a n i m a l sy n ta x? language as behaviour

25

human language start to arise even when considering much simpler systems, leading me to suggest some modiﬁcations of common theoretical distinctions. In this way, many of the concepts introduced here will also be useful in later chapters of the book. So bear with me in this section while I give you an introduction to Formal Language Theory.
In the 1950s and early 1960s, Chomsky, in a number of highly ingenious and original technical papers,16 set out the skeleton of a subject that became known as ‘Formal Language Theory’. At the heart of this theory is a hierarchy of possible language types, now known, especially among linguists, as the ‘Chomsky Hierarchy’. Although Chomsky’s early work, such as Syntactic Structures (1957), argued largely from premisses established within the framework of this theory, his later work moved away from it, reﬂecting a growing recognition of its irrelevance to a theory of human languages. It is said that Chomsky never personally approved of the label ‘Chomsky Hierarchy’, and out of respect for this, and to emphasize its content rather than its personal associations, I will refer to it as the ‘Formal Language Hierarchy’.
In computer science, as opposed to linguistics, the Formal Language Hierarchy became very important as a way of classifying computer languages. The hierarchy deﬁnes a ranking of classes of languages paired with the kinds of machine that could automatically process the languages of each class, given a relevant program, or ‘grammar’. Outside computer science, the only area of theoretical linguistics that has maintained any common reference to this hierarchy is learnability theory, which is also a highly formal, highly idealized and technical branch of linguistics, dealing in theorems and proofs. Mainstream syntactic theory is not completely devoid of theorems and proofs; the Formal Language Hierarchy remains part of a syntactician’s basic training, but it does not typically ﬁgure in the focus of theoretical attention for working syntacticians.
For those interested in the evolution of language, the Formal Language Hierarchy holds out the promise of a kind of easily deﬁnable scala naturae in terms of which it might be possible to classify the communication systems of various animals. The motivating idea is that human language makes computational demands on the mind of a qualitative type unattainable by other creatures. And it might be possible to peg the communication systems of other species

16 See Chomsky (1956a, 1956b, 1956c, 1958, 1959a, 1959b, 1962a, 1963); Chomsky and Miller (1958); Chomsky and Schutzenberger (1963). Chomsky’s formulations did not, of course, spring from nowhere. As noted by Scholz and Pullum (2007, p. 718), it was Emil Post (1943) who invented rewriting systems of the kind assumed in Formal Language Theory, and also did the ﬁrst work on the generative power of such systems.

26

the origins of grammar

at various lower levels on the hierarchy. Then the evolutionary story would be of an ascent up the Formal Language Hierarchy from the syntactic abilities of various non-humans to the present impressive syntactic abilities of humans. Some recent experiments with tamarin monkeys and starlings have appeared to take this idea seriously, in that they have expressed their conclusions literally in terms of the Formal Language Hierarchy. We will come to those studies later. The scala naturae analogy is not totally crazy, although, as we will see, many serious reservations must be expressed about it. Even the most complex of animal songs seem to occupy lower places on the Formal Language Hierarchy than human languages. Something about the hierarchy expresses some truth about animal songs, but it is too idealized in its conception to tell the whole story about the factors affecting real biological systems. In this section I will explain the central ideas of the Formal Language Hierarchy, especially its lower end. In the ensuing subsections I will consider its application to animal songs, and discuss those animal studies which have used the hierarchy as a frame of reference.
Two theoretical distinctions made by linguists are crucial to thinking about human languages in terms of the Formal Language Hierarchy. These are the distinctions (1) between the weak and strong generative capacity of descriptions (or grammars), and (2) between competence and performance. I will explain these concepts, but ﬁrst here is why they are relevant. Linguists have tended to think of animal songs only in terms of weak generative capacity and performance. I will argue that animal songs, like human languages, are sensibly considered in terms of strong generative capacity and competence. Thus animal song and human language, despite huge differences between them, can be thought of using the same conceptual tools. In its original basic conception, the Formal Language Hierarchy is also resolutely non-numerical. I will also argue for numerical augmentation of the animal song grammars. These arguments will be woven into a basic exposition of what the Formal Language Hierarchy is.
Within the theory of the Formal Language Hierarchy, a ‘language’ is taken to be nothing more than a set of strings of elements, a ‘stringset’. (We will later have reason to move on from this idealized view of what a language is, but it will be helpful to stay with the simple stringset idea for the moment.) Applied to a human language, think of a language as a set of sentences, say the set of well-formed sentences in French. In relation to Formal Language Theory, it is assumed that this is an inﬁnite set. Sets can be inﬁnite, like the set of natural numbers. Postulating inﬁnite languages conveniently eliminates any awkward question of constraints on the length of sentences and on the memory mechanisms involved in processing them. Also, the theory makes

a n i m a l sy n ta x? language as behaviour

27

the idealization that there is a clear-cut distinction between the grammatical expressions in a language and strings of elements which are not grammatical. That is, the assumption is that there are no unclear or borderline cases. Let that pass for now. A formal grammar is a set of precise statements (usually called ‘rules’) which speciﬁes the whole set of grammatical sentences in a language, and nothing but those sentences. The usual formulation is that a grammar ‘generates’ all and only the well-formed expressions in the language. The elements constitute the (‘terminal’) vocabulary of the language, and the grammar deﬁnes, or generates, all and only the well-formed strings of these elements. The elements are the smallest observed parts of the signals. I don’t call the elements ‘symbols’ because that could carry the implication that the elements in the vocabulary are treated as if they mean something. Formal Language Theory doesn’t deal with the meanings of the vocabulary elements in languages, nor with the meanings of the strings of these elements which belong in the language.
The avoidance of any issue of meaning is actually an advantage when dealing with animal communication systems such as birdsong or whale or gibbon songs, because the elements of these songs are not put together by the animals in such a way that the whole song conveys some complex message assembled from the meanings of the parts. Animal songs have no semantically compositional syntax. For human languages, however, treating them as merely sets of uninterpreted strings of uninterpreted elements is clearly wrong. Human languages are not merely sets of sentences. But it does not follow that Formal Language Theory has nothing to contribute about the ways in which human languages syntactically construct their sentences. That is, knowing that human sentences convey meaning is not enough in itself to tell us how the grammar of a language will construct its meaningful sentences. Just to say ‘put the meaningful elements together in any way that makes the whole string express a complex meaning’ only describes a recipe for ‘semantic soup’, as Anderson (2004) calls it. It is not an adequate description of any language,17 except a pidgin ‘language’. Pidgin languages are not fully-ﬂedged human languages. Pidgins are arguably semantically compositional, in the simplest possible way, but have no syntactic organization. The songs of birds, whales, and gibbons, by complete contrast, have somewhat complex syntax, but no hint of semantic compositionality linked to this syntactic organization.

17 This statement is true, but we will see in Chapter 5 that some languages get nearer than others to a semantic soup state. See also discussion of protolanguage in Chapter 6.

28

the origins of grammar

The weak generative capacity of a grammar is its capacity to generate a set of strings of elements, no matter whether this seems naturally to capture the way in which we as insightful humans intuitively feel the system works. Imagine a simple system with a vocabulary of a thousand nouns and a thousand verbs, and a single grammatical rule forming two-element sentences by putting any noun ﬁrst and any verb second; only two-element strings exist in this language. This ‘language’, then, has just a million sentences, and hence is ﬁnite. So the language could be speciﬁed with a long list. But this would obviously be to miss something about the organization of the language. As far as weak generative capacity is concerned, a million-long list is as good as the more elegant and sensible description in terms of a combinatory rule which I just used to describe the language. The strict Formal Language Hierarchy is based on considerations of weak generative capacity. If a language is technically ﬁnite, it belongs at the bottom of the hierarchy. So, in terms of weak generative capacity, our hypothetical language, with a systematic way of combining its thousand nouns and its thousand verbs, and, crucially, a two-element limit to sentence length, sits in the same broad rank in this hierarchy as the call of the cuckoo and the hiss and rattle of the rattlesnake.
A ﬁnite language can be described as a ﬁnite list of all the possible expressions in it. Mere lists are boring, of little theoretical interest. A ﬁnite language can be learned by rote by anybody with enough memory; the whole language can literally be memorized. Where learning is not involved, a short ﬁnite list of communicative signals can be coded into the genes. The ﬁnite repertoires of non-combinatorial calls of many animals, such as the various coos and warbles of ravens, the alarm calls and social grunts of vervet monkeys and all the calls of wild chimpanzees are presumably at this level. These systems have no apparent syntax. Only syntax raises a system from ﬁniteness to a potentially inﬁnite number of signals. Human languages are, at least potentially, inﬁnite;18 one cannot put a principled limit on the length of a sentence, because one can always in principle extend any sentence by conjoining some further clause. For example, in English one can always in principle lengthen any sentence by

18 Pullum and Scholz (2010b) point out that it is not an empirical fact about languages that they are inﬁnite. How could it be? One cannot observe an inﬁnite number of sentences. Rather, the ‘inﬁnitude’ claim about human languages is a consequence of one’s basic theoretical assumptions. It will do no harm here to stick with assumptions that entail inﬁnitude for languages, where appropriate. Much of the pure mathematical fascination of Formal Language Theory lies with proofs that there are different classes of inﬁnite languages, each successive class containing the class below it, and each class making successively stronger demands on the computing machinery that is needed to process its sentences.

a n i m a l sy n ta x? language as behaviour

29

adding I said that . . . to the front of it. A sentence with a number of I said thats at the beginning may be tediously redundant and stylistically awful, but it is still a sentence of English. You can in principle go on adding I said thats as long as you like. This kind of example is what gives rise to the claim that the sentences of a language, such as English, are inﬁnite in number. Just as there is no highest number (every number has a successor), there is no longest sentence, the way sentences are conceived within the Formal Language Theory approach to natural languages.
In computer science also, ﬁnite languages are of little interest, as any useful computer language should not stipulate an artiﬁcial upper bound on the length of well-formed expressions in it. Some computer programmers like to write extremely intricate ‘hairy’ code, with many embedded and conjoined conditions. Designers of computer languages and the compiling algorithms that translate them into nuts-and-bolts machine code are constrained by ﬁniteness, but they always allow for more memory than any competent programmer is likely to need. When a computer actually runs out of memory, this is usually a result of bad programming or error. Different computers have different memory limits, but the same programming language will run on them.
When we come to ask whether bird- and whalesong repertoires can be regarded as inﬁnite in the same way as human languages, we are on stickier ground, because we have no privileged insight into these systems. We can only observe ﬁnite sets of data, but it might strike us that something about the way a system works seems to project an inﬁnite number of examples similar to those we have observed. As speakers of English we know that we can always add a clause onto the end of any sentence, but there is also a practical limit to the length of sentences. Could we say the same of a bird’s, or a whale’s, repertoire if it contains many instances of repetition of some unit? As all animals are subject to constraints of the ﬂesh, it can seem reasonable to distinguish between the idealized system that guides an animal’s behaviour, and the limits on actual products of this system. Although native speakers of human languages may be credited with a tacit ‘knowledge’ of what the well-formed sentences of their language are, they obviously sometimes make errors in speaking, because of tiredness, running out of memory, being interrupted, and so on. You might observe an English speaker literally say ‘of of the of’ in the middle of an utterance, but would put this down to hesitancy or distraction, rather than admitting that of of the of can be part of a well-formed English sentence. Many linguists (and I am one of them) ﬁnd it sensible to distinguish between two factors affecting what we actually say when we speak: (1) a set of canonical

30

the origins of grammar

target expressions, or knowledge of the ‘right’ way to say something,19 and (2) factors of a different type, which affect not only speech but other kinds of activity as well, such as getting dressed, cooking, and driving. These factors are competence and performance, respectively.
There is evidence that adult birds have tacit target canonical songs, built more or less closely, depending on the species, upon innate templates. For various reasons, the birds sometimes produce these canonical songs imperfectly, or with some unprogrammed variation. MacNeilage (2008, p. 305) mentions work of Thorpe and Hall-Craggs (1976) on birdsong errors: in their research notes, they used such phrases as ‘Bird getting in a muddle’. Mooney (2004, p. 476) refers to birds ‘using auditory feedback to match their own song to a memorized tutor model’. On a long ontogenetic timescale, ‘Male swamp sparrows reared in the laboratory and exposed to taped songs during infancy produce accurate imitations of the material following an 8-month interval with no rehearsal’ (Marler and Peters 1981, p. 780). When they do start to sing, and before they eventually home in on the adult song, these sparrows produce a range of relatively imperfect ‘subsong’ and ‘subplastic’ song. This indicates storage of a canonical target song as an auditory template guiding the gradual perfection of performance. Stored learned templates can be maintained intact without feedback for impressively long periods, sometimes over a year (Konishi 1965), but tend to deteriorate if they are not refreshed by feedback from the bird’s own singing (Nordeen and Nordeen 1992). Todt and Hultsch (1998) describe training nightingales on artiﬁcially modiﬁed variants of typical nightingale songs, and report a kind of gravitation by the learning birds back to song types more typical of the species. They conclude ‘Taken together, these ﬁndings suggest that the birds have access to a “concept” of a species-typical song’ (p. 492). Adret (2004, p. 321) warns that ‘templates (innate or acquired) represent [researchers’] constructs, rather than [actual neural] mechanisms. . . . Despite the many issues outstanding, the template concept will continue to be a heuristically useful model of the song-learning process’. These wise words apply equally well to the linguist’s quest for the mechanisms underlying human language. In the brain, of course, there are no symbolic templates or descriptions, only activation potentials and synaptic plasticity. But in the absence of detailed results on the neural mechanisms of language, the concept of a speaker’s competence, her tacit knowledge of her language, which we

19 I am not referring here to schoolbook prescriptions, or conventions of politeness or etiquette, but to whatever it is in speakers’ heads that causes them to conform, quite unconsciously, to complex regularities when they speak.

a n i m a l sy n ta x? language as behaviour

31

researchers describe symbolically, will continue to be a heuristically useful model.
It is possible that the bird’s representation of the canonical form projects an inﬁnite set of possible songs in its repertoire. If this were the case, the inﬁnite set would reﬂect the bird’s competence, and the actual observed ﬁnite subset of this, inﬂuenced by other factors, such as tiredness and distraction, would reﬂect its performance. Such a view would attribute to the bird something like a characteristically human kind of declarative knowledge about its potential behaviours. Competence is often deﬁned as a speaker’s tacit knowledge of her language. Linguists tap this knowledge by asking whether presented examples are intuited by native speakers to be grammatical. You can’t ask birds questions like that. All you can do is watch their behaviour. But arguably the behaviour of birds that learn their songs involves what can be called declarative knowledge (‘knowing that’, rather than just procedural knowledge ‘knowing how’). This is because their performance during song acquisition slowly approximates, through stages of subsong (like human infant babbling), to a target characteristic adult form that was laid down in their brain many months earlier, and not subsequently reinforced by external models.
Undoubtedly humans are often reﬂective about their language, and in the case of normative prescriptive rules, they will tailor their behaviour to the rules. It is indeed this kind of reﬂection that leads to acquiescence in the proposition that languages are inﬁnite sets, because, on reﬂection, a human cannot identify the longest sentence in a language. It is clearly impossible to put a precise number on it. There is a marked distaste in formal linguistics for describing competence in terms of numbers. For understandable reasons, one rarely, if at all, ﬁnds statements like ‘language X allows sentences of up to about 50 words in length, but no more’, or ‘language Y allows a maximum of three adjectives modifying a noun’, or ‘language Z only permits centre-embedding of clauses within clauses to a depth of two’.20 In the next subsections I will revisit these issues in the light of speciﬁc examples of animal song. I will maintain the usefulness of a distinction between competence and performance, but will suggest a renegotiation of the division of labour between them, and a rethinking of the relationship between them in the light of behaviour in biological organisms generally.
Competence resides in individuals, and only indirectly in the social group, as a result of all members sharing (roughly) the same individual competences.

20 See Chapter 3, section 7 for discussion of such points.

32

the origins of grammar

Competence is not essentially social, even though some of it may be acquired through social processes, by learning. For this reason, the descriptions I will consider will only be of the repertoires of individual animals, rather than trying to make generalizations over the varied ‘dialects’ of social groups. This is not to deny the relevance of group dynamics in the historically evolving patterns of animal songs and human languages. But the focus of this chapter is on the extent to which any individual non-human exhibits human-like syntactic behaviour.
The strong generative capacity of a system of rules (or equivalent diagrams) is a more intuitive notion than weak generative capacity. It appeals to the naturalness with which a system can be described. When dealing with human languages, such considerations of naturalness can involve semantics as well. A natural description provides an efﬁcient way of carving up a string so that the parts are meaningful substrings which are re-used with the same meaning in other examples.21 For instance, a purely sequential description of The cat sat on the mat—ﬁrst say ‘the’, then say ‘cat’, then say ‘sat’, and so on, in a purely beginning-to-end way—misses the fact that the cat and the mat carry meaning in similar ways, referring to speciﬁc objects, and show up as meaningful chunks in other sentences. With animal songs, semantics is not relevant, but there could be non-semantic aspects of naturalness, to do with the economy or simplicity of a description, and with re-use of the same substrings in different examples. To return to the case of the thousand nouns combining in two-word sentences with a thousand verbs, a description with two thousand-long lists and a simple combinatory rule is more economical than one million-long list with no rule. Put crudely, it takes ﬁve hundred times more paper to write out the million-long list of examples than to write out the alternative. There are no perfect objective numerical criteria capturing such intuitions of naturalness.22 Nevertheless, there is much agreement among linguists about a core body of examples.

21 In computational linguistics, especially in parsing theory, the goal of strong generative capacity is often associated with assigning the correct tree structures to parsed strings, rather than just judging them as well-formed or not. Syntactic tree structures for sentences are largely semantically motivated, and serve as a convenient proxy for real semantic representations in computational linguistics.
22 But numerical techniques do exist for measuring the data-compression that grammars achieve, roughly capturing the degree to which a language is susceptible to description by generalizing statements. See the end of Chapter 5, section 3 for some discussion of these ‘bit-counting’ methods, including Kolmogorov complexity and Minimal Description Length (MDL) (Rissanen 1978, 1989).

a n i m a l sy n ta x? language as behaviour

33

In Chomsky’s seminal Syntactic Structures (Chomsky 1957), he set out three successively more powerful23 ways of describing languages: in my terms State Chain descriptions, Phrase Structure descriptions, and a ‘Transformational’ descriptive method that is even more powerful than Phrase Structure descriptions. (I will deﬁne the ﬁrst two of these very soon.) Appealing just to weak generative capacity, he showed that English and other languages simply cannot be described by State Chain descriptions. There are sets of strings in human languages which cannot be generated by State Chain descriptions. We shall see that no such sets of strings are to be found in animal song repertoires, and so State Chain descriptions are all that we need, at the very most, to describe, in terms of weak generative capacity, what these animals do. Chomsky’s next step was to argue that Phrase Structure grammars are themselves unsatisfactory as descriptions of human languages, but here he could not argue from the more objective basis of weak generative capacity. At that time, no parts of any language had been found whose strings could strictly not be generated by Phrase Structure rules, though these might be ungainly and repetitive.24 Chomsky’s argument for the inadequacy of Phrase Structure grammars for human languages was based on strong generative capacity, that is the capacity of grammars to provide intuitively natural descriptions. The intuitions of naturalness involve both meaning (semantics) and economy or simplicity of the overall description.
In approaching animal song we face several decisions about our goals. One decision to be made is whether to be concerned with weak or strong generative capacity. Should we always prefer the weakest form of grammar that permits a description of a repertoire—that is, be concerned only with weak generative capacity? Or should we try to decribe the repertoires in terms that reﬂect intuitions about their structure—that is be concerned with strong generative capacity? If possible, this latter approach should be backed up by evidence from outside the bare facts of the repertoire, for example from neuroscience and from observations of the animals’ learning processes. On a weak capacity approach, we will see below that from this perspective, almost all birdsong repertoires can be captured by the least powerful type of description. But we will also see that classifying repertoires at the lowest possible level of the Formal Language Hierarchy hides facts about the underlying mechanisms,

23 Remember that to adopt a more powerful way of describing some domain is in fact to make a weaker claim about it. Power should be used sparingly.
24 Later on, linguists discovered a few languages which had ‘cross-serial dependencies’, giving a more objective way to demonstrate the inadequacy of Phrase Structure grammars, but here semantic relations also play a role in the argument.

34

the origins of grammar

leading me to prefer the approach in terms of strong generative capacity. This is consistent with standard generative theorizing: ‘The study of weak generative capacity is of rather marginal linguistic interest’ (Chomsky 1965, p. 60). This applies no less to animal songs.
Another decision regards what to do about numerical constraints on repertoires. The numerical information relevant to animal songs mostly involves how many times an element or phrase is likely to be repeated. To avoid giving numerical information, one can simply postulate that any number of repetitions is possible, idealizing the object of description to an inﬁnite set. This decision, though simplifying, is not objective. An alternative approach is to augment a description of the animal’s competence with numerical information about the typical limits of the songs. Where possible, I will add this numerical information. An idealized form of competence can still be regarded as non-numerical. But I will be concerned with what I will call competence-plus (where using this neologism is not too tedious). Competence-plus has two kinds of component, ‘algebraic’ rules for generating song repertoires, and numerical statements of the typical lengths of parts of a song or whole songs. In later chapters, when we come to human language, such numerical constraints will be applied also to the depth of embedding of phrases and clauses within each other.

1.3.1 Simplest syntax: birdsong examples

Based on considerations of weak generative capacity, it is often envisaged that complex animal songs belong at the bottom end of the Formal Language Hierarchy, while human languages belong in the higher ranks. The bottom end of the Formal Language Hierarchy, in slightly more detail than linguists usually consider, looks like this.25

Linear

Finite State

Context Free

(Finite) ⊂ Strictly 2-Local

⊂ Regular

⊂ Phrase Structure

First-order Markov

State Chain

Here, after every term, read ‘languages’, for example ‘State Chain languages’ or ‘Phrase Structure languages’. There is some variation in terminology. The terms in each column here are equivalent to each other. The boldfaced terms are my own preferences for three of the classes of languages. My preferred terms are more transparent to an interdisciplinary audience. With some terminological variability, both Strictly 2-Local and State Chain languages are

25 ‘⊂’ means ‘is a subset of’.

a n i m a l sy n ta x? language as behaviour

35

associated with ‘Markov processes’ or ‘Markov models’, named after the Russian mathematician Andrei Markov (1856–1922). I use the term ‘State Chain’, rather than the more normal ‘Finite State’, in order to avoid any possibility of confusion between these languages and merely ﬁnite languages. A State Chain language (usually called a Finite State language) is not necessarily ﬁnite, because of the possibility of indeﬁnite iterative looping behaviour, to be illustrated shortly. In this discussion, where a ﬁnite song repertoire clearly involves putting things together (i.e. some syntax), I will not locate it at the very bottom ‘Finite’ end of the Formal Language Hierarchy. For reasons of strong generative capacity, it is desirable to represent how songs are put together, even if there are only a ﬁnite number of them.
The successive classes of languages are each more inclusive of the classes lower in the hierarchy.26 Thus all First-order Markov (or Strictly 2-Local) languages can also be described, if one wishes, as State Chain languages or as Phrase Structure languages; and all State Chain languages can also be described, according to one’s theoretical motivation, as Phrase Structure languages. But the converses do not necessarily hold. A Phrase Structure language might be too complex, in a well-deﬁned sense, to be describable at all as a State Chain language. So there exist Phrase Structure languages which are not State Chain languages. Similarly, not all State Chain languages are First-order Markov languages. (In fact we will see later that the Bengalese ﬁnch repertoire is a State Chain language, but not a First-order Markov language.) So the classes of languages higher up the hierarchy are successively less restrictive. The set of First-order Markov languages is a proper subset of the set of State Chain languages, which in turn is a proper subset of the set of Phrase Structure languages, even though each class of languages contains inﬁnitely many languages. The following analogy might be helpful.

(All prime numbers

all prime

all odd numbers

all natural

below 1000)

⊂ numbers ⊂ and 2

⊂ numbers

I will start to illustrate the formal devices used to describe particular languages, or song repertoires, by considering the call of a particular bird, the blue-black grassquit, a native of South and Central America. On the face of things, this bird has a simple and boring repertoire, a single note without pauses,27 each

26 To understand this paragraph, it is essential to remember the deﬁnition of a ‘language’ as a set (possibly inﬁnite) of sentences, where a sentence is a ﬁnite string of elements.
27 There is some variability in the birdsong literature in the use of the term ‘note’. For some (e.g. Fandiño-Mariño and Vielliard 2004; Williams 2004) a note is any sequence of sound uninterrupted by silence; inside a note, there may be ‘elements’ delineated by

36

the origins of grammar

Vib1 Mod2

Vib3

KHz Mod1 Vib2

Ara1

12

Ara2

8

4

0.25

0.5

Fig. 1.1 Basic song structure of the blue-black grassquit Volatinia jacarina showing its single note compacted into a ‘window’ between 2 and 13 kHz and rarely occupying more than half a second.

Note: The labels above the spectrogram are my abbreviations for the seven different identiﬁable parts of the song.
Source: From Fandiño-Mariño and Vielliard (2004).

call ‘rarely occupying more than half a second’ (Fandiño-Mariño and Vielliard 2004, p. 327). To a human ear, such a short call sounds like nothing more than a simple chirp or squeak. And this bird’s repertoire is deﬁnitely ﬁnite. In fact it could be described by a simple list with one member, give or take some aberrations. But a case can be made that even this simple call has some clear syntactic organization, in the basic sense where syntax is ‘putting things together’. Have a look at Figure 1.1, a spectrogram of a call lasting no more than four-tenths of a second. All the bird’s chirps are like this. Fandiño-Mariño and Vielliard (2004) analyse the call as a sequence of seven ‘blocks’ of three different types which they classify as ‘Isolated modulations’, ‘Vibrations’ and ‘Arabesques’. Clearly the bird has a program deﬁning the sequence of parts in its chirp. Even though the sequence is always the same, any description of the call needs to reﬂect the nature of this motor program.
The song of the blue-black grassquit can be adequately described by a Firstorder Markov model, or Strictly 2-Local stringset description, without any mention of internal states of the organism, as below: the list below speciﬁes all the possible transitions in the bird’s repertoire, which happens in this case to be a single call.

abrupt transitions to spectrally different sound structures. For others (e.g. Leonardo 2002), these are the deﬁnitions assumed for a ‘syllable’ and a ‘note’ respectively; in this case a syllable may consist of several notes.

a n i m a l sy n ta x? language as behaviour

37

START Mod1 Mod1 Vib1 Vib1 Vib2 Vib2 Mod2 Mod2 Ara1 Ara1 Vib3 Vib3 Ara2 Ara2 END
The symbol means ‘may be followed by’. This First-order Markov, or Strictly 2-Local, description captures the bird’s repertoire adequately.
Deﬁnition of First-order Markov languages: A First-order Markov language is one that can be completely described by a list of pair-wise transitions between elements of the language (e.g. notes of a bird’s song or words in a human language). The only ‘abstract’ items in the description are START and END. At least one (possibly more) of the pair-wise transitions must begin with START, and at least one transition must have END as its second term. The set of transitions must provide at least one ‘route’ from START to END. There is no further restriction on the pair-wise transitions between elements that may be listed as belonging in the language concerned.
A First-order Markov language is not necessarily ﬁnite. To cite a human example, inclusion of the transition very very beside possible transitions from very to other elements, will generate an inﬁnite language. Strings in this language could have indeﬁnitely long sequences of verys in them. The song repertoire of the blue-black grassquit is, however, ﬁnite, consisting of a single call. Representing this extremely simple repertoire by a First-order Markov description, rather than as a holistic chirp, does justice to its somewhat complex internal structure.
A First-order Markov, or Strictly 2-local, model speciﬁes the set of possible sequences of actions, or sequences of elements in a string, by a transition table which shows, for each element in the system, what element may immediately follow it.28 Sometimes the pairs in the transition list are augmented by probabilities. For instance, a First-order Markov model approximation to English would calculate from a large corpus of English texts the probabilities with which each English word in the corpus is followed immediately by the other

28 Sometimes, just to add to the confusion, such a model is called a ‘Second-order’ model, and in such cases all the other orders are promoted by 1. We won’t be concerned with higher-order Markov models.

38

the origins of grammar

words. The model would manage to generate an extremely crude approximation to English text by simply moving from the production of one word to production of the next, according to the probabilities in the transition table. Here is an example of a 20-word string generated by such a First-order Markov process: sun was nice dormitory is I like chocolate cake but I think that book is he wants to school there.29 By sheer chance here, some sequences of more than two words are decent English, but the model only guarantees ‘legal’ transitions between one word and the next.
An interesting demonstration that birds can learn somewhat complex songs on the basis only of First-order transitions (as above) is given by Rose et al. (2004). The white-crowned sparrow (Zonotrichia leucophrys) song is typically up to ﬁve phrases30 in a stereotyped order, call it ABCDE. The authors isolated white-crowned sparrow nestlings and tutored them with only pairs of phrases, such as AB, BC, and DE. They never heard an entire song. Nevertheless, when the birds’ songs crystallized, several months later, they had learned to produce the whole intact song ABCDE. By contrast, birds who only ever heard single phrases in isolation did not eventually produce a typical white-crowned sparrow song. These researchers also gave other birds just pairs of phrases in reverse of normal order, for example ED, DC, and BA. In this case, the birds eventually sang a typical white-crowned sparrow song backwards. (Other work on the same species demonstrates, however, that the order of phrases is not solely a product of learning, but to some degree a matter of innate biases. Soha and Marler (2001) exposed white-crowned sparrows just to single phrases at a time, but the birds ended up singing songs with more than one phrase, and in a species-typical order.)
The example of the blue-black grassquit was a simple start, showing serial structure in what might seem to the human ear to be a unitary, atomic signal. The white-crowned sparrow study showed the adequacy, for this bird at least, of a simple First-order transition model for song-learning. In general, Firstorder Markov descriptions are adequate to capture the bare observable facts of wild birdsong repertoires. That is, in terms of weak generative capacity, the natural songs do not even require the slight extra power of State Chain descriptions (which I will describe immediately). For even such a versatile bird as the nightingale, ‘the performance of his repertoire can be described as a Markov process of ﬁrst (or some times second) order’ (Dietmar Todt, personal communication).

29 From Miller and Selfridge (1950, p. 184). 30 Each phrase consists only of a note of a single type, sometimes repeated several times, so these are rather low-level ‘phrases’.

a n i m a l sy n ta x? language as behaviour

39

f

i a b cd

e

a

Fig. 1.2 State Chain diagram of a simple Bengalese ﬁnch song.
Note: The START state is the left-hand circle. The ﬁlled circle is the END state, where it is possible to ﬁnish the song. Note the appearance in two different places of the note ‘a’. First-order transitions after this note are: to ‘b’, but only if the ‘a’ was preceded by ‘i’ or ‘ f’; and to ‘f ’, but only if the ‘a’ was preceded by ‘e’. Thus a First-order Markov transition table could not accurately describe this song pattern.
Source: From Katahira et al. (2007).

An interesting exception is the case of Bengalese ﬁnches, bred in captivity for about 240 years (Okanoya 2004). These birds have developed a song requiring a State Chain description (or a higher-order Markov description, taking into account more than just a single preceding element). Katahira et al. (2007, p. 441) give a succinct summary of the issue: ‘Bengalese ﬁnch songs consist of discrete sound elements, called notes, particular combinations of which are sung sequentially. These combinations are called chunks. The same notes are included in different chunks; therefore, which note comes next depends on not only the immediately previous note but also the previous few notes’. A simple example is described by the State Chain diagram given in Figure 1.2. In this example, it is crucial that the note identiﬁed as ‘a’ in both places is in fact the same note. If it is actually a slightly different note, the song possibilities can be captured by a First-order Markov description. Also crucial to the analysis in terms of Formal Language Theory is a decision as to what the basic units of the song are. In this example, if the sequences ‘ab’ and ‘ea’ were treated as single units, then the song possibilities could also be captured by a First-order Markov description. In fact there is widespread agreement among bird researchers as to what the basic units are, based on (the potential for) brief periods of silence during the song, data from learning patterns, and neuroscientiﬁc probing. In the case of Bengalese ﬁnches, it is uncontroversial that the basic units are as shown in Figure 1.2. Thus this bird’s song repertoire should be classiﬁed as a State Chain language.
Deﬁnition of a State Chain language: A State Chain language is one which can be fully described by a State Chain diagram. A State Chain diagram represents a set of ‘states’ (typically as small circles in the diagram), with transitions between them represented as one-directional arrows. On each arrow is a single element (e.g. word or note) of the language described. One particular state

40

the origins of grammar

is designated as START, and one is designated as END. A sentence or song generated by such a diagram is any string of elements passed through while following the transition arrows, beginning at the START state and ﬁnishing at the END state. The transition arrows must provide at least one route from START to END.31 There is no other restriction on the transitions between states that may be speciﬁed as contibutory to generation of the language concerned.
A State Chain language is not necessarily ﬁnite, because of the possibility of a transition arrow looping back to a previously passed state, thus generating an indeﬁnite number of possible passages through a certain portion of the diagram.
State Chain languages make only very simple demands on computational machinery, such as keeping no memory of earlier parts of the sentence (or string of characters input to a computer). The instructions needed to generate a sentence of a State Chain language basically say only ‘given the state you have got yourself in, here is what to do next’. For instance, at the start of the utterance there is a limited choice of designated ﬁrst elements to be uttered— pick one of them and utter it. Once some ﬁrst element has been chosen and uttered, that leads the organism into some particular ‘state’, from which the next choice of designated elements can be listed. Having chosen and uttered this second element of the signal, the organism is now in a (possibly new) state, and given a choice of next (in this case third) elements of the signal. And so on, until an ‘END!’ choice is given. For State Chain languages, the structure is inexorably linear, from beginning to end of the signal. Do the ﬁrst thing, then do the next thing, then do the next thing, . . . , then stop. The speciﬁcation of a State Chain language recognizes no higher-level units such as phrases. Applied to a human language, this would be like attempting to describe the structure of grammatical sentences without ever mentioning higher-level units such as phrases or clauses—obviously inappropriate. But as we will see later, some phrase-like hierarchical structure can be easily captured in a State Chain description.
A more complex example of Bengalese ﬁnch song is given by Honda and Okanoya (1999), in which a note labelled ‘b’ immediately follows, depending on the place in the song, any of four other notes. This certainly motivates a State Chain description, but it is notable that even in this case, according to

31 If there is only one route in a diagram from START to END, there would in fact be no need to use a State Chain description, because a weaker First-order Markov description would sufﬁce.

a n i m a l sy n ta x? language as behaviour

41

their diagram, just six transitions need the State Chain mechanism, whereas 26 other transitions can be accounted for in First-order Markov terms. Thus even this somewhat complex song does not exploit State Chain machinery very comprehensively. Based on a statistical analysis of chickadee songs, Hailman et al. (1985, p. 205) conclude that ‘transitional frequencies do not occur strictly according to [a] ﬁrst-order analysis . . . ; small, but possibly important, effects occur over greater distances within a call than simply adjacent notes’.
These last examples demonstrate that there exist State Chain languages, the simple Bengalese ﬁnch repertoire and possibly that of the chickadee, that are not First-order Markov languages. Rogers and Pullum (2007) mention another example (inﬁnite, as it happens) of a State Chain language that is not a Firstorder Markov language. This is a set of strings that they call ‘Some-B’, made up from any combination of As and Bs, with the sole proviso that each wellformed string must contain at least one B (but not necessarily any As). It is not possible to devise a First-order Markov transition table for Some-B, capturing all and only the ‘legal’ strings of this stringset, but a State Chain description can be given for it. The example of the Bengalese ﬁnch showed a possible, and very rare, case from birdsong where a First-order transition model is not adequate, and a State Chain description is necessary.
There is an alternative, and entirely equivalent, way of representing the information in a State Chain diagram, in terms of a very constrained type of rewrite rules. The rules below are equivalent to the diagram in Figure 1.2. You can match each of these rewrite rules to one arc in the State Chain diagram in Figure 1.2.
SSTART → i S1 S1 → a S2 S2 → b S3 S3 → c S4 S3 → e S5 S4 → d SEND S5 → a SEND SEND → f S1
Here the terms S1, S2, . . . , S5 correspond to the circles in the diagram notation; they denote internal states of the machine or organism. And each boldface small letter denotes an actual note of the song. A rule in this format (e.g. the second rule) can be paraphrased as ‘When in state S1, emit the element a and get into state S2’. The rewrite rules for State Chain systems may only take the above form, with a single internal-state symbol before the arrow, then a terminal symbol after the arrow, that is an actual observable element of the

42

the origins of grammar

system, followed optionally by another internal-state symbol, leading to the next action (rule) in the system; where no next-state symbol occurs, this is the end of the utterance. Although the format of rewrite rules does not make the form of songs as obvious to the eye as the diagram format, the rewrite rule format has the advantage of being closely comparable with the format in which the more powerful, less constrained, Phrase Structure grammars are presented. Phrase Structure grammars are deﬁned and illustrated in a later section.
Notice the reference to states of the organism or machine in the characterization of State Chain languages. A description of a language in State Chain terms thus postulates abstract entities, the states through which the machine or organism is running, in addition to the actual elements of the language, so-called terminal symbols.32 The action that is to be performed next, or the elementary sound unit that is to be emitted next, depends on the state that the system is currently in, and not directly on the action that it has just previously performed or the sound that it has just previously uttered. This distinguishes State Chain languages from weaker systems such as First-order Markov models.
As State Chain machinery is clearly inadequate for human languages, linguists pay little attention to classes of languages of this lowly rank on the Formal Language Hierarchy. In terms of weak generative capacity, almost all birdsong repertoires belong down here, even below the State Chain languages. It is in fact possible to deﬁne a richly textured sub-hierarchy of languages below the level of State Chain languages, and characterizations of these classes of languages can be given purely in terms of the terminal elements of the languages. Rogers and Pullum (2007) describe a ‘Subregular Hierarchy’, which subdivides the space of languages below the State Chain (or Regular) languages in the Formal Language Hierarchy. Only one of these classes of languages has concerned us here, a class that Rogers and Pullum call the ‘Strictly Local’ stringsets. The members (strings or ‘sentences’) in a Strictly Local stringset are deﬁned, as the label suggests, just by the local preceding neighbours of each word. The Strictly Local (SL) stringsets are in fact themselves an inﬁnite set (of classes of language), one for each natural number from 2 up. The number associated with each level of Strictly Local stringset indicates

32 In fact, appeal to abstract states is not necessary to describe a State Chain, or ‘Regular’, language, as it can also be done by Boolean combinations of expressions consisting of only the elements of the language, so-called ‘regular expressions’. But in this case, the regular expressions themselves can be indeﬁnitely large, and such a description is no more conspicuously insightful than a State Chain description.

a n i m a l sy n ta x? language as behaviour

43

how many elements ﬁgure in a string deﬁning what element may come next. For example, a SL2 stringset description of a language is just a list of the pairs of successive elements that occur in strings of the language. Thus a SL2 stringset description of a language is equivalent to a (non-probabilistic) Firstorder Markov model of the language. As Rogers and Pullum (2007, p. 2) note, in the context of some attempts to apply the Formal Language Hierarchy to animal behaviour, ‘the CH [the Formal Language Hierarchy] seems to lack resolution’. As far as linguists are typically concerned, the bottom is the level of State Chain languages, and even these are only mentioned as a way of quickly dismissing non-human behaviours as far less complex than human languages. The message is well taken, and this subsection has shown that, on a narrow approach, many animal song repertoires can be described by devices even less powerful than State Chain descriptions, namely First-order Markov descriptions.
Let’s take a moment (three paragraphs, actually) to reﬂect on the spirit of the enterprise that is our background here. One way of conceiving a central goal of linguistics is that we are interested in ﬁnding the strongest justiﬁable hypotheses about what can be, and what cannot be, a human language. ‘This general theory can therefore be regarded as a deﬁnition of the notion “natural language” ’ (Chomsky 1962b, p. 537). At the level of weak generative capacity, we can also use the Formal Language Hierarchy to arrive at the deﬁnition of ‘possible bird song’. This approach has the simplifying attraction that it brings with it a pre-conceived broad hypothesis space, and the goal becomes to eliminate wrong hypotheses. Further, the Popperian imperative to make more readily falsiﬁable, and therefore stronger, conjectures pushes theorists to constrain the class of languages that they claim are possible human languages. On this approach, to claim, for example, that all human languages are State Chain languages is to make a more falsiﬁable claim than claiming that all human languages are Phrase Structure languages. Chomsky’s early work convincingly demonstrated that human languages are not State Chain languages, leaving us with the less falsifable hypothesis that they occupy a rank higher on the Formal Language Hierarchy than State Chain languages. So humans have evolved brain mechanisms allowing them to control a larger class of languages than State Chain languages. Among birds, only the captive human-bred Bengalese ﬁnch apparently has a song complex enough to require a State Chain description. All other songs, as far as weak generative capacity is concerned, are of the simplest type, namely First-order Markov systems.
In the history of this branch of linguistics, the problem became where to stop on the Formal Language Hierarchy without going all the way to the top. The very top, which I have not included in the scheme above, is the

44

the origins of grammar

class of all abstractly conceivable languages.33 It is empirically uninteresting, in fact tautologous, to equate the class of human languages with the class of all abstractly conceivable languages. It says nothing more than that human languages are languages. This became a problem in the 1970s, when Peters and Ritchie (1973) proved that the formalisms current at the time were capable of describing any conceivable language, and were therefore strictly empirically vacuous.
To brieﬂy step aside from linguistics, biologists do not consider the central goal of their discipline to be the characterization of the set of theoretically possible life forms. Even more outlandishly, social anthropology students are not taught that the main point of their subject is to delineate the set of theoretically possible human societies. In both cases, the ‘theoretically possible X’ goal is not obviously incoherent, but within life sciences, including human sciences, only linguistics (and only one branch of it) has taken it seriously as a central goal. In non-life sciences, many subjects, for example chemistry or astronomy, have set out basic principles, for example the periodic table of elements or Einsteinian laws, which do in fact set limits on possible systems. An implicit understanding has been reached in these subjects of what systems there could possibly be, based on ‘known’ principles of how things are. In life sciences, such as neuroscience or genetics, however, although obviously many basic principles are known, the ongoing quest to discover more and further principles in these subjects is so vital and consuming that ultimate objectives such as ‘theoretically possible functioning nervous system’ or ‘theoretically possible viable genome’ are impractical distractions from the central research effort. It is a mark of the ambition of early Chomskyan linguistics that it articulated a goal so closely resembling what had been implicitly approximated in chemistry or astronomy, but not in neuroscience or genetics. This ambition seemed more realistic to the extent that the study of language was detached from such considerations as viability or function. These considerations bring in complications from outside the domain of immediate concern, such as how individuals manage to get along in primate society, and what kinds of message it would be advantageous be able to communicate and understand. But it seems very likely that the language faculty and individual languages got to be the way they are largely under the constraints and pressures of viability and function. These thoughts echo Culicover and Nowak (2003, pp. 6–12), writing ‘Linguists have, either consciously or unconsciously, modelled their

33 More technically put, an organism that could manage any of the abstractly conceivable languages would have the power of a universal Turing machine, i.e. it could generate any member of the class of recursively enumerable languages.

a n i m a l sy n ta x? language as behaviour

45

ideas of what a linguistic theory should look like on physics. . . . [Language] is a social and psychological phenomenon, and has its roots in biology, not physics. . . . [Linguistics] has historically not aggressively sought uniﬁcation [with other subjects], while physics has’.34

1.3.2 Iteration, competence, performance, and numbers
I turn now to two birds with slightly fancier repertoires, the much-studied zebra ﬁnch and the chafﬁnch. These provide good examples for making a point about iterative re-use of the same elements in a variety of somewhat different songs.

Microphone signal

i i i i iAB C 9

D E F G AB C D E F GAB C

D EF G

Frequency (kHz)

0

0.5

Time (s)

3.5

Introductory notes

Motif

Motif

Motif

Song bout
Fig. 1.3 The normal structure of adult zebra ﬁnch song. Top trace shows the raw microphone signal, parsed into discrete bursts of sound (syllable). Bottom trace shows the time-frequency spectrogram of the song. After some introductory notes, syllables are produced in a repeated sequence called a motif. During a bout of singing, a motif is repeated a variable number of times.
Source: From Leonardo (2002).

34 See Newmeyer’s 2005 book Possible and Probable Languages for extensive discussion of the idea of possible languages, relating it to the competence–performance distinction.

46

the origins of grammar

Leonardo (2002, p. 30) gives a spectrogram of normal adult zebra ﬁnch song, reproduced in Figure 1.3. Leonardo gives a parse of a typical songbout as:35
iiiiiABCDEFGABCDEFGABCDEFG
This song pattern is economically described by the First-order Markov transition table below.
START i ii iA AB BC CD DE EF FG GA G END
In some cases there is more than one possible transition. These account for the optional iteration of initial ‘i’s, and the option of going round the A B C D E F G cycle again after a G, or just ending the song.
The zebra ﬁnch repertoire is varied, in several ways. (1) The motif may be repeated a variable number of times, and (2) within a motif there can be slight variations, although apparently not enough to make it a different motif. I will discuss the variable repetitions of single notes or whole motifs later. Now, we’ll address the within-motif variation, which has only been described recently. ‘In his landmark study, Immelmann (1969) indicated that individual zebra ﬁnches sing the notes in their song motifs in a stereotyped order. One assumes Immelmann to mean that males sing notes of unvarying form in a ﬁxed order in each motif.’ (Sturdy et al. 1999, p. 195). Sturdy et al.’s own research showed that this is not strictly true. There is some variability in the song, but nothing that cannot be handled (on a narrow approach) by a First-order Markov model. ‘The predominant motif accounted for an average proportion of only .66 of all the motifs sung by 20 zebra ﬁnches recorded. . . . How do zebra ﬁnches deviate from their predominant note order? About half the deviations result from

35 Researchers vary somewhat in their classiﬁcation of the basic units of the zebra ﬁnch song, but not enough to affect the discussion here. Whereas Leonardo recognized seven distinct notes, A to G, the classiﬁcation of Sturdy et al. (1999) identiﬁed six notes, labelled ‘Introductory’, ‘Short slide’, ‘Flat’, ‘Slide’, ‘Combination’, and ‘High’. One of these is the vocabulary of the zebra ﬁnch repertoire; we’ll use Leonardo’s analysis. The classiﬁcation by Zann (1993) was ﬁner, identifying 14 different units. Nothing rests on these differences here.

a n i m a l sy n ta x? language as behaviour

47

skipped notes and the other half from added and repeated notes’ (ibid., p. 201). Skipped steps and repetitions of the immediately preceding note can be incorporated into a First-order Markov description (but at the cost of projecting an inﬁnite set of potential songs). A First-order Markov description of zebra ﬁnch song breaks the song down into a number of elements and shows how these are linearly combined. ‘The probabilistic sequencing of syllables by the bird on a particular day can be fully characterized as a Markov chain. . . , in which the likelihood of singing a particular syllable depended only on the occurrence of the last syllable produced, and not on any prior syllables’36 (Leonardo 2002, p. 36).
Zebra ﬁnches, then, have a stereotyped song, which can be varied by occasional skips, repeats, and additions of notes. This actually raises an issue much discussed in connection with human language, the distinction between competence and performance. Without using these speciﬁc terms, Sturdy et al. suggest an explanation of this kind for the variability in zebra ﬁnch motifs. ‘One explanation for why zebra ﬁnches sing more than one note order is that intact, normally reared males intend to produce a stereotyped motif but memory and other constraints interfere’ (1999, p. 202). The use of ‘intend’ here may shock some. Who can know what a ﬁnch intends? Nevertheless, it seems plausible that the ﬁnch’s behaviour is determined by two distinct kinds of factor: (1) a learned motor routine, requiring ideal conditions for its smooth execution, and (2) the natural shocks that ﬂesh is heir to. Sturdy et al. (1999) mention some evidence for this competence/performance distinction affecting variability in zebra ﬁnch song—lesioned or deprived ﬁnches produce more variable songs (Scharff and Nottebohm 1991; Volman and Khanna 1995).
Linguists tend strongly to compartmentalize competence and performance. Syntactic theorists only study competence,37 native speakers’ intuitions of the well-formedness of strings of words in their language. The study of performance, for example relative difﬁculty in parsing sentences, speech errors by normal speakers, and aphasic language, typically assumes certain canonical target forms as a baseline for study. That is, it is assumed that performance factors can disrupt the output of an idealized competence. The sentence blueprint (competence) deﬁnes a perfect product; execution of this blueprint in real time and space introduces imperfections. Certainly, this happens. But it

36 That is, in the terms I have used, a First-order Markov transition table, with probabilities associated with each transition. Leonardo’s assertion is probably not strictly true in that assigning probabilities at the micro-level to transitions between notes will not capture the observed distribution of numbers of repetitions of a higher-level motif.
37 At least in their capacity as syntactic theorists. Some individual researchers can switch roles.

48

the origins of grammar

is seldom admitted that the causality can also go the other way, that is that performance factors can affect the shape of competence. We will discuss this in greater detail in several later chapters, but for now, it is interesting to note an insightful comment on zebra ﬁnch variability by Sturdy et al.:
It is possible to accept the hypothesis that an intact brain and normal experience work together to make the note order in individual birds’ songs more consistent without accepting the idea that the goal of this consistency is highly stereotyped songs. According to this explanation, zebra ﬁnches strike a balance, singing more than one note order to create variation to avoid habituation effects on females without increasing variation in note order so much that it hinders mate recognition.
(Sturdy et al. 1999, p. 203)
Metaphorically, what they are suggesting is that zebra ﬁnches allow themselves a certain amount of deviation from their canonical target motif, and that this may have adaptive value. This is putting it too anthropomorphically. A more acceptable formulation is that evolution has engineered an adaptive compromise between absolutely faultless control of the stereotype song and a certain level of disruptibility. This seems to be a particular case of a very general property of evolution, that it tolerates, and even tends toward, a certain level of ‘error-friendliness’—see von Weizsäcker and von Weizsäcker (1998). If such a thing worked in human language, this would mean, translated into linguists’ terms, an evolutionary interaction between performance factors and what determines competence. I think this does work for human language, especially when thinking about how languages evolve over time; the theme will be taken up in later chapters. For now, note that the issue arises even in a species as remote from us as zebra ﬁnches.
Some hummingbird song is complex in a similar way to the zebra ﬁnch’s repetition of motifs. The longest song that Ficken et al. (2000, p. 122) reported from a blue-throated hummingbird was ABCDEBCDEBCDEABCDE. On a narrow approach, this repertoire is also economically described by a First-order Markov transition table.
Chafﬁnch songs are quite complexly structured too, for a bird. Here is a description by an expert:
Each bird has 1–4 song types, rarely 5 or 6. The sequence of syllable types within a song type is absolutely ﬁxed, though numbers of each may vary. Every song has a trill, of 2–4 phrases, rarely 1 or 5, followed by a ﬂourish of unrepeated elements. The occasional brief unrepeated element may occur between phrases in the middle of a song (we call these ‘transitional elements’). The same phrase syllable or ﬂourish type may occur in more than one song type in an area or in the repertoire of an individual bird but, unless

a n i m a l sy n ta x? language as behaviour

49

[kHz] t1a
6

t1b tr t2a

t2b tf1

song

4

element
2
syllable

Phrase1 Trill

Transition

Phrase2

Flourish

0

1.0

2.0 [sec]

Fig. 1.4 A typical chafﬁnch song.
Note its clear structure into discrete parts. The initial ‘Phrase1’ consists of a number of iterated ‘syllables’; this is followed by a single ‘transition’, after which comes ‘Phrase2’ also consisting of a number of iterated ‘syllables’ of a different type; the song ends with a single distinctive ‘Flourish’. (I have used scare quote marks here because the use of terms like ‘phrase’ and ‘syllable’ in linguistics is different.)
Source: From Riebel & Slater (2003).

this happens, hearing the start of a bird’s song will tell you exactly what the rest of it

will be.

(Peter Slater, personal communication)

Figure 1.4 is an example a typical chafﬁnch song, from Riebel and Slater (2003). ‘The transitions between different syllable types are ﬁxed, but the number of same type syllable repetitions within phrases varies substantially between different renditions of the same song type (Slater and Ince 1982)’ (Riebel and Slater 2003, p. 272). A First-order Markov description of this particular chafﬁnch song type is given below, in seven transition statements.

START syllable1 syllable1 syllable1 syllable1 transition transition syllable2 syllable2 syllable2 syllable2 Flourish Flourish END

50

the origins of grammar

Here the transitions from one element to the same element describe iteration. Iteration must be carefully distinguished from recursion. Purely repetitive behaviour usually does not involve recursion. Iteration is doing the same thing over and over again. Walking somewhere is achieved by iterated striding. Dogs scratch iteratively. In doing something iteratively, the main consideration is when to stop, usually when some goal or satisfactory (or exhausted!) state has been reached. With iteration, no memory for the number of times the repeated action has been performed is necessary. Iteration is simply about sequence; recursion entails hierarchical organization.
Recursion involves keeping track of the steps that have been gone through. Recursion is deﬁned as performing an operation of a particular type while simultaneously performing the same type of operation at a ‘higher’ level. For example, in the sentence John said that Mary had left, the sentence Mary had left is embedded inside the larger sentence. English allows this kind of recursive embedding quite extensively, as in I know that Bill wondered whether Harry believed that Jane said that Mary had left.
In this example, the successive underlinings indicate the successive recursive embeddings of a sentence within a sentence. To correctly grasp the meaning of the whole large sentence, it is necessary to keep track of exactly what is embedded in what, for example what was the object of Harry’s belief, or of Bill’s wondering.
Recursion is a special subcase of hierarchical procedural organization. Depending on one’s analysis, not every hierarchically organized activity involves doing an action of type X while doing the same type of action at a ‘higher’ level. For forty years, until recently, linguists have deﬁned recursion in terms of the phrasal labels assigned to parts of a sentence’s structure. So a noun phrase (NP) inside a larger NP, as in the house at the corner, or a sentence inside another larger sentence, as in Mary said she was tired, counts as a case of recursion. But a sentence can have quite complex structure, with phrases inside other, different kinds of, phrases, and this would not have been counted as recursion. For instance, the sentence Yesterday Mary might have bought some very good shoes on the High Street is hierarchically structured, but linguists would not have given it as an example of grammatical recursion, because here there is no embedding of a phrase of one type inside a phrase of the same type. But if one considers the overall goal of parsing such a phrase, then arguably recursion is involved in even such a simple expression as very good shoes because one has to parse the constituent very good and store (keep track of) its analysis as a subtask of the parsing of the whole phrase. There is parsing of

a n i m a l sy n ta x? language as behaviour

51

parts within parsing of the whole. This new interpretation of recursion, quite reasonably based more on procedures of use than, as hitherto, on grammatical labels, has crept rather surreptitiously into the recent literature. Essentially the same idea seems to be what Nevins et al. (2009a) have in mind when they write ‘if Pirahã really were a language whose fundamental rule is a nonrecursive variant of Merge, no sentence in Pirahã could contain more than two words’ (p. 679). It is a pity, and confusing, that in their previous paper in the same debate (Nevins et al. 2009b), they constantly referred to ‘iterative Merge’. If a Merge operation can be applied to its own output, as it deﬁnitely can, this is recursion. (The Pirahã language and recursion will be discussed more fully in Chapter 5, section 4.) This suggestion does not weaken the concept of recursion to vacuity. There remains a crucial difference between iteration and recursion. A dog scratching, for instance, does not keep track of the individual ‘strokes’ in its scratching routine. Nor does the production of the chafﬁnch song involve recursion. The chafﬁnch song is a hierarchical arrangement of subparts ‘Phrase1’ and ‘Phrase2’, each of which consists of iterated syllables. But the song is not a case of recursion, as far as can be seen, principally because there are no meanings of subparts of the song whose contribution to the meaning of the whole needs to be kept track of. The First-order Markov description I have given for a typical chafﬁnch song, while adequate in weak generative capacity, does not explicitly recognize the hierarchical organization into phrases, which would be intuitively desirable from a standpoint of strong generative capacity. I will return in a later section to how neuroscientiﬁc evidence can shed some light on a bird’s neural representation (subconscious of course) of hierarchical ‘phrasal’ structure in its song.
Iteration can be captured in a First-order Markov description by a transition from one element to itself. And as we saw with the zebra ﬁnch and bluethroated hummingbird songs, iterated sequences of longer ‘phrases’ or motifs can also (on a narrow approach) be handled by First-order Markov descriptions. (The champion syllable iterator among birds is the canary. Stefan Leitner (personal communication) tells me he has observed a canary ‘tour’, in which a syllable is iterated 137 times; and he has sent me the sonogram to prove it!) The closest we get in English to this kind of iteration is with the childish string very, very, very, very, . . . repeated until the child gets tired. Extensive iteration is indeed rare in human language, but at least one language is reported as using it with, interestingly, a similar numerical distribution of iterations as found for the chafﬁnch syllables. The Hixkaryana language of northern Brazil had, in the 1970s, about 350 speakers remaining. Derbyshire (1979a), in a careful description of this language, writes ‘The ideophone is a noninﬂected onomatopoeic word . . . The ideophone may be a single morpheme . . . or a

52

the origins of grammar

sequence of reduplicated forms (e.g. s-ih s-ih s-ih s-ih s-ih “action of walking”; in the latter case the number of repeats of the form may be from two to ten or more, but it is usually not more than six’ (p. 82).
In the above Markov description of chafﬁnch song, I have not built in any upper or lower limit to the number of times the bird may go around the several iterative loops. According to this transition table, the bird might repeat the syllable in the ﬁrst phrase of the trill perhaps a hundred times, perhaps not repeat it at all, choosing not to go around the loop. This fails to capture a typical feature of the song. In real chafﬁnch song, some phrase types may involve between four and eleven iterations, with a median of seven (Riebel and Slater 2003); other phrase types may involve somewhat fewer iterations. So the transition table does not do justice to the numerical range of iterations in the chafﬁnch song. No doubt, the number of iterations is conditioned by such factors as the bird’s current state of health, how long it has been singing in the current bout, and the phrase type. But it also seems reasonable to assume that the median number of seven iterations is part of the bird’s canonical target. There is also a negative correlation between the length of the initial Trill component and the ﬁnal Flourish component; ‘. . . the two song parts must be traded off against each other as either long trills or long ﬂourishes can only be achieved by shortening the other part of the song’ (Riebel and Slater 2003, p. 283). The authors suggest the idea of a ‘time window’ for the whole song. It is tempting to attribute this to the bird’s need to ﬁnish the whole call on the same out-breath, but in fact we cannot assume that the whole call is achieved in a single breath. Franz and Goller (2002) found that zebra ﬁnch syllables within the same call are separated by in-breaths. Hartley and Suthers (1989) show that canaries take ‘mini-breaths’ in the pauses between syllables of their songs.
A natural description of the chafﬁnch song recognizes it as a complex motor program, organized into a sequence of subroutines. These subroutines may loop iteratively through certain deﬁned gestures, a particular gesture being appropriate for each separate subroutine type. The whole motor program for the song is constrained by certain limits on length, so that the whole program has to be got through in a certain number of seconds. Here again, we see something analogous to an interaction between competence and performance, as linguists would perceive it. Evolution seems to have engineered the chafﬁnch song so that it is constructed in phrases, with the possibility of iteration of syllables inside each phrase. First-order Markov descriptions are not inherently designed for expressing numerical limits on iteration or on the overall length of whole signals. And indeed no type of grammar as deﬁned by the basic Formal Language Hierarchy is designed to express such quantitative

a n i m a l sy n ta x? language as behaviour

53

facts. A deﬁning assumption is that the classes of grammars and languages speciﬁed are not subject to any numerical constraints. In the typical division of labour used in describing human languages, issues to do with number of repetitions (e.g. of adjectives or prepositional phrases) or of sentence length are the province of stylistics or performance and not of grammar or competence. From the perspective of human language, chafﬁnch song is designed to be both syntactically somewhat complex and attractively stylish. The sequential structure with apparent ‘phrases’ gives it a certain complexity, and the numerical limitations on iterations and overall length mould this to a style attractive to female chafﬁnches. But for the chafﬁnch, its syntax and its style are all one indissoluble package. I have suggested the term ‘competence-plus’ to describe such a package of ‘algebraic’ and numerical information.
For human language, the distinction between grammaticality and good style is, for most linguists and for most cases, clear. But there are deﬁnitely borderline cases where it’s not clear whether a problem with a sentence is grammatical or stylistic. Here is a notorious example, involving self-centre-embedding, a contentious issue since the beginnings of generative grammar.
Where is the book that the students the professor I met taught studied?
The orthodox view in generative linguistics is that such examples are perfectly grammatical English, but stylistically poor, because they are hard to parse.38 If some degree of control over complex syntax evolved in humans independent of any semantic function, as suggested by Darwin and Jespersen, it was probably also constrained by the kind of factors I have, from my human viewpoint, identiﬁed as ‘stylistic’ in chafﬁnch song. If the Darwin/Jespersen scenario has any truth in it, then possibly as we humans later began to endow our signals with complex referential content, rather than just to impress mates with their form, numerical constraints on style or form were relegated to a lesser role compared to the more pressing need for conveying complex meanings. But there is no reason to suppose that numerical constraints were eliminated completely from the factors determining syntactic competence. This highlights a very general problem with a purely formal approach to natural biological systems, both human and non-human. Nowhere are numerical constraints on memory taken into account. Yet biological organisms, including humans, are constrained by memory and processing limitations. Grammars are good tools for describing idealized potential behaviour. A full account of actual behaviour, in humans and non-humans alike, needs to marry the regular non-numerical

38 The topic of centre-embedding will come up again in a later subsection, and in Chapter 3.

54

the origins of grammar

grammar-like properties of the behaviour with the constraints of memory and processing. This is not to abandon the idea of competence, but rather to envisage a description with two kinds of component, grammar rules and numerical constraints, ‘competence-plus’. I will revisit this issue in Chapter 3, on human syntax. For the moment, in the context of chafﬁnch song, the canonical target song can be described by the First-order Markov transitions given earlier, but now signiﬁcantly augmented by numerical statements of the approximate numerical constraints, as follows.

START syllable1 syllable1 syllable1 syllable1 transition transition syllable2 syllable2 syllable2 syllable2 Flourish Flourish END

4 ≤ x ≤ 11
4 ≤ y ≤ 11 x + y ≈ 14

This says that there can be between 4 and 11 iterations of ‘syllable1’, and between 4 and 11 iterations of ‘syllable2’, and that the total of the two iterative batches should be approximately 14. This conforms to Riebel and Slater’s (2003) description, and gives a much more accurate picture of the possible range of chafﬁnch songs. Augmenting the transition table with numbers like this actually makes it no longer a First-order Markov model, because it implies a counting mechanism that must remember more than just the previous syllable, in fact maybe as many as the ten previous syllables.39 Thus adding the numbers implies a signiﬁcant increase in the power of the processing mechanism. I will not delve into the implications for the place of such numerically augmented models in relation to the Formal Language Hierarchy. Very likely, the incorporation of numerical information fatally undermines a central pillar of the Formal Language Hierarchy.
The description given is of one particular song type. An individual chafﬁnch may have several (typically two to four) different song types. The structural pattern of all song types is very similar: from one to four ‘phrases’ each consisting of a number of iterated identical syllables, all followed by a ‘ﬂourish’ marking the end of the song. The First-order Markov description of one song type above is easily expanded to accommodate the whole repertoire of an individual. For each different song type in the repertoire, the transition from

39 Specifying probabilities for the individual First-order transitions would not give the desired frequency distribution with about 7 as the most common number of iterations.

a n i m a l sy n ta x? language as behaviour

55

START is to a different initial syllable; after the prescribed number of iterations of this syllable constituting the initial phrase, the next signiﬁcant transition is to a syllable characterizing the second phrase in that song; and so on.40
Chafﬁnch songs are highly stereotyped, the birds being genetically disposed to singing from a narrow range of songs, summarized by Peter Slater’s description: ‘Every song has a trill, of 2–4 phrases, rarely 1 or 5, followed by a ﬂourish of unrepeated elements’ (see above, p. 48). A description of the innate template, then, should also incorporate numerical information about the number of phrases that the learned songs may contain, as well as the range and central tendency of the repetitions of notes.
As with iterations of chafﬁnch syllables, a numerical qualiﬁer can be added to the First-order Markov description of zebra ﬁnch songs to account for the variable number of repetitions of its motif. A linguist might object, if he were bothered about birdsong, ‘How messy!’. Well, yes, biological facts are messy. We shall see later to what extent such ideas can be applied to the syntax of human languages. It must be acknowledged here that allowing the augmentation of a First-order Markov description with such numerical qualiﬁers introduces a new class of descriptions whose place on the Formal Language Hierarchy is not made clear. Indeed the introduction of numerical information seriously affects the pristine categorical approach to classes of languages. It is not my business here to try to develop a numerically sensitive alternative to the Formal Language Hierarchy.
One more point about the competence/performance distinction is in order. Performance factors are often portrayed as whatever is accidental or temporary, factors such as distraction, interruption or drunkenness while speaking. Another emphasis links performance to a distinction between what applies only to the language system (e.g. syntactic principles) and factors applying to other activities, factors such as short-term memory and processing speed, with performance factors being the latter. Note that these latter factors, such as short-term memory limits, are relatively permanent properties of organisms. Short-term memory, for example, does not ﬂuctuate signiﬁcantly in an adult (until dementia), whereas happenings like interruption by loud noises, distraction by other tasks, or medical emergencies are genuinely accidental and beyond prediction. A description of an organism’s typical behaviour cannot be responsible for these accidental factors. But relatively constant factors, such as processing speed and memory limitations, can be incorporated into

40 For quite thorough exempliﬁcation of a range of chafﬁnch song types, and how they change over time, within the basic structural pattern, see Ince et al. (1980).

56

the origins of grammar

a description of communicative behaviour, once one has made the a priori decision to be responsible for them. In fact, a comprehensive account of the growth of linguistic competence in an individual, or of the learning of its song by a songbird, cannot ignore such factors. No organism learns or acquires competence immune from the quantitative constraints of its body.
This last point about quantitative physical constraints contributing to the form of competence echoes a connection made in The Origins of Meaning (pp. 90–6). There, a robust quantitative constraint on the number of arguments that a predicate41 can take was attributed to a deep-rooted constraint on the number of separate objects the visual system can track. When linguists describe the argument structure of verbs, their valency is drawn from a very small range of possibilities, either 1 or 2 or 3 (some may argue for 4). Newmeyer (2005, p. 5) lists ‘No language allows more than four arguments per verb’ as a ‘Seemingly universal feature of language’, citing Pesetsky (2005). A few languages, including Bantu languages, have ‘causative’ and/or ‘applicative’ constructions which add an extra argument to a verb, but even in these languages the number of arguments explicitly used rarely exceeds three. The number of arguments that a predicate can take is central to human language, and the same general numerical constraints on semantic structure apply, though they are seldom explicitly stated, in the grammars of all languages.

1.3.3 Hierarchically structured behaviour
This subsection is mainly descriptive, giving well-attested examples of the hierarchical organization of singing behaviour in some species. We have already seen hierarchical structure in the chafﬁnch song. There are more spectacular examples. The species most notable are nightingales and humpback whales, very distantly related. In the case of whales’ songs, while accepting their clear hierarchical organization, I will dispute the claims of some authors that they reveal previously unsuspected complexity going beyond what has become familiar in birdsong.
In all the birdsong literature, there is a convergence on the concept of a song as a central unit of the birds’ performance, very much like the concept of a sentence in human syntax. A distinction is made between songs and mere calls, with calls being very short and having no complex internal structure. ‘Although

41 It is vital to certain arguments in this book to make a distinction between predicates in a semantic, logical sense, and the Predicate element of a grammatical sentence. I will always (except when quoting) use lowercase ‘predicate’ or small caps predicate for the semantic/logical notion, and an initial capital letter for ‘Predicate’ in the grammatical sense.

a n i m a l sy n ta x? language as behaviour

57

the differences between songs and calls are occasionally blurred, most of the time they are clear and unequivocal. First, calls are usually structurally much simpler than songs, often monosyllabic. . . . Singing is always a more formal affair. . . . Calling behavior is much more erratic and opportunistic’ (Marler 2004, p. 32).
. . . the linkage between a given social context and a particular signal pattern is quite ﬁxed in calls, but astoundingly ﬂexible in songs. In other words, during an episode of singing, most bird species perform different song patterns without any evidence that the social context has changed. . . . In contrast to calls, songs are learned and generated by vocal imitation of individually experienced signals.
(Bhattacharya et al. 2007, pp. 1–2)
Bird songs are roughly the same length as typical spoken human sentences, between one and ten seconds, and have some internal structure of syllables and notes. ‘In most species, songs have a length of a few seconds and the pauses separating songs usually have a similar duration. This patterning allows birds to switch between singing and listening, and suggests that songs are signiﬁcant units of vocal interactions. A song is long enough to convey a distinct message and, at the same time, short enough to allow a sensory check for signals of conspeciﬁcs or to reply to a neighbor’ (Todt and Hultsch 1998, p. 488). In looking for bird behaviour possibly related to human sentential syntax, it is natural to focus mainly on the song as the unit of interest.
The birdsong literature is generally conﬁdent in stating how many songs a species has in its typical repertoire, within some range. This assumes that song-types are categorially distinct, and can be counted. It is also of great interest, of course, to know whether the distinct categories are valid for the birds, rather than just for the human researchers. Searcy et al. (1995) describe the results of habituation tests suggesting that ‘in the perception of male song sparrows, different song types are more distinct than are different variants of a single type’ (p. 1219). Consistent with this, Stoddard et al. (1992) found that ‘song sparrows readily generalize from one exemplar of a song type to other variations of that song type’ (p. 274). Most bird researchers, working with a variety of species, assume the psychological reality of categorially distinct song-types, and the size of repertoires of song-types can be reliably quantiﬁed.
The champion combinatorial songster is the nightingale ‘(Luscinia megarhynchos), a species that performs more than 200 different types of songs (strophen), or more than 1000 phonetically different elements composing the

58

the origins of grammar

songs’ (Todt and Hultsch 1998, p. 487).42 A nightingale’s song repertoire is quite complex in itself, but still technically describable by First-order Markov transitions. This bird’s behaviour is also interesting because it can clearly be analysed into several units larger than the individual song, just as human discourse can be analysed into units larger than the sentence, for example paragraphs and chapters in written language. We will discuss this higher-level structuring of nightingale behaviour shortly, after a brief survey of the internal structure of the songs of this versatile bird.
A nightingale song typically contains sections (phrases) of four types, which the main researchers of this bird, Dietmar Todt and Henrike Hultsch, label Alpha, Beta, Gamma, and Omega.

Alpha sections are low in volume, whereas Beta sections consist of louder element

complexes or motifs. Gamma sections are made up by element repetitions that results

in a rhythmical structure of this song part (trill), whereas Omega sections contain only

one unrepeated element.

(Todt and Hultsch 1998, p. 489)

The sections always occur in this order, barring the odd accident. Figure 1.5 reproduces Todt and Hultsch’s ﬂowchart of element-types composing the nightingale songs. The ﬂowchart makes it clear that the repertoire can be captured by a First-order Markov transition table, as given partially below, equivalent to Todt and Hultsch’s incomplete ﬂowchart.

START 1a, b (2a,b other notes) 5a,b 6a,b 6a,b 7b 7a 8a 10a 9a 7b 7b 9b 10b 11b END

1a,b 2a,b 3a,b 4a,b (5a,b other notes) (6a,b another note) 8a 9a 10a 11a 7b 8b 10b 9b

2a,b 3a,b 4a,b 5a,b 6a,b 7a 7a 7a 9a 10a 11a END 8b 9b 10b 11b

To conﬁrm the applicability of a First-order Markov model to nightingale song, I asked Dietmar Todt, the main expert on this bird, ‘Is the end of one part

42 The assertion of over 1000 different elements is at odds with Anderson’s (2004, p. 151) assertion that ‘The nightingale’s many songs are built up from a basic repertoire of about forty distinct notes’. I take Todt and Hultsch (1998) to be the experts. 1000 notes, rather than 40, is consistent with the interesting generalization which Hultsch et al. (1999) make about all birdsong having smaller repertoires of songs than of notes, thus failing to exploit combinatoriality to advantage. A smaller vocabulary used for composing a larger song repertoire is in line with a linguist’s expectations, but apparently not the way birds do it.

a n i m a l sy n ta x? language as behaviour

59

1a,b

2a,b

3a,b

4a,b

5a,b

6a,b

7a

8a

9a 10a 11a

7b

8b

9b 10b 11b

α

β

γ

ω

Fig. 1.5 Flowchart of a typical nightingale repertoire.
Note: The numbers with subscript letters in the boxes represent distinct song elements. The empty boxes represent the beginning notes of sequences left unspeciﬁed in this incomplete ﬂowchart. The Greek letters below the chart label the Alpha, Beta, Gamma, and Omega sections of the song. It is clear that this ﬂowchart can be converted into an equivalent set of First-order Markov transition statements.
Source: From Todt and Hultsch (1998, p. 489).

(e.g. alpha) identiﬁable as a distinct end note/syllable of that part, so that the transition to a following part (e.g. some beta part) can be predicted just from the last note of the previous part?’ He replied ‘Yes, but with stochastic transitional probabilities’ (D. Todt, personal communication). That is, where there is more than one transition from a particular element, some information on the probability of the respective transitions needs to be given. A corroborative piece of evidence that the repertoire does not technically require a more powerful form of description, such as a State Chain diagram, is this statement: ‘Particular types of elements assessed in the singing of an individual bird occur at one particular song position only’ (Todt and Hultsch 1998, p. 488). In human grammar this would be like a particular word being conﬁned to a single position in a sentence. There is perhaps an ambiguity in the authors’ statement. Does it apply to all element-types, or just to a ‘particular’ subset of elementtypes? It is consistent with the rest of their descriptions of nightingale song, in this paper and others, that the statement applies to all element-types. In this case, every note in a song occupies its own unique characteristic slot in the sequence of notes. This is still compatible with there being several possible transitions from one note to the next. It also makes it clear how 200 songs are composed from a vocabulary of a thousand different notes. An analogy is with different journeys radiating outward from the same point of origin, with different routes often diverging but never reconverging (what Todt and Hultsch call ‘difﬂuent ﬂow’). A given route may make short, one-or-two-place loops back to the same place, later in the journey/song. Many journeys are

60

the origins of grammar

possible, the places visited are a predictable distance from the origin, and never revisited, apart from the short iterative loops, and more places are visited than there are journeys. These facts show nightingale song to be strikingly different from human grammar.
Nightingale song-types are typically collected into higher-level units called ‘packages’. ‘Each package was a temporally consistent group of acquired song types which could be traced back to a coherent succession of, usually, three to ﬁve (max. seven) model song types’ (Hultsch and Todt 1989, p. 197). In other words, depending on the order in which the young nightingale had experienced song-types in infancy, it reproduced this order in its adult performance, up to a maximum sequence of seven song-types. If it heard the song-type sequence A B C D frequently enough (about twenty times) as a youngster, then its characteristic song would also have these song-types contiguous in a long session of song. The particular packages have no clear structural features. That is, you can’t say, given a large set of song-types, which ones most naturally go together to form a package. The formation of packages results from each individual bird’s learning experience, and different birds have different packages. Isolated nightingales, who hear no model songs, do not form packages of song-types (Wistel-Wozniak and Hultsch 1992). So it seems that nightingales memorize whole sequences of song-types, analogous to a human child memorizing a whole bed-time story, except without the meaning.
The ﬁnal notes of song-types are distinctive of the song-types, so memory for a transition between the last note of one song and the beginning of the next would be possible. But this is not how the birds keep their packages together. ‘[M]ost song types within a package were connected to each other by multidirectional sequential relationships, in contrast to the unidirectionality of transitions in the tutored string’ (Hultsch and Todt 1989, p. 201). Thus it seems unlikely that packages are maintained through memorization of oneway transitions between song-types or notes. This is clear evidence of higherlevel hierarchical structuring. Hultsch and Todt (1989) assume a battery of submemories, each responsible for a package. They justify this analysis by pointing out that nightingales can learn sequences of up to sixty song-types with as much ease as sequences of twenty, and this most probably involves some chunking process. They also point out that songs that are presented during learning as not part of any frequently experienced package tend to be the songs that the birds fail to acquire.
In the wild, a bird’s choice of what song to sing depends on many factors, including responding to singing from rivals. In competitive singing, the patterns are harder to discern. Todt and Hultsch (1996) studied the simpler case of solo

a n i m a l sy n ta x? language as behaviour

61

singing by nightingales. Song sequencing here shows a remarkable fact. With a repertoire of 200 songs
. . . on average about 60–80 songs of other types are used before a given song type recurs. The recurrence number of 60–80 is not a mere function of repertoire size of an individual but varies with the frequency of use of a given song type: rare song types, for example, normally recur after a sequence that is two, three, or even four times as long as the average intersong string (i.e. after 120, 180 or 240 songs).
(Todt and Hultsch 1996, p. 82)
The different frequency of songs is a complicating factor, but there is an analogy here with the familiar statistical ‘birthday problem’. How many people need to be in a room for there to be a 50–50 chance of some two of them having the same birthday? The answer is 23. If there were only 200 days in a year, the answer would be much lower.43 So if a nightingale chooses its songs randomly and with equal frequency from a repertoire of 200, how many songs does it need to sing for there to be a 50–50 chance that the next song will be one that it has sung before? The answer is much lower than 23. Even with the different frequency of songs (like there being some days of the year on which more people are born than others), the ﬁgure of 60–80 is signiﬁcant. As humans, we would ﬁnd it hard to keep track of items from a vocabulary of 200, making an effort not to repeat any item too soon after its previous use. The obvious trick to achieve this is to recite the vocabulary in a ﬁxed order. Then we can be certain that each item will occur only once every 200 words. It is clear that nightingale song sequences are rather strictly ﬁxed. Todt and Hultsch conclude, ‘Because the periodic recurrence of a song type is not a consequence of a rigid sequence of song type delivery, the periodic recurrence has to be distinguished as a separate rule of song delivery’ (p. 82). But they give no statistical reasoning. I am not so sure that the periodicity of the songs is not a consequence of their somewhat ﬁxed order. If it is a separate rule of song delivery, it attributes an impressive memory feat to the nightingale. Memorizing a ﬁxed sequence is one memory feat. Not memorizing a ﬁxed sequence, but having the ability to remember what items have occurred in the last 60–80 events (like poker players remembering what cards have already appeared on the table) is a different kind of feat, certainly rarer in humans than the ability to memorize passages by rote.
The hierarchical behaviour of nightingales goes further than packages. Todt and Hultsch (1996) report a ‘context effect’. Two different tutors (humans using tape-players) exposed young nightingales to different sequences of master

43 Geoff Sampson tells me he believes the answer, for a 200-day year, would be 17, based on calculations at http://en.wikipedia.org/wiki/Birthday problem.

62

the origins of grammar

songs. The birds learned these song sequences, but kept them separate as ‘subrepertoires’. They tended not to mix packages, or songs, from one context of learning with packages or songs from another context. This reminds one of the behaviour of children growing up bilingually. If they get one language from their mother and another from their father, they will mostly keep the two languages separate, apart from occasional mid-sentence code-switching.
Overall, the authors propose a deep hierarchical organization of nightingale song:
notes (elements) < phrases (or motifs) < songs < packages < context groups
As argued above, the song is the most natural correlate in bird vocalization of the human sentence. Birdsong (or at least nightingale song) has structured behaviour both above and below the level of the song. Similarly, human language has discourse structure above the level of the sentence and grammatical and phonological structure below that level. But there the similarities peter out. Even such a versatile performer as the nightingale achieves the complexity of its act very largely by drawing on memorized sequences. There is very little of the ﬂexibility and productivity characteristic of human language.
At this point, still guided by the overall framework of the Formal Language Hierarchy, we leave the birds in their trees or lab cages and take a dive into the depths of the ocean, where whales and other cetaceans sing their songs. Recent studies have spawned some badly exaggerated reports in the popular science press and websites: for example, ‘fresh mathematical analysis shows there are complex grammatical rules. Using syntax, the whales combine sounds into phrases, which they further weave into hours-long melodies packed with information’ (Carey 2006). On the contrary, I will show that the rules are very simple, and the songs are far from being packed with information. And as previous examples from birdsong show, whalesong is not the only natural song with hierarchical organization. Don’t believe articles by credulous pop science reporters!
Not all whale ‘song’ is structured in the same way. Sperm whales, for example, have distinctive calls and regional dialects, all based on a vocabulary of one! The one unit is a click; clicks can be emitted in groups of various sizes, and with various time spacings between the groups. The distinctive calls are known as ‘codas’. ‘Codas can be classiﬁed into types according to the number and temporal pattern of the clicks they contain. For example, “2+3” is a coda containing two regularly spaced clicks followed by a longer gap before three more clicks while “5R” is a coda with ﬁve regularly spaced clicks’ (Rendell and Whitehead 2004, p. 866). This is a lesson in itself. Communicative signals can

a n i m a l sy n ta x? language as behaviour

63

be based on a vocabulary of one, and rhythm and temporal spacing used to distinguish calls. This is not how human language works. We won’t consider sperm whale codas further.
It has been claimed that humpback whale songs are in a clear sense more complex than anything we have seen so far. The most data have been collected from humpback whales, mainly by Payne and McVay (1971). Complete humpback whale songs may last as long as half an hour, and they string these songs together into sessions which can last several hours. In a single song session, the whale cycles around the same song over and over again, usually without a break between the end of one instance and the beginning of the next. The longest song session recorded by Winn and Winn (1978) lasted twenty-two hours! Each individual whale has a characteristic song, which changes somewhat from one year to the next. ‘There seem to be several song types around which whales construct their songs, but individual variations are pronounced (there is only a very rough species-speciﬁc song pattern)’ (Payne and McVay 1971, p. 597). It is not known whether both sexes or only one sex sings. In any given season, an individual whale sings just one song, over and over. Other whales in the same group sing distinct but similar songs. Across seasons, whale songs change. The ‘dialect’ changing over the years is reminiscent of chafﬁnch dialects changing.
Payne and McVay (1971) published a detailed description of recordings of humpback whale songs, in which they detected many instances of repeated ‘phrases’ and ‘themes’. They attributed a hierarchical structure of considerable depth to the songs, with different-sized constituents nested inside each other as follows: subunit < unit < phrase < theme < song < song session (p. 591). Their ﬁgure illustrating this structure is reproduced in Figure 1.6.
The great regularity of the songs is captured in the following quotations:
. . . phrases in most themes are repeated several times before the whale moves on to the next theme. . . . we ﬁnd it true of all song types in our sample that, although the number of phrases in a theme is not constant, the sequence of themes is. (For example, the ordering of themes is A,B,C,D,E . . . and not A,B,D,C,E. . . ). We have no samples in which a theme is not represented by at least one phrase in every song, although in rare cases a phrase may be uttered incompletely or in highly modiﬁed form.
(Payne and McVay 1971, p. 592)
In our sample, the sequence of themes is invariable, and no new themes are introduced or familiar ones dropped during a song session. Except for the precise conﬁguration of some units and the number of phrases in a theme, there is relatively little variation in successive renditions of any individual humpback’s song.
(Payne and McVay 1971, p. 591)

64

12 SUBUNITS

the origins of grammar
4 SUBUNITS

UNIT

UNIT UNIT

PHRASE

THEME

UNIT UNIT UNIT PHRASE

UNIT UNIT UNIT UNIT

PHRASE

SONG

THEME

SONG SESSION

Fig. 1.6 Hierarchical structuring of humpback whale songs.

Note: The circled areas are spectrograms enlarged to show the substructure of sounds which, unless slowed down, are not readily detected by the human ear. Note the six-tier hierarchical organization: subunit < unit < phrase < theme < song < song session.
Source: From Payne and McVay (1971, p. 586).

‘A series of units is called a “phrase.” An unbroken sequence of similar phrases is a “theme,” and several distinct themes combine to form a “song” ’.
(Payne and McVay 1971, p. 591)
From the spectrograms the authors give on p. 591 of repeated phrases within a theme, it can be seen that the phrases morph gradually with each repetition. Each repeated phrase is very similar to the next, but after a large number of repetitions similarity between the ﬁrst phrase and the last phrase of the cycle is much more tenuous. In these examples, the lowest number of repeated phrases with a theme is nine, and the highest number is forty-one. Figure 1.7 shows the same theme, sung twice by the same whale, once with nine repetitions of its characteristic phrase and once with eleven repetitions.
A human analogue of this behaviour is musical variations on a theme. It is unlike anything required by the structure of any language, although poets can reproduce such an effect, given the resources that a language provides. The difference between the ﬁrst and last instances of the phrase in the same theme is so great that, given these two spectrograms, a birdsong researcher would almost certainly classify them as different units.
For the birdsong examples, researchers typically rely on an intuitive ‘eyeballing’ method to spot repetitions of phrases. iiABCDEABCDEABCDE obviously contains three repetitions of ABCDE, which we might decide to call a ‘phrase’. Payne and McVay (1971) relied on similar impressionistic methods for the humpback whale song, albeit backed up by very careful and detailed

a n i m a l sy n ta x? language as behaviour

65

1

2

Whale I

Fig. 1.7 The same theme sung by the same whale in two separate songs.
Note the broad similarity between the two instances of the theme. But note also that the term repetition for the phrases is not strictly accurate, as each successive ‘repetition’ changes the phrase slightly, so that the ﬁrst and last instances are hardly the same at all.
Source: From Payne and McVay (1971, p. 591).

examination of their recordings. Technically, a repeated sequence in a series illustrates a case of autocorrelation, that is the correlation of a portion sliced out of a series of events with other portions earlier in the series. If the portions in question are identical, there is a perfect correlation, but lessthan-perfect matches can still be signiﬁcantly correlated. Suzuki et al. (2006) took Payne and McVay’s recordings and subjected them to close mathematical analysis, based on information theory. They discovered two layers of autocorrelation, conﬁrming an important feature of the analysis of the earlier researchers. Their analysis demonstrated that: ‘(1) There is a strong structural constraint, or syntax, in the generation of the songs, and (2) the structural constraints exhibit periodicities with periods of 6–8 and 180–400 units’ (p. 1849). They continue with the strong claim that ‘This implies that no empirical Markov model is capable of representing the songs’ structure’ (ibid.). Note the position of the apostrophe (songs’) in this latter claim, meaning that a Markov model is incapable of accounting for patterns across songs from many whales in different seasons. It will also be clear that there is a crucial difference between what Payne and McVay called a ‘song’ and the use of ‘song’ by Suzuki et al.
What we have with whales goes one step (but only one step) further than the zebra ﬁnch and chafﬁnch songs. Both birds repeat elements of their song at one speciﬁc level in its hierarchical organization. Chafﬁnches repeat low-level syllables within a phrase; the very same low-level unit is iterated, like a child’s

66

the origins of grammar

very, very, very, very, . . . . The repetitions in zebra ﬁnch song are of higher-level elements, namely whole motifs, consisting of several syllables, more like a repeated multi-word mantra. The humpback whale song has repetitions at two levels, of ‘phrases’ and of ‘themes’. The repeated themes are not identical, as the phrases are (allowing for the signiﬁcant morphing of a phrase during a theme). But themes are nevertheless identiﬁable as repeated structural types, in that each theme consists of a repetition-with-morphing of a single phrase. The repetitions are nested inside each other. The ‘phrase’ tier of this twolevel layered organization is reﬂected in the shorter (6–8 units) of the two periodicities detected by Suzuki et al.’s autocorrelation analysis. They write of ‘a strong oscillation with a period of about 6, corresponding to the typical phrase length of Payne et al. (1983)’ (p. 1861).
The longer of the two autocorrelations, with a phase of between 180 and 400 units, is most likely to come about because a whale sings the same song over and over again, without pausing between versions. Payne and McVay (1971) are clear on this point. ‘The gap between spectrographs of songs 1 and 2 is designed to make the individual songs clear and is not indicative of any gap in time’ (p. 586). ‘At the end of the second song, whale II stopped singing— one of our few examples of the end of a song’ (p. 588). ‘[H]umpback songs are repeated without a signiﬁcant pause or break in the rhythm of singing’ (p. 590).
Following the last phrase of the ﬁnal theme in either song type A or B, the whale starts the ﬁrst sound in the next song. . . without any noticeable break in the rhythm of singing. The pause between any two phrases of the last theme is, if anything, longer than the pause between the last phrase of one song and the ﬁrst phrase of the succeeding song. . . .
It is clear, however, that, regardless of where a song may begin, the whale continues the sequence of themes in the same irreversible order (that is, 3, 4, 5, 6, 1, 2, 3, 4, 5 . . . ).
(Payne and McVay 1971, p. 595)
The article by Suzuki et al. uses the term ‘song’ in a crucially different way. They took sixteen of Payne and McVay’s recordings, each containing several part or whole songs, and referred to these recordings as ‘songs’. For consistency with the earlier paper, they should have expressed their results in terms of recordings, not ‘songs’. Suzuki et al.’s longest recording lasted forty-ﬁve minutes and contained 1,103 units of song; the shortest recording was twenty minutes long and contained 380 units. Payne and McVay’s longest recorded song lasted thirty minutes and the shortest lasted seven minutes. At an ‘average singing rate of 2.5 s/unit’ (Suzuki et al. 2006, p. 1855) the longest, thirtyminute, song would have had about 720 units, and the shortest, seven-minute, song would have had about 168 units. The average length in song units of Suzuki et al.’s recordings was 794 units, longer than the likely length of Payne

a n i m a l sy n ta x? language as behaviour

67

Tone1

Sweep2

Note2 Arpeggio

Warble2

Grunt2 Warble1

Pulse

Tone2

Note1

Grunt3

Sweep1

Chirp Roar

Grunt1

Tone3

Grunt4

Note3

Tone4

Note4

Fig. 1.8 State Chain diagram for humpback whale song.

Note: Subscripts are mine, to distinguish between elements of the same type, e.g. different grunts. Note the six repeatable (looping) themes, the A, B, C, D, E, . . . mentioned in the text. Note also the transition from the end of the song back to the beginning, without a break. The unlabelled transition arrows are a convenience in diagramming, with no theoretical implications. (The typical number of repetitions of phrases is not speciﬁed here. The diagram also makes no provision for the morphing of phrases within a theme; in fact such continuous, rather than discrete, variation is outside the scope of Formal Language Theory.)
Source: This is my diagram based on Payne and McVay’s detailed but informal prose description of one particular song type, and using their descriptive terms.

and McVay’s longest song.44 The range of song lengths, in units, that is 168– 720, is comparable to the range of the longer periodicity, 180–400, detected by Suzuki et al. This mathematically detected higher layer of organization very probably comes about because of a whale’s habit of singing one same song over and over again without pausing.
Given that a phrase can be repeated an unspeciﬁed number of times, the distance between the units the next level up, the themes, is also unspeciﬁed, but nevertheless the whale remembers where it has got to in its sequence of themes. Suzuki et al. (2006) write: ‘The correlation data demonstrate that the songs possess strong long-distance dependencies of the sort discussed in Hauser et al. (2002) as a hallmark of phrase structure grammar’ (p. 1864). This is a serious overestimate of the humpback’s sophistication. The whale’s song is a rigorously uniform sequence of themes, A B C D E F, never in any other order, with each theme repeating (and morphing) its characteristic phrase many times. Such a song can be adequately described by a State Chain diagram, as in Figure 1.8.

44 In one of Payne and McVay’s recordings, not one used by Suzuki et al., there were seven successive songs.

68

the origins of grammar

The ﬁgure gives an idea of the complexity of the whale’s habitual song, even though it is describable by a State Chain description.
It is even just possible that an individual humpback’s song, at any given time in a given season, can technically be described by a First-order Markov model. It depends whether the units such as those labelled ‘grunts’, ‘tones’, ‘sweeps’, and so on in Figure 1.8 are the same units wherever they occur in the song, or are somewhat different, depending on their place in the song. If exactly the same ‘grunt’, for example, is used in four different places, with different transitions before or after it, this calls for a State Chain description. But if the grunts, notes, sweeps, etc., are actually different at each place in the song, then a First-order Markov transition model would be adequate. Judging from the various descriptions in the literature, it seems likely that at least some of these units are re-used ‘verbatim’ at several different places, so a State Chain description is called for.
I attribute less complex capacity to the humpback whale than Suzuki et al., but we are concerned with different things. In line with a linguist’s approach, where competence is the property of an individual, I am interested in the repertoire of a single animal. Social behaviour begins with and develops out of (and ﬁnally reciprocally affects) the behaviour of individuals. Suzuki et al. were trying to generalize over all sixteen recordings, from different whales, over two seasons. At one point they write ‘the humpback songs contain a temporal structure that partially depends on the immediately previous unit within a song’ (p. 1860). So even across a population there is some scope for First-order Markov description. This is followed up by ‘the Markov model failed to capture all of the structure embodied by the majority of the humpback songs we analyzed, and that the humpback songs contain temporal structure spanning over the range beyond immediately adjacent units’ (p. 1860). But this last conclusion was reached on the basis of only nine out of the 16 recordings. The other seven recordings were discounted because they ‘were recorded in a one week period in early February 1978. Since they are likely to be quite similar, those data points may not be statistically independent’ (p. 1860). This is fair enough, if one is trying to ﬁnd structure across a broad population, but our interest is in the singing capabilities of individual whales. So for the seven discounted recordings, taken within a one-week period, it is not the case that a Markov model failed to capture the structure of the song. Further, the ‘majority of the humpback songs’ referred to is nine out of 16 recordings— not an impressive majority, and certainly not a statistically signiﬁcant one.
It is not necessary to invoke any power greater than that of a State Chain model to describe humpback whalesong. A certain level of hierarchical

a n i m a l sy n ta x? language as behaviour

69

organization can be accommodated with State Chain descriptions. A more powerful type of description, Phrase Structure grammar (to be deﬁned in section 1.3.4) is designed to accommodate phrasal structure of a certain complex kind, which is most obviously associated with its semantic interpretation. Of whalesong, we have only the recorded behaviour, with no evidence that the song is informed by any compositional semantic principles. Simply equating hierarchical organization with Phrase Structure grammar is incorrect, despite the naturalness of describing some of the constituents of a song as ‘phrases’. The extraordinary structural rigidity of the whale’s song is easily captured by a State Chain diagram. As for long-distance dependencies, mentioned by Suzuki et al. (2006), this is a matter of terminology. Generally when linguists talk of dependencies between elements in a sentence, the criteria are at least partly semantic. An item is said to be dependent on another if their two meanings interact to contribute to the meaning of the sentence. A standard linguistic example of a long-distance dependency occurs in a sentence-type such as If X, then Y, where the clause instantiating X can be of any length; thus the distance separating the mutually dependent items if and then is unpredictable. Another kind of long-distance dependency, often purely syntactically motivated, involves agreement, as between a subject and its verb in English. Such a dependency requires there to be some choice of a feature, say between singular and plural, or between genders, where choice of a feature at one point in the sentence requires a matching choice to be made some distance away. But there is nothing like this in humpback song. Note that many of the longdistance dependencies mentioned by linguists are of a kind that Suzuki et al.’s heuristic methods could not possibly detect, because they involve empty or null items, not physically present in the sentence, but inferred to be ‘present’ for purposes of semantic interpretation. An example would be Who did Mary think John was asking Bill to try to ﬁnd?, where an understood ‘gap’ after ﬁnd is taken to be in a dependency relation with the Who at the beginning of the sentence. By contrast, consider the hypothetical case of a pathological person whose performance consists solely of reciting the alphabet over and over again, sometimes repeating a particular letter several times, but never deviating from strict alphabetical order, apart from these repetitions. Here, it would be true, strictly speaking, that occurrence of, say, M, depended on prior occurrence of B, and of F, and of K, and at unpredictable distances, because of the unpredictability of the individual letter repetitions. In this sense, and only in this very limited sense, there are in this case, essentially like the humpback whale’s song, long-distance dependencies. They are not the kind of long-distance dependencies that require Phrase Structure grammar to describe them.

70

the origins of grammar

My arguments here directly counter the following conclusions of Suzuki et al:
The hierarchical structure proposed by Payne and McVay (1971) for humpback whale song challenges these conjectures on the uniquely human nature of long-distance hierarchical relations, and potentially on the uniquely human property of recursion and discrete inﬁnity. Hierarchical grammars may be efﬁciently represented using recursion, although recursion is not necessarily implied by hierarchy. (2006, p. 1863)
The long-distance hierarchical relations in humpback whalesong are of an entirely simpler nature than those in human language. The mention of recursion is gratuitous; nothing in whalesong suggests the capacity to keep track of an element of one type while simultaneously processing an element of the same type ‘inside’ it, which is what recursion involves. There is no sense in which the humpback embeds one song inside a bigger song. There is certainly hierarchical structure, embedding phrases inside a song, but that is another matter. The authors continue to speculate on ‘the possibility that humpback whales can, in theory, create an inﬁnite number of valid songs from the ﬁnite set of discrete units’ (p. 1863). This is in stark contrast to the fact that a single humpback, at any one time in its life, only sings one song (over and over).45 Over the course of many seasons, observing many whales, many different songs would be observed. A ‘theory’ that extrapolated an inﬁnite number of songs from such observations would need some justiﬁcation. We don’t need to leap from cyclical repetitive behaviour all the way to human-like (e.g. Phrase Structure) grammars for humpback song. And humpback whalesong is by no means unique among animal songs in showing autocorrelation. Many of the bird species discussed above exhibit quite strict autocorrelation: the zebra ﬁnch song in Figure 1.3 is a very clear example. Humpback whalesong may be unique in showing autocorrelation at two different levels, but it is likely that this can also be found in nightingale song, if one looks for it.
The autocorrelations discovered by Suzuki et al. are not characteristic of normal human use of language. Mark Liberman, in another perceptive online comment46 has amusingly noted the lack of autocorrelation in ordinary prose by doing an autocorrelation analysis of Suzuki et al.’s own Discussion section, ﬁnding no autocorrelation. Liberman also mentions the autocorrelations that can be found in particularly repetitive songs. A hymn with many verses sung to

45 Two performances of a song with different numbers of repetitions of a phrase still

count as one song. This is the common practice with birdsong researchers, and is also

assumed by these writers on humpback whalesong.

46 This html. This

cwoeublsditebewafos uancdtivaetashtotfp9://J1u5n8e.123000.81.7.5/˜myl/languagelog/archives/002954.

a n i m a l sy n ta x? language as behaviour

71

the same tune is an example of autocorrelation in the musical tune. Somewhat weaker types of autocorrelation are typical of many artistic forms, especially rhymed verse. Here is Robert Louis Stevenson’s epitaph:47
Under the wide and starry sky Dig the grave and let me lie. Glad did I live, and gladly die, And I laid me down with a will.
This be the verse you grave for me: “Here he lies where he longed to be. Home is the sailor, home from the sea, And the hunter home from the hill.”
There are two levels of autocorrelation here, the short-distance sky/lie/die and me/be/sea rhymes, and the longer-distance will/hill rhyme. Within the third line of each stanza, there are further local autocorrelations, glad . . . gladly and home . . . home. The level of autocorrelation here is weaker than it is in the whale’s song, because the poet does not repeat whole portions of the work over and over again. This degree of repetitiveness would be uninformative. Though some repetitive patterning is appreciated in poetry, it should also be informative. The artist strikes a balance. But notice that if this Requiem poem were the only song you ever sing, always producing it word-for-word in this order, sometime stutteringly repeating a word, and you were otherwise mute, your vocal behaviour could be described by a Second-order Markov model. A First-order model would be almost adequate, but not quite, because such frequent words as and, the, me, and he are immediately followed or preceded by different other words. But the previous two words are enough to predict the next word in all cases. This is just about the situation with humpback whale song. The only variation is in the number of phrases repeated in each theme.48
Suzuki et al. (2006) also calculated the amount of information carried by humpback song. Given the variable number of repetitions of a phrase, the next unit in a song is not always entirely predictable, so the song is capable of carrying some information. They calculated that ‘the amount of information carried by the sequence of the units in the song is less than 1 bit per unit’ (p. 1864). Compare this with . . . . . . anything that could ﬁnish this sentence! The number of possible next words in a typical human conversational sentence

47 Stevenson actually wrote ‘home from sea’, which scans better, in the seventh line, but the carvers of his gravestone thought they knew better.
48 For an extremely ingenious demonstration relating to human perception of autocorrelation in ordinary language, see Cutler (1994).

72

the origins of grammar

is immense. Sometimes people say predictable, so strictly uninformative, things, but most of the time they don’t. The extreme redundancy of humpback songs, and indeed birdsong too, means that they are easier to remember. Despite a certain level of complexity, they don’t carry much information. It can be argued that the lack of any detailed semantic plugging-in to the circumstances of the animal’s life is what limits the complexity of these songs. If the songs systematically carried any propositional information about who is doing what to whom, or where is a good place to look for food, they would conceivably gain in complexity. This is a possibility to be taken up in later chapters.

1.3.4 Overt behaviour and neural mechanisms
I have deﬁned and explained two lower ranks on the Formal Language Hierarchy, namely First-order Markov models and State Chain models. In terms of weak generative capacity, almost all animal song belongs at the lowest end of this hierarchy. First-order Markov descriptions are adequate to describe even the most complex wild animal songs, if one dismisses intuitive judgements that they exhibit phrasing. A more powerful model of grammar, Phrase Structure grammar, explicitly reﬂects phrasing in human language. So the question arises whether any further evidence can be found justifying a Phrase Structure approach to at least some animal songs. This follows the theme of recommending that considerations of strong generative capacity be applied to animal songs, no less than in analysis of human language. I will put a case that neuroscientiﬁc ﬁndings may be used to justify a Phrase Structure analysis of some complex bird songs.
As we have seen, the internal structure of songs is often described by bird and whale researchers in terms of middle-sized units, with labels such as ‘phrase’, ‘motif’, ‘theme’, ‘trill’, and ‘ﬂourish’. A very low-level component of songs is the syllable. A syllable itself can sometimes be somewhat complex, as with the chafﬁnch ﬂourish, but researchers concur in labelling it a syllable. Adopting the least powerful descriptive devices available, we have seen that it is possible to describe songs by First-order Markov transitions between syllables, and occasionally, as in the case of the Bengalese ﬁnch, by a State Chain diagram. None of these descriptive methods refer to units bigger than the syllable, such as phrases, trills, or motifs.
Students of zebra ﬁnch, chafﬁnch, and many other species’ songs usually analyse them into phrases, as implied above. And this seems right. The First-order Markov descriptions given earlier for zebra ﬁnch and chafﬁnch songs do not recognize any hierarchical structure. A description of chafﬁnch

a n i m a l sy n ta x? language as behaviour

73

song more ﬁtting the bird researcher’s natural-seeming description would be the Phrase Structure grammar49 below.
SONG → TRILL FLOURISH TRILL → PHRASE1 transition PHRASE2 PHRASE1 → syllable1∗ PHRASE2 → syllable2∗
Read the arrow here as ‘consists of’, so that the ﬁrst rule can be paraphrased as ‘A SONG consists of a TRILL followed by a FLOURISH’. Read the other rules in the same way. The superscript stars in the 3rd and 4th rules indicate optional iteration. For example, the grammar states that a phrase of type ‘PHRASE1’ can consist of any number of instances of syllables of type ‘syllable1’. Likewise the star in the 4th rule allows indeﬁnite iteration of syllables of type ‘syllable2’, constituting a phrase of type ‘PHRASE2’. For some added clarity here (although this is not a general convention of such grammars), the capitalized terms are abstract labels for particular types of recurring stretches of the song (non-terminal symbols), and the lower-case terms are all actual notes of the song (‘terminal symbols’).
Phrase Structure grammar is the third and last rank in the Formal Language Hierarchy that I will describe and deﬁne.
Deﬁnition of Phrase Structure grammars: A Phrase Structure language is one that can be fully described by a Phrase Structure grammar. A Phrase Structure grammar consists of a ﬁnite set of ‘rewrite rules’, each with one abstract, or nonterminal, element on the left-hand side of the arrow, and with any sequence of symbols on the right of the arrow. These latter symbols may be either actual ‘terminal’ elements of the language described (e.g. notes or words), or abstract ‘nonterminal’ symbols labelling phrasal constituents of the language. Such nonterminal symbols are further deﬁned by other rules of the grammar, in which they appear on the left-hand side of the arrow. One nonterminal symbol is designated as the START symbol. In the case of grammars for human languages, this starting symbol can be taken as standing for ‘sentence’, since the grammar operationally deﬁnes the set of possible sentences in the language. In the case of birdsong the start symbol can be taken to stand for ‘song’. A string of words or notes is well-formed according to such a grammar if it can be produced by strictly following the rewrite rules all the way to a string

49 Recall that I depart from the normal terminology of the Formal Language Hierarchy. What I will call ‘Phrase Structure grammars’ here are usually called ‘Context Free grammars’, not a label that brings out their natural way of working.

74

the origins of grammar

of terminal elements. Here, following the rewrite rules can be envisaged as starting with the designated start symbol, then rewriting that as whatever string of symbols can be found in a rewrite rule with that symbol on its left-hand side, and then rewriting that string in turn as another string, replacing each nonterminal symbol in it by a string of symbols found on the left-hand side of some rule deﬁning it. The process stops when a string containing only terminal symbols (actual words of the language or notes of the song) is reached. As a convenient shorthand, the rewrite rules may also use an asterisk (a so-called Kleene star) to indicate that a particular symbol may be rewritten an indeﬁnite number of times. Use of the Kleene star does not affect the weak generative power of Phrase Structure grammars.

Note that the format of Phrase Structure grammars, in terms of what symbols may occur on the right-hand side of a rewrite rule, is more liberal than that for the rewrite format of State Chain languages. This is what makes Phrase Structure grammar more powerful than State Chain grammar. There exist Phrase Structure languages that are not State Chain languages. This result of Formal Language Theory depends on the postulation of inﬁnite languages. Any ﬁnite language requires no grammar other than a list of its sentences, in terms of weak generative capacity.
In terms of weak generative capacity, the little Phrase Structure grammar above describes the stereotypical chafﬁnch song just as well, and just as badly, as the First-order Markov transition table. They both generate the same inﬁnite set of potential songs, so both are observationally adequate. (Neither description captures the numerical range of the iterations inside the two phrases, as discussed earlier, but for simplicity we’ll overlook that point here.) Is there any reason to prefer the Phrase Structure grammar, which may appeal to an undesirably powerful type of mechanism, over the simple transition table, which attributes a less powerful type of mechanism to the bird? I will argue that certain neuroscientiﬁc facts can be interpreted in such a way as to justify a Phrase Structure description as ‘psychologically real’ for the birds in question.
The intuitions of bird researchers about song structure are obviously valuable, and the strategy of rigorously adopting the least powerful descriptive device may deprive us of insights into the bases of behaviour. This conclusion applies very generally to a wide range of animal behaviour. Fentress and Stillwell (1973), for example, studying self-grooming behaviour by mice, found that while the sequences of actions could be somewhat adequately described in a totally linear (First-order Markov) way, a hierarchically organized description was more satisfactory. ‘Even in animals there are sequential rules embedded among other sequential rules’ (Fentress 1992, p. 1533). The limitations of

a n i m a l sy n ta x? language as behaviour

75

the most economical description of behaviour are echoed, with a different emphasis, in Stephen Anderson’s statement, that ‘We cannot assume that the tools we have are sufﬁcient to support a science of the object we wish to study in linguistics’ (Anderson 2008b, p. 75). The most economical description of surface behaviour may not reﬂect the mechanisms underlying that behaviour. For both birdsong and language, sources of extra evidence include studies of the acquisition (of song or language) and neurological studies of brain activity (while singing or speaking).
Williams and Staples (1992) studied the learning of songs by young zebra ﬁnches from older ‘tutors’ ﬁnding that the learning is structured by chunks. ‘Copied chunks had boundaries that fell at consistent locations within the tutor’s song, . . . Young males also tended to break their songs off at the boundaries of the chunks they had copied. Chunks appear to be an intermediate level of hierarchy in song organization and to have both perceptual (syllables were learned as part of a chunk) and motor (song delivery was broken almost exclusively at chunk boundaries) aspects’ (Williams and Staples 1992, p. 278). Cynx (1990, p. 3) found structuring into lower-level units (syllables) in experiments in which the birds were artiﬁcially distracted by bursts of strobe light while singing: ‘Ongoing zebra ﬁnch song can be interrupted, interruptions occur at discrete locations in song, and the locations almost always fall between song syllables’. In this latter case, the units are likely to be inﬂuenced by the bird’s brief in-breaths during song (Suthers and Margoliash 2002; Franz and Goller 2002).
The neuroscience of birdsong is well developed, and reports ample evidence of hierarchically organized management of song production. In a neurological study of birdsong generally, Margoliash (1997, p. 671) writes that ‘neurons in the descending motor pathway (HVc and RA) are organized in a hierarchical arrangement of temporal units of song production, with HVc neurons representing syllables and RA neurons representing notes. The nuclei Uva and NIf, which are afferent to HVC, may help organize syllables into larger units of vocalization’. ‘HVc’ (sometimes HVC) stands for ‘higher vocal centre’, and RA neurons are ‘downstream’ neurons more directly involved in motor output to the syrinx and respiratory system.50 Both syllables and notes are low-level

50 Both HVc and RA are premotor forebrain nuclei. More exactly, HVc is in a telencephalic nucleus in the neostriatum. See Brenowitz et al. (1997, p. 499) for the historic morphing of the referent of HVC or HVc from ‘hyperstriatum ventrale, pars caudale’ to ‘higher vocal centre’. ‘RA’ stands for ‘robustus archistriatum’, and ‘RA is a sexually dimorphic, spherical-to-oval, semi-encapsulated nucleus in the medial part of the arcopallium’ (Wild 2004, p. 443). NIf is the neostriatal nucleus interfacialis. Uva is a thalamic nucleus.

76

the origins of grammar

units in the hierarchical organization of song. A motif, as in zebra ﬁnch song, is a higher-level unit. A study by Fee et al. (2004) associates HVc with control of motifs, and RA with control of syllables. Either way, from syllables to notes or from motifs to syllables, there is hierarchical neural control of the song.
How does hierarchically organized brain control of song relate to the hierarchical assumptions built into such high-level descriptive terms as ‘phrase’ and ‘motif’? Can bird neuroscience resolve an issue of whether a First-order Markov transition description for zebra ﬁnch song, such as I gave on p. 46, is less faithful to the neurological facts than a Phrase Structure description as below?
SONG → INTRO MOTIF∗ INTRO → i∗ MOTIF → A B C D E F G
Here, the superscript star notation represents iteration, paraphraseable as ‘one or more repetitions of’. Both the First-order Markov transition table and the little Phrase Structure grammar capture the facts of zebra ﬁnch song. Doesn’t the neuroscience discovery of separate control of motifs (by HVc) and syllables i, A, B, C, D, E, F, G (by RA neurons) clinch the matter in favour of the Phrase Structure description? After all, the Phrase Structure description identiﬁes a unit, MOTIF, which corresponds to a segment of the song controlled by a particular brain structure, HVc. Well, with some caution, and deﬁnite reservations about the extent of the parallelism claimed, yes, the neural facts seem to support the Phrase Structure version. My claim is based on a study by Fee et al. (2004), following up on an earlier study by the same research team (Hahnloser et al. 2002). The details are fascinating and instructive of how neural research may possibly inform some linguistic descriptions.
Fee and colleagues compared two hypotheses about the role of HVc and RA neurons in the control of zebra ﬁnch song. These hypotheses correspond nicely with our two kinds of description, First-order Markov versus Phrase Structure. One hypothesis was labelled ‘Intrinsic dynamics in RA’. According to this, HVc sends a signal to the particular RA neurons responsible for producing the ﬁrst note of the motif, ‘A’. After that, all the action is between groups of RA neurons, and HVc is not involved until the next motif. Within a motif, according to this hypothesis, the production of ‘A’ by its RA neurons triggers activation of other RA neurons responsible for producing the next note, ‘B’. And production of ‘B’ triggers production, all still within RA, of ‘C’; and so on until the last note of the motif. This is strikingly analogous to the idea of a First-order Markov transition table. One thing leads to another, and no higher control is involved, except to kick off the beginning of the motif. This

a n i m a l sy n ta x? language as behaviour

77

is a familiar type of mechanism in neuroscience, famously criticized by Lashley (1951) as ‘associative chaining’, and appearing in a new guise as ‘synﬁre chains’ (Abeles 1991) as one way of explaining serial behaviour. Despite Lashley’s critique, the idea has not gone away.
The alternative hypothesis was labelled ‘Feedforward activation from HVC’. According to this, putting it informally, HVC (or HVc) has a plan for the serial activation, at 10-millisecond intervals, of all the notes in the motif. An instruction for each separate note is sent from HVC to RA, in sequence. This is analogous to the Phrase Structure rule above deﬁning MOTIF:
MOTIF → A B C D E F G
If the Feedforward activation from HVC hypothesis is correct, it should be possible to detect ﬁrings in HVC timed in lockstep, with a small latency, with the various ﬁrings in RA producing each of the notes in a motif. And, in brief, this is what the researchers found. No need here to go into such feathery and intricate details as the insertion of probes into sleeping birds or the statistical tests used to verify that the HVC ﬁrings were genuinely in lockstep with the RA ﬁrings. ‘. . . the simplest explanation is that burst sequences in RA, during sleep and singing, are driven by direct feedforward input from HVC’ (p. 163).51 The separate functions of RA (individual syllables) and HVC (sequencing of syllables) is also borne out by a study of song learning; Helekar et al. (2003) showed a dissociation between the learning of these two aspects of the zebra ﬁnch song.
So it seems that the higher vocal centre stores information spelling out the sequence of notes in a speciﬁc motif. This conveniently static way of putting it, in terms of stored information, still hides a neural puzzle. What makes the HVC send out the sequence of timed bursts to RA? In their concluding paragraph (2004, p. 168), Fee et al. accept that this question naturally arises, and propose using a similar methodology to investigate the control of HVC by nuclei that project to HVC, such as the nucleus Interface (NIf) or nucleus Uvaeformis (Uva). But isn’t this just a repeated process of shifting the problem ever upstream to a higher brain centre, ﬁrst from RA to HVC, then from HVC to NIf and/or Uva? In one account, NIf sends auditory input to the HVC (Wild 2004, p. 451) and is thus plausibly involved in monitoring

51 Glaze and Troyer (2006) take the ‘clock-like bursting’ in HVC to imply ‘nonhierarchical’ organization of the song. I can’t see that it does. The correlation of hierarchical song patterning with corresponding hierarchical brain control is otherwise generally accepted (see, e.g. Yu and Margoliash 1996). The hierarchicality lies in the undisputed relationship between HVC and RA, not in matters of timing.

78

the origins of grammar

the song through feedback. In another account (Fiete and Seung 2009, p. 3), ‘. . . auditory feedback is not important for sequence generation. Also, lesion studies indicate that input from the higher nucleus NIf to HVC is not necessary for singing in zebra ﬁnches’. What is true for zebra ﬁnches is apparently not true for Bengalese ﬁnches, which have a more complex song. Okanoya (2004, pp. 730–1) reports lesion studies on this species. Unilateral lesions of NIf did not affect the complexity of their song. However, for two birds with complex song, bilateral lesions of NIf reduced the complex song to a much simpler song. A third bilaterally lesioned bird had a rather simple song in the ﬁrst place, as apparently some Bengalese ﬁnches do, and its song was not affected by the lesions. Okanoya concludes that the ‘NIf is responsible for phrase-tophrase transitions’ (p. 730). The phrase-to-phrase transitions are what makes the Bengalese ﬁnch’s song more complex than that of its wild relative, the white-rumped munia.
For relatively simple birdsongs, such as that of the zebra ﬁnch, the neuroscientist’s consensus formulation is that HVC is the main organizer of the song: ‘HVC generates the spatiotemporal premotor drive for sequential motor activation in the form of sequential neural activity. The HVC activity is “abstract”, in the sense that it encodes only temporal ordering, rather than song features’ (Fiete and Seung 2009, p. 2). This way of putting it is somewhat misleading; how can you specify a temporal ordering of elements without somehow referring to what those elements are? But certainly the information that HVC sends to RA about the detailed features of the notes that it orders is coded in a sparse form. The triggers from HVC to RA can perhaps be thought of as ‘abstract’ concise labels for notes: on receiving a particular ‘label’, RA starts to ﬁll in all the appropriate detailed articulatory information to be sent to the syringeal and respiratory muscles. An earlier study (Vu et al. 1994) showed that electrical stimulation of RA in zebra ﬁnches distorted individual syllables but did not change the order or timing of syllables, but stimulating HVC did alter the overall song pattern. At the higher ‘abstract’ level of HVC, it remains a possibility that the timed sequence of instructions is implemented by a synﬁre chain.
The coding of information about particular song types in HVC is sparse. That is, the instructions sent to RA ultimately causing particular notes to be sung are provided by ensembles of rather few specialized neurons; and each ensemble responsible for sending a particular instruction to RA for a particular note is only active for a small proportion of the total duration of the song. Fee et al. (2004) suggest that this sparse coding could have advantages for songlearning. The ensembles of HVC-to-RA neurons correspond in my linguist’s interpretation with the symbols after the arrow in a Phrase Structure rule

a n i m a l sy n ta x? language as behaviour

79

deﬁning the sequence of notes in a motif. During learning, when the bird is struggling to match its behaviour with a song template acquired months earlier, individual mismatches between behaviour and template at any point in the sequence can be ﬁxed more easily if the responsible ensembles are relatively isolated from each other by being sparsely coded. This is putting it very informally, but I hope it helps non-neuroscientists (like me) to get some grip on the advantage of what Fiete and Seung (2009), above, called the ‘abstract’ nature of the song representation in HVC. The key point is that an animal that learns its song, rather than just having it completely innately speciﬁed, needs much more complex upstream structuring in its neural apparatus. If the song is wholly innate, there need be no connection between hearing and production. The animal just sings. For learning, there has, of course, to be machinery for comparing stimuli, or the templates acquired from stimuli, with feedback from the animal’s own faltering attempts to get it right, plus machinery for gradually adjusting these faltering steps in the right direction. This kind of advantage of more powerful grammar types for learning is seldom discussed in the formal language literature.
If a bird has several motifs in its repertoire, details of each of these are presumably stored in its HVC. Female canaries and zebra ﬁnches have little or no song repertoire, and females have markedly smaller vocal control centres, HVC, RA, and ‘Area X’, than males (Nottebohm and Arnold 1976). In species whose females are not so mute, typically from tropical regions, there is also less sexual dimorphism in the vocal control centres (Brenowitz et al. 1985; Brenowitz and Arnold 1986). Besides this striking sexual dimorphism correlated with singing behaviour, there is a correlation across songbird species. Fee et al. report that ‘across many species of songbirds, total repertoire size is correlated with HVC volume’ (2004, pp. 167–8). This is corroborated by DeVoogd (2004, p. 778): ‘Across a group of 41 very diverse species, the relative volume of HVC was positively correlated with the number of different songs typically produced by males in the species’. Pfaff et al. (2007) established a three-way correlation between size of song repertoire, volume of HVC and body quality, as measured by various physiological and genetic properties, in song sparrows. Finally, Airey and DeVoogd (2000) found a correlation between the typical length of a phrase in a song and HVC volume in zebra ﬁnches. All this is consistent with the idea of HVC as a store for abstract song templates.
So far, I have only mentioned the machinery involved in song production. A central tenet of generative linguistics is that the object of interest is a speaker’s tacit knowledge of his language, the declarative store of information upon which performance in speaking and interpreting speech is based. A linguist’s formal description of a language aims to be neutral with respect to production

80

the origins of grammar

or perception, just as a geographical map is neutral with respect to eastward or westward travel—the information in the map can be used to go in either direction. A linguist’s Phrase Structure rule is not relevant only to production, but also to perception. In birdsong neuroscience, a well-established ﬁnding is that individual neurons in HVC respond selectively to playback of a bird’s own song. See, for example, Theunissen and Doupe (1998) and Mooney (2000), studying zebra ﬁnches. A zebra ﬁnch only has a repertoire of one song. In species with more than one song, there is evidence that HVC is also a centre where different individual neurons are responsive to different songs played back from a bird’s repertoire. Mooney et al. (2001) investigated swamp sparrows by playing back recordings of their songs to them. Swamp sparrows have small repertoires of between two and ﬁve song types. The main ﬁnding was that ‘single HVc relay neurons often generate action potentials to playback of only a single song type’ (p. 12778). This indicates a role for HVC in song perception as well as song production. This work was followed up by Prather et al. (2008), still working with swamp sparrows. These researchers discovered a vocal-auditory analogue of mirror neurons, as found in the macaque brain (Rizzolatti et al. 2001; Gallese et al. 1996). Mirror neurons have been widely regarded as providing a basis for imitative action. In the swamp sparrows, neurons projecting from HVC to Area X responded robustly to speciﬁc song playback.

In a substantial proportion of responsive HVCX neurons (16 of 21 cells), auditory activity was selectively evoked by acoustic presentation of only one song type in the bird’s repertoire, deﬁned as the ‘primary song type’, and not by other swamp sparrow songs chosen at random. The primary song type varied among cells from the same bird, as expected given that each bird produces several song types.
(Prather et al. 2008, p. 305)

The same neurons ﬁred when hearing a song as did when singing that song. Interestingly, this HVCX response was switched off while the bird was actively singing, so the bird does not confuse feedback of its own song with song from other birds (or experimenters’ loudspeakers). ‘HVCX cells are gated to exist in purely auditory or motor states’ (Prather et al. 2008, p. 308). These versatile cells, then, can be taken as part of a declarative system that represents particular song types in the swamp sparrow’s brain, usable as the occasion demands for either production or recognition of a song.52

52 Tchernichovski and Wallman (2008) provide a less technical summary of these ﬁndings.

a n i m a l sy n ta x? language as behaviour

81

We need to beware of naive localization. Of course, HVC is not the only place in a bird’s brain where it can be said that song is represented. ‘HVC by itelf does not learn or produce a song. It is part of sensory and motor circuits that contain many brain regions, and it is the connectivity and interaction between these components that determines outcome’ (DeVoogd 2004, p. 778).
While the evidence about HVC and RA neurons indicates something parallel to a Phrase Structure rule, it would be wrong to think of the bird brain as storing a truly generative grammar of its repertoire. The essence of generative syntax is the capacity to ‘make inﬁnite use of ﬁnite means’, by taking advantage of many combinatorial possibilities. In a generative grammar, this is typically achieved by each type of constituent (e.g. a noun phrase) being deﬁned just once, and the deﬁnition being re-used in the many different contexts in which it is called by other rules of the grammar. In this way, many thousands (perhaps even an inﬁnite number) of sentences can be generated by a set of only tens of rules.53 In humans, it is in the lexicon that storage matches data, taking little or no advantage of combinatorial possibilities. The lexicon (passing over some complexities) is basically memorized item by item. The correlation between repertoire size and HVC volume in birds indicates that they memorize each item, even though it seems appropriate to describe their production in terms of Phrase Structure rules. Birds whose songs are naturally analysed into phrases store sets of unconnected Phrase Structure rules. In this sense, the bird’s store of motifs is like a list of simple constructions, each deﬁned by a particular Phrase Structure rule. We shall see in a later chapter that a recent grammatical theory, Construction Grammar, claims that human knowledge of grammar also takes the form of a store of constructions, described, in essence, by complex Phrase-Structure-like rules. Human constructions are far more complex than bird phrases, and combine with each other in far more productive ways, but the parallel is noteworthy. Complex birdsong is hierarchically organized in ways parallel to human composition of simple phrases, and thus shows signs of human-like syntax. But the full combinatorial possibilities of Phrase Structure grammar are deﬁnitely not exploited by birds. In fact, though most constructions in human languages are quite well described by Phrase Structure grammars or their equivalent, the full range of theoretically possible Phrase Structure grammars is also not exploited by humans. One can write fanciful descriptions of made-up languages using Phrase Structure rules alone, but

53 This is a Mickey-Mouse example. No one knows how many grammatical rules human speakers have in their heads. But the principle stands, that whatever they store in their heads generates, through use of combinatoriality, vastly more diverse data than they could possibly memorize explicitly.

82

the origins of grammar

nothing like these languages is found in real human populations. We will see an example in the next subsection.
I interpreted the function of HVC to store a sequence of instructions to RA as parallel to the Phrase Structure rule MOTIF → A B C D E F G. As mentioned above, two other nuclei, Uva and NIf are afferent to HVC (or HVc), and may help organize units larger than that organized in HVC. Here the interpretation of brain structure and activity as Phrase Structure rules becomes problematic. Are we to expect that for each successive higher layer in a hierarchical description there will be a separate higher brain nucleus, sending signals downstream to lower-level nuclei? This is certainly the implication of Okanoya (2004), reporting on the responsibility of NIf for higher-level phraseto-phrase transitions in Bengalese ﬁnches. Maybe the lack of much depth to the hierarchical structure of birdsong can be accounted for by the lack of higherlevel nuclei, ultimately attributable to lack of sufﬁcient brain space. Human phrase structure can get very deep, with many layers of structure, from words, through various types of phrases, then via subordinate clauses to the level of the main clause, the whole sentence. There is no evidence for hierarchical stacks of nuclei in the human cortex, or elsewhere, corresponding to the various levels of phrase structure.
It is implausible that the phrase structure of human sentences is implemented in the same way (mutatis mutandis) as the zebra ﬁnch brain implements the structure of motifs as series of notes. At least one fatal objection is that the bird’s HVC-to-RA instruction is associated with very speciﬁc timing, in milliseconds. ‘Remarkably, during directed singing, syllable duration is regulated to around 1 ms or a variation of <1%’ (Suthers and Margoliash 2002, p. 687).54 Such precise timing is not a feature of human sentential syntax. One can speak fast or slowly, one may hesitate in mid-phrase and still pick up the thread. Precisely timed serial behaviour is more typical of human phonetics and phonology. In a recent paper, examining perception rather than production, Pulvermüller and Shtyrov (2009, p. 79) ‘found that acoustic signals perceived as speech elicited a well-deﬁned [precisely timed] spatiotemporal pattern of sequential activation of superiortemporal and inferiorfrontal cortex’; this work, however, related to the perception of a single, low-level unit, a [t] sound. It is possible that a repertoire of syllables, or other small phonotactic units, in a language is stored as a set of phonological constructions each represented in a way parallel to a Phrase Structure rule. Pulvermüller (2002, pp. 150–4) discusses the possibility of synﬁre chains underlying phonological

54 Glaze and Troyer (2006) conﬁrmed this impressively precise timing, but found slight but signiﬁcant variations in the global tempo of the song related to time of day and the arousal state of the bird.

a n i m a l sy n ta x? language as behaviour

83

behaviour, such as the organization of syllables into sequences of phonemes, whose relative timing is more narrowly constrained than the timing of phrases in a sentence. However, even here, Pulvermüller admits the problem of fast and slow speech. He also, in a following section, sets out clearly ‘at least ﬁve reasons’ why such chain mechanisms are inappropriate as direct explanations of the production of higher-order sequences of meaningful units, morphemes, and words (2002, pp. 154–6).
The zebra ﬁnch’s HVC ﬁres off a sequence of instructions, A B C D E, at precisely timed intervals, dictating the shape and timing of a motif. Not all repeated items in bird and whale songs are as tightly time-controlled as the elements in a zebra ﬁnch motif. For example, the trill part of a chafﬁnch song varies in length, so the instruction to execute the following ﬂourish part cannot be a constant time after the instruction to execute the trill. In nightingale song, the Alpha, Beta, Gamma, and Omega parts are always in this order, so there is likely to be something in its brain determining this sequence, analogous to a Phrase Structure rule. But these song-parts are of variable length, especially the Gamma part, due to optional repetitions of some notes or pairs of notes. Here again, the bird’s mental rule determining the sequencing is at least somewhat free from strict timing. The best such example is in humpback whalesong. As we have seen, a whale’s song consists of a ﬁxed sequence of themes, always A B C D E F, each of which is internally complex and of variable length, due to optional repetitions. The whale’s brain must have a mechanism for specifying the serial order relatively free from timing information. All these animals have evolved ways of representing the serial order of elements in a higher-level component of their song, relatively free of precise timing information. We don’t know how they achieve this trick,55 but it is a signiﬁcant step toward the much fuller syntactic competence of humans. Consistent serial order of subparts of a construction is a hallmark of human syntax; without it syntax as we know it is inconceivable. Our ancestors achieved the same trick of controlling serial order unconstrained by tight timing as the whales and nightingales, either before or in parallel with the systematic assignment of meanings to the ordered elements.
This discussion brings out a feature of the Formal Language Hierarchy that is possibly disguised by formulating it as a hierarchy of systems for generating successively inclusive sets of languages. It is possible to be led to believe that if a stringset cannot be generated by State Chain diagram, then no chainlike mechanism at all can be involved in generating it. This is an error. The crucial difference in descriptive devices, as one proceeds up the hierarchy, is in

55 Imagine doing neuroscience on live whales!

84

the origins of grammar

degree of abstraction. First-order Markov transition tables describe languages in terms of chains of their terminal elements; and State Chain diagrams describe languages in terms of chains of slightly more abstract entities, the internal states of the organism or machine. A State Chain diagram is a single transition network connecting states, with terminal elements of the language emitted (or accepted) in the transitions between states. It is possible to have not just one, but a set of such basic networks, with a higher-level network connecting them, that is a network of networks, or a chain of subchains. This step up in abstraction corresponds to a generating (or accepting) device known as a Recursive Transition Network (Woods 1970). Recursive56 Transition Networks (RTNs) were proved, in the early literature of the Formal Language Hierarchy, to be capable of generating (or accepting) exactly the class of Phrase Structure languages.
A ‘natural’ description of an animal song recognizes hierarchical phrasal organization where it seems to exist, even though a description in terms of a less powerful grammar type (e.g. State Chain or First-order Markov table) may be strictly possible. The more complex animal songs are naturally described by Phrase Structure grammars, equivalently by RTNs. But no animal song, not even a humpback whale’s, has hierarchical structure with a depth greater than 2. An animal with a large repertoire, such as a nightingale, may have (the neural equivalent of) a large number of separate RTNs stored, one for each song, but none are of a greater depth than 2. Also, no natural description of any animal song requires an RTN (or Phrase Structure grammar) incorporating genuine recursion, that is reference to a subnet labelled ‘X’ within a subnet labelled ‘X’.
The job of accounting for a bird species’ song capacity involves more than just production and recognition. An RTN is not a model of how the song repertoire is acquired. The focus of the Formal Language Hierarchy is on mechanisms for producing and recognizing strings from speciﬁed stringsets, assuming they have somehow been acquired. Whatever level of the hierarchy turns out to be appropriate to generate sentences of human languages, the architecture of models at that level does not give any direct insight into the crucial question of how the grammar of the language was learned. If all human

56 Beware the slippery word ‘recursive’. It is used in a loose variety of senses in the literature. The systems known as Recursive Transition Networks can implement ‘true’ recursion (embedding a procedure of type X within a procedure of type X), but do not necessarily do so. It is possible that a bird’s brain implements something like a simple Recursive Transition Network, but highly unlikely that the bird has any capacity for recursion in the strict sense of using one procedure while simultaneously using another instantiation of that same procedure ‘inside itself’. No proper recursion is evident in birdsong or whalesong.

a n i m a l sy n ta x? language as behaviour

85

languages can be generated with Phrase Structure rules, the bare theory of Phrase Structure is not in itself a model of the human language capacity, because it does not address the question of how Phrase Structure grammars can be acquired on exposure to relevant experience. Tunable devices, such as connectionist neural nets, can be trained to accept languages at certain levels of the Formal Language Hierarchy. There is, however, no well-developed theory of the different neural net architectures necessary for acquiring languages of different degrees of complexity.
In many cases, a straightforward correlation holds between rank on the formal language hierarchy and processing effort or complexity. For example, parsing strings that can only be generated by a Phrase Structure grammar typically requires more computational effort than parsing strings generated by a State Chain process. There are exceptions to this correlation, however. A language with indeﬁnitely many ‘cross-serial dependencies’ occupies a yet higher rank in the Formal Language Hierarchy than Phrase Structure languages. I will not discuss these cases, except to note that Gibson (1998) shows ‘the lower complexity of cross-serial dependencies relative to center-embedded dependencies’ (p. 1). Gibson is here referring to complexity of processing, as measured by reasonable criteria of load on memory. Languages with crossserial dependencies (of indeﬁnite length) cannot be generated by a Phrase Structure grammar, whereas languages with centre-embedded dependencies can be. Centre-embedded dependencies are in fact also harder for the real human parser to process. This is an area in which rank on the formal language hierarchy does not neatly correlate with processing complexity, undermining its potential usefulness in theorizing about classes of grammars that humans, or any animals, can manage. See Chapter 3 for further discussion.

1.3.5 Training animals on syntactic ‘languages’
A study by two eminent primate researchers (Fitch and Hauser 2004) certainly did take the Formal Language Hierarchy seriously. They claimed to show, in the spirit of the scala naturae that I have linked to the hierarchy, that tamarin monkeys lack the computational capacity to process languages at the Phrase Structure level, while they are capable of processing lower-level State Chain languages. Humans, in contrast, and unsurprisingly, were shown to be able to process languages of both classes. The experiments involve two speciﬁc ‘languages’ well known in the literature of the Formal Language Hierarchy, called AnBn and (AB)n.
In the original literature, both AnBn and (AB)n are inﬁnite languages (stringsets). AnBn is the name given to a language whose sentences consist

