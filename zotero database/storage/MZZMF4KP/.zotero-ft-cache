See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/6632593
Metaphor explanation attenuates the righthand preference for depictive co-speech gestures that imitate actions
ARTICLE in BRAIN AND LANGUAGE · JULY 2007
Impact Factor: 3.22 · DOI: 10.1016/j.bandl.2006.11.006 · Source: PubMed

CITATIONS
18
3 AUTHORS, INCLUDING: Christine Mohr University of Lausanne 105 PUBLICATIONS 2,052 CITATIONS
SEE PROFILE

READS
30

All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

Available from: Christine Mohr Retrieved on: 03 February 2016

Brain and Language 101 (2007) 185–197

www.elsevier.com/locate/b&l

Metaphor explanation attenuates the right-hand preference for depictive co-speech gestures that imitate actions ଝ
Sotaro Kita a,¤, Olivier de Condappa b, Christine Mohr b
a School of Psychology, University of Birmingham, Birmingham B15 2TT, UK b Department of Experimental Psychology, University of Bristol, Bristol, UK
Accepted 7 November 2006 Available online 12 December 2006

Abstract
DiVerential activation levels of the two hemispheres due to hemispheric specialization for various linguistic processes might determine hand choice for co-speech gestures. To test this hypothesis, we compared hand choices for gesturing in 20 healthy right-handed participants during explanation of metaphorical vs. non-metaphorical meanings, on the assumption that metaphor explanation enhances the right hemisphere contribution to speech production. Hand choices were analyzed separately for: depictive gestures that imitate action (“character viewpoint gestures,” [McNeill, D. (1992). Hand and mind. What gestures reveal about thought. Chicago: University of Chicago Press.]), depictive gestures that express motion, relative locations, and shape (“observer viewpoint gestures”), and “abstract deictic gestures.” It was found that the right-hand over left-hand preference was signiWcantly weaker in the metaphor condition than in the non-metaphor conditions for depictive gestures that imitated action. Findings suggest that the activation of the right hemisphere in the metaphor condition reduces the likelihood of left hemisphere generation of gestures that imitate action, thus attenuating the right-hand preference. © 2006 Elsevier Inc. All right reserved.
Keywords: Hemisphere; Laterality; Hand use; Metaphor; Language; Broca’s area

1. Introduction
When we speak, we spontaneously produce hand gestures that are semantically related to the concurrent speech. We use the right hand or the left hand or both hands for producing gestures (henceforth we use the term “gesture” to refer to spontaneous co-speech gestures). However, various factors inXuence which hand(s) is (are) chosen. For instance, the dominant hand is preferentially used for gesturing, especially among right-handers (Dalby, Gibson, Grossi, & Schneider, 1980; Foundas et al., 1995; Kimura, 1973a, 1973b; Lavergne & Kimura, 1987; Sousa-Poza, Rohrberg, & Mercure, 1979; Stephens, 1983). Properties of
ଝ This project was supported by a research grant from British Academy (SG-41169). We thank Alex Lievesley for her help in assessing intercoder reliability.
* Corresponding author. E-mail address: s.kita@bham.ac.uk (S. Kita).
0093-934X/$ - see front matter © 2006 Elsevier Inc. All right reserved. doi:10.1016/j.bandl.2006.11.006

the event depicted by gestures can also inXuence hand choice. For example, the right hand tends to depict the object on the right side and the left hand the one on the left side when gestures represent two horizontally moving objects (Lausberg & Kita, 2003). Finally, hemispheric language dominance inXuences hand choice for gesturing. Right-handed (Kimura, 1973a) and non-right-handed (Kimura, 1973b) individuals with left hemisphere (LH) language dominance (right-ear advantage in a dichotic listening task) revealed a stronger right-hand preference for gesturing than those with a left-ear advantage. Kimura’s observations lead to the idea that gestures and speech are produced by a common brain system in the LH in most right-handers (Kimura, 1973a).
If the LH’s language dominance is causing the righthand preference in gesturing, then one could also hypothesize that this right-hand preference should be attenuated and left-handed gestures should be more frequent when linguistic processing engages right-hemispheric language

186

S. Kita et al. / Brain and Language 101 (2007) 185–197

skills. The right hemisphere (RH) has been associated with language skills such as metaphor and humour processing, emotional prosody (see Taylor & Regard, 2003; Van Lancker, 1997 for overviews) and metalinguistic tasks such as foreign accent detection (Berman, Mandelkern, Phan, & Zaidel, 2003). The processing of conversational pragmatics also involves the RH (as well as the LH) (Kasher, Batori, Soroker, Graves, & Zaidel, 1999). When considering these lines of research Wndings, two diVerent hypotheses on the role of the two hemispheres in gesture production can be formulated. First, a common system in the LH provides the exclusive channel of information into gesture production processes. In other words, only the LH can generate underlying semantic representations of gestures. Second, both hemispheres can generate underlying semantic representations of gestures, and relative activation levels of the two hemispheres in a given speaking task inXuence which hemisphere dominates in the generation of gestural representation, and consequently which hand is more likely to produce gestures.
Lavergne and Kimura (1987) provided evidence for the Wrst hypothesis. They investigated whether right-handers would change their right-hand preference for gesturing as a function of speech topic. More speciWcally, the authors compared hand preferences when talking about spatial topics (e.g., description of routes on a university campus), verbal topics (deWnition and use of abstract words such as “faith” and verbatim description of recent phone conversation), and neutral topics (e.g., a typical school-day routine). Given the assumption that the RH is dominant for spatial processing (e.g., Bisiach & Luzzatti, 1978; Guariglia, Piccardi, Iaria, Nico, & Pizzamiglio, 2005), and that hand choices for gesturing are inXuenced by relative activation levels of the two hemispheres, fewer right-hand gestures (and more left-hand gestures) should be produced when talking about spatial topics than verbal or neutral topics. However, the authors found that the degree of right-hand preference was comparable between spatial and non-spatial topics. The authors consequently concluded that all information “destined for speech output is similarly channelled through a left-hemisphere-based motor control system” to generate underlying representations for both speech and gesture (Lavergne & Kimura, 1987, p. 692). This view is consistent with a theory of speech-gesture production stating that gestures are generated from one of the subprocesses of speech production. For example, gestures may be generated from the semantic content of words after they were selected for speech production (Butterworth & Hadar, 1989).
The recent Wnding by Miller and Franz (2005), however, may challenge Laverne and Kimura’s position. These authors observed that the proportion of bimanual gestures was higher when talking about spatial topics (e.g., layout of a house) as compared to a neutral topic (typical university day routine). Though the authors did not attribute their Wnding to hemispheric specialization, their result would support the idea that the RH was engaged in gesture pro-

duction when talking about a spatial topic, resulting in an increase in bimanual gestures. Such an interpretation supports a view contra Lavergne and Kimura’s. That is, both right and left hemispheres can generate underlying semantic representations for gestures, and the relative activation levels of the two hemispheres are reXected in the likelihoods with which the two hemispheres generate gestural representations. Such a view is consistent with theories of speechgesture production, stating that the underlying semantic representation for gestures is generated at conceptual processes that are coordinated with, but not a proper part of, speech production processes (e.g., Hadar & Yadlin-Gedassy, 1994; Kita & Özyürek, 2003; Krauss, Chen, & Gottesman, 2000; McNeill, 1992).
There are two caveats with respect to the two previous studies relevant to the two competing hypotheses (Lavergne & Kimura, 1987; Miller & Franz, 2005). First, these studies did not distinguish diVerent gesture types. It is possible that speech topics inXuence the hand preference only for certain types of gestures. This possibility is plausible since previous studies have shown that the hand preference diVers between various types of gestures. Sousa-Poza et al. (1979) found that right-handers preferred their dominant hand for “representational gestures” (depicting objects, spatial relationships, sensations, or emotions), but they showed no speciWc hand preference for “non-representational gestures” (accentuating what is being said). Stephens (1983) found that right-handers produced “iconic gestures” (depicting physical objects, situations, and events) more often with the right than left hand. She additionally reported that the right-hand preference was stronger for both iconic gestures and “metaphorical gestures” (depicting abstract concepts as a concrete action or movement) as compared to “beat” gestures (small bi-directional movements often in synchrony with the speech rhythm). Finally, hand preference studies with split-brain patients indicated that the two hemispheres may contribute diVerently to the production of diVerent gesture types (Lausberg, Davis, & Rothenhäuser, 2000; McNeill & Pedelty, 1995).
In the current study, we examine the hand preference for “depictive gestures,” which subsumes Stephens’s (1983) and McNeill’s (1992) iconic and metaphoric gestures. We created this category since we aim to compare gesture production between speaking tasks with diVerent degrees of abstractness of referents. McNeill (1992) diVerentiated two types of iconic gestures, which we will apply to our category of depictive gestures. The Wrst is “iconic gestures with the observer viewpoint,” in which a gesturing hand stands for an entity (e.g., an object or an animate character) and the hand movement depicts the shape, relative location, or movement of the entity. The second type is “iconic gestures with the character viewpoint,” in which a gesturing hand stands for the hand of an animate character and the hand movement enacts the action by the character. As will be detailed in the method section, the viewpoints can also be distinguished when gestures metaphorically refer to abstract referents. This distinction is important because

S. Kita et al. / Brain and Language 101 (2007) 185–197

187

there are reasons to suspect that the hand choice for character viewpoint gestures may diVer from that for observer viewpoint gestures. For example, the hand choice for character viewpoint gestures, unlike that for observer viewpoint gestures, may largely be determined by the handedness of the speaker because these gestures often enact hand actions typically performed by the dominant hand. Thus, unlike previous studies, the current study distinguishes depictive gestures with these two diVerent viewpoints. In addition, we investigated the hand preference for abstract deictic gestures (McNeill, Cassell, & Levy, 1993), which indicate locations in seemingly empty space in front of the speaker’s body.
The second caveat of the crucial previous two studies (Lavergne & Kimura, 1987, Miller & Franz, 2005) is that the content of speech and, by inference, the content of gestures diVered substantially between the spatial and nonspatial conditions. Properties of what is depicted by gestures can inXuence hand choice (Lausberg & Kita, 2003). Thus, diVerences in speech contents might have obscured the eVect of RH activation on the right- over left-hand preference for gesturing in Lavergne and Kimura’s study. Also, it is likely that spatial topics (e.g., layout of a house) elicited more bimanual gestures in Miller and Franz’s study because participants had to express spatial relationships in 3D space (two sides of a room, the spatial relationship between two rooms). Gestures representing locations in 3D space are, on mere logical grounds, represented most conveniently with bimanual gestures. Thus, we conjecture that the increase in bimanual gestures when talking about spatial topics might not have been due to an overproportional activation of the RH.
To overcome these limitations of the previous studies, we here controlled the contents of speech in diVerent conditions more strictly. Instead of spatial topics, we asked righthanded participants to explain metaphorical expressions, which have been shown to engage the RH. For instance, patients with RH lesions performed worse in metaphor comprehension tasks than did patients with LH lesions (Brownell, Potter, Michelow, & Gardner, 1984; Rinaldi, Marangolo, & Baldassarri, 2004; Winner & Gardner, 1977). The studies with healthy participants, either using functional brain imaging (Bottini et al., 1994; Sotillo et al., 2005, but see Rapp, Leube, Erb, Grodd, & Kircher, 2004) or visual half-Weld paradigms (Anaki, Faust, & Kravetz, 1998), also implicate the RH in metaphor processing.
In the crucial condition of the current study (the metaphor condition), participants were instructed to state the meaning of metaphorical phrases such as “spill the beans” and the mapping between the literal meaning and the metaphorical meaning, which ensures that participants engage in metaphorical thinking (rather than simply retrieving a conventional meaning associated with a phrase). Since participants referred to both the literal and abstract meanings of stimulus phrases in the metaphor condition (as conWrmed in a pilot experiment), we further controlled for speech content by adding two conditions. Namely, partici-

pants explained variations of the metaphoric phrase (e.g., “spill the beans”) that refer only to concrete events (e.g. “spill the marbles,” the concrete condition) and that capture some aspects of the abstract meaning of the metaphoric phrases (e.g. “disclose something conWdential,” the abstract condition). Thus, the experiment has three conditions: (1) metaphor, (2) concrete, and (3) abstract.
In three analyses, we compared the hand choice for gesturing (left, right, or bimanual) between the above three linguistic conditions. The Wrst analysis assessed the preference for the right hand over the left hand. If both hemispheres can generate gestures, and relative activation levels of the two hemispheres inXuence the hand choice for gesturing, then the proportion of left-hand (as compared to righthand) gestures should increase in the metaphor condition than in the concrete and abstract conditions. The second analysis targeted the possibility that a stronger engagement of the RH in the metaphor condition might be reXected not only by an increase in left-hand gestures, but also by an increase in bimanual gestures as opposed to right-hand gestures. In order to test for this possibility, we compared the preference for bimanual gestures over right-hand gestures between the three linguistic conditions. In the third analysis, we assessed the preference for bimanual gestures over unimanual gestures. This analysis was motivated by Miller and Franz’s (2005) Wnding that the proportion of bimanual symmetrical gestures were higher when people talked about spatial topics as compared to when they talked about a non-spatial topic.1 Though the current study did not distinguish symmetrical and asymmetrical bimanual gestures, the prediction based on Miller and Franz’s Wnding would be a stronger bimanual preference in the metaphor condition as compared to the abstract and concrete conditions.
Furthermore, the above analyses were carried out separately for diVerent gesture types, and the coding scheme for gesture types extended McNeill’s (1992) gesture classiWcation. Crucially, it distinguished depictive gestures with the character and observer viewpoints. Fine-grained coding of gesture types allowed us to assess whether diVerent types of gestures equally showed sensitivity to the hemispheric specialization in linguistic processing.
As Miller and Franz (2005) showed that spatial vs. nonspatial topics inXuence handedness of gestures, we assessed the density of spatial contents in each response. To this end, we identiWed all spatial prepositions in the verbal responses, and calculated the proportion of spatial prepositions (e.g., above, along, at, down, from, in, oV, on, over, round, through, up)2 among all words as an index of density of spatial contents. We chose this index because presence or
1 The proportion of asymmetrical bimanual gestures did not diVer between the topics either, but the authors do not provide explanations as to why only symmetrical bimanual gestures reacted to the manipulation of topics.
2 Some of these words can be used as verb particles (as in “you fall down”). For the purpose of this study, “spatial prepositions” include such verb particles uses.

188

S. Kita et al. / Brain and Language 101 (2007) 185–197

absence of spatial prepositions in a phrase has successfully been used to predict gesture behaviours (more speciWcally, gesture rates) in a previous study (Rauscher, Krauss, & Chen, 1996). We expect that density of spatial contents is highest in the concrete condition. Thus, any gestural reactions to spatial contents should be most prominent in that condition.
2. Method

include an explanation as to how the literal meaning can be mapped on to the metaphoric meaning of the expression (e.g., In ‘to spill the beans,’ ‘beans’ refer to secrets and ‘spilling’ refers to spreading them to everybody). The three linguistic conditions were presented sequentially in blocks, with the block order was counterbalanced across participants. The order of stimuli within blocks was reversed for half of the participants. In each condition, two practice trials preceded the main trials.

2.1. Participants

2.4. Speech coding

20 (9 men, 11 women) native English speakers (age: M D 24.7 years, SD D 7.85) participated in the study. They were all right-handed according to a standardized 10-item handedness questionnaire (OldWeld, 1971): each right-hand preference was given a score of “1,” each either-hand preference “0.5,” and each left-hand preference “0.” We calculated the mean of the sum of these scores, and deWned as right-handed those participants who scored 7 7.5 (M D 9.1). As 95.5–99.7% of right-handers are reported to have left-hemispheric language dominance (Bryden, 1982; Levy & Gur, 1980), we assume the same for our population. From our original sample (N D 26), we excluded six: one participant had the freedom of one hand compromised (holding an object), two participants did not follow the instructions for the linguistic tasks properly, and three participants produced no gestures.
2.2. Stimuli
We created 12 stimuli each for the metaphoric, concrete and abstract conditions (see Appendix A). The stimuli for the metaphoric condition were idiomatic expressions with metaphorical meanings, such as “to spill the beans.” The stimuli for the concrete condition and abstract condition were derived from the metaphoric stimuli, but were altered so as not to evoke any metaphoric thought. The literal stimuli referred to a physical event that was similar to the literal meaning of the corresponding metaphorical expression (“to spill the marbles”), whereas the abstract stimuli referred to an abstract feature of the corresponding metaphor’s meaning (“to disclose something conWdential”).
2.3. Procedure
Participants were tested individually. They were seated on a chair. The experimenter faced the participant, and the video camera recording participants’ responses (PAL DV camera at 25 frames per second) was placed next to the experimenter. Stimuli were presented (Microsoft PowerPoint) one by one on a computer screen.
Participants were instructed to explain the meaning of various written English expressions as if they were explaining it to a non-native English speaker. The instruction did not mention gestures. To encourage metaphorical thinking in the metaphor condition, participants were instructed to

The verbal responses in the metaphor condition were coded for whether they include an explicit link between the literal and metaphoric meanings. In order to establish the inter-coder reliability, a second independent coder doublecoded one response (8.3% of all responses) per participant. The codes from the two coders matched 90% of the time (Cohen’s D .692, p D .001).
2.5. Gesture coding
The video recordings were converted into the MPEG1 format. All gestures produced by participants were segmented and coded using a software (ELAN) developed by the Max Planck Institute for Psycholinguistics (Nijmegen, the Netherlands). ELAN allows frame-by-frame video analysis, and thus gesture coding based on both visual and audio information. First, we identiWed and segmented stroke phases (Kendon, 1980; McNeill, 1992) of gestural hand movements in accordance with the procedure outlined in Kita, van Gijn, and van der Hulst (1998). The stroke phase involves a pronounced level of eVort in execution, and it carries the meaning of the gesture. Gesture strokes were categorised into the following types, which is an expanded version of McNeill’s (1992) classiWcation system.
2.5.1. Depictive gesture with the character viewpoint The action performed by the gesturer’s hand(s) iconi-
cally represents the one performed by an animate character (e.g., in a story). For example, repeated downward movements of a Wst may represent somebody pounding on something with a Wst. While this type of gesture is typically discussed in relation to concrete physical actions, abstract concepts can also be the meaning of such gestures (e.g., holding/having an idea). In such instances, an abstract concept is physically held or manipulated by the gesturer’s hand(s) as if it was a physical entity (Fig. 1a).
2.5.2. Depictive gesture with the observer viewpoint The gesturing hand(s) represent a physical object or
entity, and the movement of the hand(s) represents the movement of the object or entity. For example, a downward movement of a hand may represent falling of an object. The gesture creates the impression that the gesturer is witnessing the movement as an observer of the event or

S. Kita et al. / Brain and Language 101 (2007) 185–197

189

Fig. 1. Depictive gestures with the character and observer viewpoints that express abstract concepts.

situation. This type of gesture can represent both concrete and abstract entities (Fig. 1b). The gesture can depict the shape, motion, direction or existence of an entity.
2.5.3. Abstract deictic gestures The gesture indicates the location or direction of a spe-
ciWc referent within a gesturally created scene, without any iconic component in the representation. There is no physically present target for these pointing gestures. The referent can be either concrete (e.g., physical movement) or abstract, as in the deWnition of McNeill et al. (1993).3 The hand shape does not have to have an extended index Wnger.
2.5.4. Other gesture types In addition, the following gesture types were distin-
guished, but not analyzed due to low frequency, as we detail below: Depictive gesture for shape by action, depictive unclear gestures, deictic gestures with a real-space referent, beats, desire-to-communicate gesture, conduit metaphor gestures, palm-up emblem, other emblems, metacognitive gestures. The deWnitions and further information for all gesture types can be seen in the On-line Supplementary Material.
2.6. Gesture handedness

(both hands perform a gesture stroke). Gestures in which one hand performed a stroke while the other hand was holding often from a previous gesture were not included in the analysis as they are ambiguous as to the uni- vs. bi-manual status.
2.7. Inter-coder reliability for gesture coding
For each participant and linguistic condition, the response to one randomly selected expression was selected for double-coding (8.3% of the data), and a second person coded the gestures in these responses in terms of gesture type and handedness (total 879 gestures). These codes were matched against the codes by the Wrst coder, who coded the entire data set. As we detail below, we included only three gesture types for statistical analysis (i.e., depictive gestures with the character viewpoint, depictive gestures with the observer viewpoint, and abstract deictic gestures). Thus, we calculated the inter-coder reliability of gesture-type coding by collapsing the remaining gesture types into one category. Gesture classiWcation matched between coders 74% of the time (Cohen’s D .61, p < .001). Coding of handedness matched between the two coders 92% of the time (Cohen’s
D .84, p < .001). For all analyses, the Wrst coder’s original coding was used.

Gestures were coded for their handedness, i.e. unimanual right-hand, unimanual left-hand, and bimanual gestures
3 In McNeill (1992), abstract deictic gestures include only pointing gestures with abstract referents, but not those that points to a location in a spatial scene that is iconically constructed by gestures. The latter type is, however, included in abstract pointing in this study and also in McNeill et al. (1993).

2.8. Analysis
To obtain measures that are sensitive to changes in hand choice, we calculated three indices, which we discussed in the introduction. The Wrst index indicates the preference for right-hand over left-hand unimanual gestures: the number of right-hand gestures divided by the sum of the numbers of right-hand gestures and left-hand gestures. A value of .5

190

S. Kita et al. / Brain and Language 101 (2007) 185–197

indicates that the two hands are equally frequent chosen, and values above (below) .5 indicate a preference for the right-hand (left-hand). Analogous indices were calculated (1) for the choice of bimanual gesture over right-hand gestures: the number of bimanual gestures divided by the sum of the numbers of right-hand gestures and bimanual gestures, and (2) the choice of bimanual gestures over unimanual gestures: the number of bimanual gestures divided by the sum of the numbers of right-hand gestures, left-hand gestures and bimanual gestures.
Because all the dependent variables used for statistical comparisons were proportions (the three gesture handedness indices and the proportion of spatial prepositions among all words), they were arc-sine transformed before ANOVAs and t-tests were conducted (Mosteller & Youtz, 1961; Snedecor & Cochran, 1989). Means and standard deviations reported in the tables and the main text were calculated from the untransformed proportions. The means and standard deviations were all calculated from the scores from each participant. For post-hoc tests comparing the three linguistic conditions, Fisher’s LSD t-tests were used, as recommended by Howell (2002) for a comparison of three means. All comparisons were two-tailed and an alpha level of .05 was used throughout.
3. Results
3.1. Speech
First, we examined if the crucial manipulation in the metaphor condition was eVective. The participants knew the phrase in the metaphor condition and were able to give responses on average 92% of the time (SD D 14). Among the analyzable responses in the metaphor condition, the participants explicitly mentioned a mapping between the literal meaning of the phrase and its metaphoric meaning on average 88% (SD D 14) of the time. Thus, the crucial manipulation in the metaphor condition was successful.
Second, we assessed the density of spatial contents in the verbal responses in the three linguistic conditions. The proportions of spatial prepositions among all words in the responses (abstract, M D .046, SD D .018; concrete, M D .073, SD D .016; metaphor, M D .066, SD D .011) were entered into a one-way ANOVA with linguistic condition as the independent variable. The proportions signiWcantly diVered between the linguistic conditions, F (2, 38) D 24.34, MSE D .000730, p D .001. Post-hoc tests indicated that the proportion was higher in the concrete condition than in the abstract, t (19) D 6.59, p < .001, and metaphor conditions, t (19) D 4.23, p D .002. The proportion was also higher for the metaphor condition than for the abstract condition, t (19) D 2.27, p D .035. Thus, the density of the spatial information and, by inference, spatial processing demands were in the following order (from high to low): concrete, metaphor, and abstract. This means that if handedness for gesture is diVerent in the metaphor condition as compared to

the concrete and abstract conditions, the diVerence cannot be attributed to a particularly high density of spatial content in the metaphor condition. If, however, hand choice for gesturing is diVerent between the three conditions, and the diVerence was in the order, concrete, metaphor, and abstract, then the diVerence might be better explained by the amount of spatial contents in speech rather than by metaphorical thinking in the metaphor condition.
3.2. Gesture
Across participants and linguistic conditions, a total of 6192 gestures could be categorized into one of the gesture types. From these gestures, we excluded 731 gestures in which one hand performed a stroke while the hand was holding, as they are ambiguous as to the uni- vs. bimanual status. Among the remaining gestures (unimanual gestures and bimanual gestures with both hands performing a stroke), we focused on the following three gesture types that allowed us to statistically compare hand preferences in the three linguistic conditions with at least Wve participants: depictive gestures with the character viewpoint gestures (frequency, M D 62, SD D 41; 31% were unimanual), depictive gestures with the observer viewpoint gestures (frequency, M D 126, SD D 86; 45% were unimanual), and abstract deictic gestures (frequency, M D 15, SD D 17; 64% were unimanual). The mean frequency of other types of gestures ranged from 2.6 (SD D 1.8) for desire-to-communicate gestures to 24 (SD D 20) for palm-up-emblems. We did not analyze palm-up-emblems as only a small minority, 14%, were unimanual gestures, making it impossible to investigate the right-hand over left-hand preference.
3.3. Linguistic tasks and the preference for right-hand over left-hand gestures
We Wrst compared the right-hand over left-hand preference for unimanual gestures between linguistic conditions.
Five participants produced unimanual abstract deictic gestures in all three linguistic conditions. The mean proportions of unimanual right-hand abstract deictic gestures (see Table 1) did not signiWcantly diVer between the linguistic conditions, F (2,8) D 1.50, MSE D .0141. Eleven participants produced unimanual depictive gestures with the character viewpoint in all three linguistic conditions. The mean proportions (see Table 1) were signiWcantly diVerent between the linguistic conditions, F (2,20) D 4.13, MSE D .0248,

Table 1 Mean proportion (SD) of right-hand gestures among all unimanual gestures in the three linguistic conditions for the three gesture types

Gesture type

Linguistic conditions

Abstract Concrete Metaphorical

Abstract deictic Depictive, character viewpoint Depictive, observer viewpoint

.88 (.25) .73 (.38) .83 (.25)

.96 (.09) .71 (.43) .82 (.25)

.98 (.04) .58 (.42) .77 (.36)

S. Kita et al. / Brain and Language 101 (2007) 185–197

191

p D .032. Post-hoc tests showed that the right-hand bias was lower in the metaphor condition than in the abstract, t (10) D 3.02, p D .013, and concrete conditions, t (10) D 2.26, p D .047, and that the abstract and concrete conditions did not diVer from each other, t (10) D .095. Thirteen participants produced unimanual depictive gestures with the observer viewpoint in all three linguistic conditions. The mean proportions (see Table 1) did not signiWcantly diVer between the linguistic conditions, F (2,24) D 0.34, MSE D .0429.
We further explored whether the proportion of righthand gestures signiWcantly deviated from .5 (i.e., equal use of the right and left hands) in the three linguistic conditions for the three gesture types, using one-sample t-tests on the data presented in Table 1. For abstract deictic gestures, the right over left-hand preference was signiWcant in all three conditions: abstract, t (4) D 3.30, p D .03; concrete, t (4) D 6.79, p D .03; metaphor, t (4) D 10.42, p < .001. For depictive gestures with the character viewpoint, the righthand preference was marginally signiWcant in the abstract condition, t (10) D 2.02, p D .07, and in the concrete condition, t (10) D 1.92, p D .08, but not signiWcant in the metaphor condition, t (10) D 0.66. When we included all participants who produced uni-manual depictive gestures with the character viewpoint in a given condition (unlike the data presented in Table 1, they did not have to produce relevant gestures in all three conditions), the right-hand preference was signiWcant in the abstract condition (M D .78, SD D .34), t (13) D 2.88, p D .013, and the concrete condition (M D .76, SD D .37), t (12) D 3.09, p D .008, but not in the metaphor condition (M D .60, SD D .41), t (14) D 0.82. For depictive gestures with the observer viewpoint, the right hand preference was signiWcant in all three conditions: abstract, t (12) D 4.07, p D .002; concrete, t (12) D 4.50, p D .001; metaphor, t (12) D 2.65, p D .021.
Finally, though the above results already suggest otherwise, we directly examined the possibility that depictive gestures with the character viewpoint had a weaker right-hand preference than the other two types of gestures, regardless of the linguistic conditions. We calculated the right-hand over left-hand preference by combining gestures across the three linguistic conditions (depictive gesture with the character viewpoint, M D .72, SD D .35; depictive gesture with the observer viewpoint, M D .76, SD D .32; abstract deictic gestures, M D .84, SD D .30) from the 17 participants who produced all three types of gestures unimanually. A oneway ANOVA with the three gesture types as the independent variable showed that the right-hand preference did not signiWcantly diVer between the gesture types, F (2, 32) D 2.28, MSE D 0.0465. We did the same analysis just for the gestures in the metaphor condition with the 13 participants who produced all three types of gestures unimanually in this condition (depictive character viewpoint, M D .66, S D .40; depictive observer viewpoint, M D ,85 S D .29, abstract deictic gestures, M D 83, S D .37). The right-hand preference signiWcantly diVered between the gesture types, F (2, 24) D 4.83, MSE D .0586, p D .017. Thus, the reduced

right-hand gesture preference in depictive gestures with the character viewpoint is speciWc to the metaphor condition.
To summarize, for depictive gestures with the character viewpoint, the right-hand over left-hand preference was signiWcantly reduced for the metaphor condition in comparison to the abstract and concrete conditions. The right-hand preference did not signiWcantly vary as a function of linguistic conditions in both abstract deictic gestures and depictive gestures with the observer viewpoint. The right hand was preferred over the left hand for abstract deictic gestures, depictive gestures with the observer viewpoint, regardless of the linguistic conditions. The right hand was preferred also for depictive gestures with the character viewpoint in the abstract and concrete conditions, but not signiWcantly so in the metaphor condition. Thus, there is speciWc attenuation of the right-hand preference for depictive gestures with the character viewpoint in the metaphor condition, as compared to other gesture types and other linguistic conditions.
3.4. Linguistic tasks and the preference for bimanual over right-hand gestures
Next, we compared the preference for bimanual over right-hand gestures between linguistic conditions. The numbers of participants producing bimanual and/or righthand gestures in all three linguistic conditions were as follows: Wve for abstract deictic gestures, 18 for depictive gestures with the character viewpoint, and 19 for depictive gestures with the observer viewpoint.
The preference for bimanual over right-hand gestures (see Table 2) did not signiWcantly diVer between the linguistic conditions for abstract deictic gestures, F (2, 8) D .423, MSE D 0.0601, for depictive gestures with the observer viewpoint, F (2, 36) D 2.48, MSE D 0.0566. For depictive gestures with the character viewpoint, the preference signiWcantly diVered, F (2, 34) D 4.40, MSE D 0.0594, p D .02. Post-hoc tests showed that the bimanual preference was stronger in the concrete condition than in the abstract condition, t (17) D 3.12, p D .006. Other pair-wise comparisons were not signiWcant: abstract vs. metaphor, t (17) D 1.86, metaphor vs. concrete, t(17) D 1.09.
This eVect could be either due to high demands on spatial processing in the concrete condition (see the above subsection on speech results) or due to the fact that many of the phrases in the concrete condition referred to bimanual action (e.g., tying a knot). In order to investigate these two possibili-

Table 2 Mean proportion (SD) of bimanual gestures among all bimanual gestures plus right-hand gestures in the three linguistic conditions for the three gesture types

Gesture type

Linguistic conditions

Abstract Concrete Metaphor

Abstract deictic Depictive, character viewpoint Depictive, observer viewpoint

.34 (.37) .63 (.37) .58 (.40)

.40 (.35) .83 (.25) .53 (.27)

.43 (.43) .74 (.28) .66 (.28)

192

S. Kita et al. / Brain and Language 101 (2007) 185–197

ties, the stimulus triplets (i.e., the corresponding items in the concrete, abstract, and metaphor conditions, as presented in Table 4 in the Appendix A) were divided into the following two categories: those for which the phrase in the concrete condition refers to bimanual action (“to create thread by interweaving many Wbres together,” “to remount a bike,” “to shoot an elevated target,” “to tie a knot using unconnected ropes,” to rotate a desk around”) and the rest of the triplets.
These action type categories were added to the ANOVA that compared the bimanual over right-hand preference in depictive gestures with the character viewpoint in the three linguistic conditions. The 2 ¤ 3 ANOVA yielded a signiWcant main eVect of action type, F (1,10) D 14.00, MSE D 0.0567, p D .004, but no signiWcant main eVect of language condition, F (2, 20) D 0.81, MSE D 0.0727. The interaction between action type and language condition was signiWcant, F (2, 20) D 10.07, MSE D 0.0420, p D .001.
The interaction was further explored by two separate oneway ANOVAs with language condition as the independent variable. For the stimulus triplets with bimanual actions in the concrete condition, the preference for bimanual over righthand gestures signiWcantly diVered between the linguistic conditions (abstract, M D.60, SD D .37; concrete, M D .95, SDD .08; metaphor, M D .74, SD D .36), F(2, 26) D 7.87, MSE D0.0796, p D .002. The bimanual preference was higher in the concrete condition than in the abstract, t(13) D 4.19, p D .001, and metaphorical conditions, t(13) D 2.51, p D .026, but the preference did not signiWcantly diVer between the abstract and metaphorical conditions, t(13) D 1.43. For the stimulus triplets without bimanual actions in the concrete condition, the preference for bimanual gestures did not signiWcantly diVer between the linguistic conditions (abstract, M D .73, SD D .36; concrete, M D .60, SD D .37; metaphor, M D .71, SD D .27), F(2, 26) D 1.59, MSE D 0.0475. Note that this lack of signiWcant diVerence between the linguistic conditions cannot be attributed to the lack of diVerence in spatial density between the linguistic conditions for this set of stimulus items (abstract, M D.046, SD D .021; concrete, M D .088, SDD .023; metaphor, M D .069, SD D .013). The density of spatial information signiWcantly diVered between the three conditions, F(2, 38) D 22.02, MSED 0.00174, p < .001. Posthoc tests showed that the density was highest in concrete (concrete vs. abstract, t(19) D 6.18, p < .001, concrete vs. metaphor, t(19) D 3.51, p D .002), and that the density was higher in metaphor than in abstract, t(19) D 3.46, p D .003. Thus, we conclude that the degree of bimanual preference was determined by the contents of what was depicted (Lausberg & Kita, 2003), rather than by spatial processing demands. Namely, when gestures depicted bimanual action (e.g., to tie a knot), they tended to be bimanual gestures, rather than righthand unimanual gestures.
3.5. Linguistic tasks and the preference for bimanual over unimanual gestures
Finally, we compared the preference for bimanual over unimanual gestures between linguistic conditions. The

numbers of participants producing bimanual and/or unimanual gestures in all three linguistic conditions were as follows: six for abstract deictic gestures, 18 for depictive gestures with the character viewpoint, and 19 for depictive gestures with the observer viewpoint.
The preference (see Table 3) did not signiWcantly diVer between linguistic conditions for abstract deictic gestures, F (2,10) D 0.65, MSE D 0.0571, for depictive gestures with the observer viewpoint, F (2, 36) D 1.72, MSE D 0.0563. The preference signiWcantly diVered for depictive gestures with the character viewpoint, F (2, 34) D 4.48, MSE D 0.0718, p D .019. Similarly to the analysis in the previous subsection, the post-hoc tests indicated that the bimanual preference was higher in the concrete condition than in the abstract condition, t (17) D 2.98, p D .008. Other pair-wise comparisons were not signiWcant: abstract vs. metaphor, t (17) D 0.91, metaphor vs. concrete, t (17) D 1.94.
In order to examine if this eVect could be either due to bimanual actions referred to in the phrases in the concrete condition, the bimanual over unimanual preference index for depictive gesture with the character viewpoint was entered into a 2 ¤ 3 ANOVA with action type (bimanual vs. non-bimanual) and linguistic conditions (concrete, abstract, metaphor). The main eVect of action type was signiWcant, F (1,12) D 11.94, MSE D 0.0721, p D .005, but not the main eVect of linguistic condition, F (2, 24) D 1.36, MSE D 0.0997. The interaction between action type and language condition was signiWcant, F (2, 24) D 14.66, MSE D 0.0395, p < .001.
The interaction was further explored by two separate one-way ANOVAs with language condition as the independent variable. For the stimulus triplets with bimanual actions in the concrete condition, the preference for bimanual gestures signiWcantly diVered between the linguistic conditions (abstract, M D .49, SD D .39; concrete, M D .92, SD D .14; metaphor, M D .64, SD D .35), F (2, 28) D 12.60, MSE D 0.0799, p < .001. The bimanual preference was higher in the concrete condition than in the abstract, t (14) D 4.74, p < .001, and metaphorical conditions, t (14) D 3.63, p D .003, but the bimanual preference was comparable for the abstract and metaphorical conditions, t (14) D 1.51. For the stimulus triplets without bimanual actions in the concrete condition, the preference for bimanual gestures did not signiWcantly diVer between the linguistic conditions (abstract, M D .67, SD D .37; concrete, M D .54, SD D .38; metaphor, M D .57, SD D .27), F (2, 28)

Table 3 Mean proportion (SD) of bimanual gestures among all gestures (both bimanual and unimanual) in three linguistic conditions for three gesture types

Gesture type

Linguistic conditions

Abstract Concrete Metaphorical

Abstract deictic

.31 (.38)

Depictive, character viewpoint .55 (.38)

Depictive, observer viewpoint .54 (.39)

.39 (.35) .78 (.26) .47 (.26)

.43 (.43) .62 (.28) .60 (.26)

S. Kita et al. / Brain and Language 101 (2007) 185–197

193

D 1.05, MSE D 0.0625. Thus, the higher bimanual preference in the concrete condition was again seen only for the stimulus triplets that include phrases in the concrete condition referring to bimanual actions. Thus, the degree of bimanual over unimanual preference was determined by the contents of what was depicted.
4. Discussion
The present study investigated the contribution of the two cerebral hemispheres in speech and gesture production. One hypothesis would claim that a common brain correlate for speech and gesture is situated in the language-dominant LH, and consequently, that gestures always exhibit a righthand bias (Kimura, 1973b, Lavergne & Kimura, 1987). An alternative hypothesis, which Kimura and Lavergne tested and rejected, would claim that both hemispheres can generate gestures and hand choice for gestures depends whether a given linguistic task requires a more LH or RH language ability. In order to test these hypotheses, we examined hand choices in a speaking task involving metaphors, which are thought to depend on the RH (Anaki et al., 1998; Bottini et al., 1994; Brownell et al., 1984; Rinaldi et al., 2004; Sotillo et al., 2005; Winner & Gardner, 1977). Accordingly, we assessed spontaneous hand preferences (right vs. left vs. bimanual) for gesturing during right-handed participants’ explanations of metaphorical, abstract, and concrete linguistic expressions. Crucially, the expressions in the abstract and concrete conditions were derived from the phrases in the metaphorical condition in order to maximally match the linguistic contents across the conditions. Moreover, rather then merging data across diVerent gesture types, we performed separate analyses of hand choice for three frequent gesture types (abstract deictic gestures, depictive gestures with the character viewpoint, depictive gestures with the observer viewpoint).
We used conventional phrases with a metaphorical meaning as stimuli in the metaphor condition. It has been noted, however, that novel metaphorical expressions activated the RH more than conventional ones (Mashal, Faust, & Hendler, 2005), which may suggest that retrieving conventional meaning associated with a phrase may not necessarily engage metaphor processing. In the current study, we encouraged the participants to describe the metaphorical mapping between the literal and metaphorical meanings in order to engage the RH metaphor processing. In the majority of the cases (88%), the participants explicitly mentioned the mapping, going beyond simple retrieval of conventional meaning associated with the phrases.
The main Wndings of the present study were that (1) the nature of linguistic tasks did not inXuence hand choices for both depictive gestures with the observer viewpoint and deictic abstract gestures. Both types of gestures were consistently produced more often with the right hand than with the left hand. (2) The nature of linguistic tasks signiWcantly inXuenced the right-hand over left-hand preference for depictive gestures with the character viewpoint. The right-

hand preference was signiWcantly attenuated in the metaphor condition than in the abstract and concrete conditions. The right-hand was used signiWcantly more often than the left hand in the concrete and abstract conditions, but not in the metaphor condition. (3) When the gestures from all three linguistic conditions were combined, the degree of the right-hand over left-hand preference did not signiWcantly diVer between the three types of gestures (depictive character viewpoint, depictive observer viewpoint, abstract deictic). However, when we restricted the analysis to the metaphor condition, depictive gestures with the character viewpoint had a signiWcantly attenuated right-hand preference as compared to the other two gesture types. (4) The content of what is depicted by gesture signiWcantly inXuenced the bimanual over right-hand preference and the bimanual over unimanual preference in depictive gestures with the character viewpoint gestures. When gestures depicted bimanual actions (e.g., tying a knot), not surprisingly, the character viewpoint gestures tended to be bimanual.
The signiWcantly attenuated right-hand over left-hand preference for depictive gestures with the character viewpoint during metaphor explanation would suggest that metaphor processing in the RH enhanced the RH contribution to gesture production. More importantly, this Wnding contradicts Lavergne and Kimura’s (1987) view that the LH is the exclusive source for the semantic representation of gestures. It rather supports the alternative hypothesis that both hemispheres can generate semantic representation of gestures, and relative activation levels of the two hemispheres in a given speaking task inXuence which hemisphere dominates in generating the semantic representation of gestures, and consequently, which hand is likely to produce gestures. The interpretation is consistent with the previous Wnding on spontaneous hand choice in non-gestural hand movements, as a function of relative activation levels of the two hemispheres. Hampson and Kimura (1984) showed that the hand spontaneously chosen for manipulation of blocks showed a stronger right-hand bias when the manipulation had to do with a silent verbal task (presumed LH involvement), and a weaker right-hand bias when the manipulation had to do with a silent spatial task (presumed RH involvement), as compared to a neutral base line task.
Note that existence of left hand gestures by itself is not the most convincing evidence against Lavergne and Kimura’s (1987) hypothesis. This is because the hypothesis concerns the generation of the informational content, in other words, the semantic (as opposed to motoric) representation of gesture. Motor commands for left hand gesture are clearly generated in the right hemisphere, but it is not clear if the semantic representations of such gestures are also generated in the right hemisphere. The results from the current study provide clearer evidence against Lavergne and Kimura’s hypothesis.
Further support for the view that both hemispheres can generate gestural representations has been provided in studies of split-brain patients. Lausberg and colleagues (2000) showed that split-brain patients yielded a left over

194

S. Kita et al. / Brain and Language 101 (2007) 185–197

right hand (shoulder) preference in “batons” (i.e., beats), shoulder shrug gestures, or “rise-fall” gestures (abbreviated form of a codiWed gesture that rotates the palm upward with a shoulder shrug). Moreover, Kita and Lausberg (in press) found in diVerent split-brain patients that depictive gestures with the observer viewpoint and abstract deictic gestures can be produced by the separated RH, contra the report by McNeill and Pedelty (1995). None of the splitbrain studies speciWcally investigated the hand choice for depictive gestures with the character viewpoint, which, given the present results, would be an interesting future research topic.
The rejection of Lavergne and Kimura’s hypothesis has implications for theories of speech-gesture production. The Wnding of the current study supports the view that gesture can be generated in conceptual processes that are coordinated with, but not a proper part of, speech production (e.g., Hadar & Yadlin-Gedassy, 1994; Kita & Özyürek, 2003; Krauss et al., 2000; McNeill, 1992). They are at odds with the view (e.g., Butterworth & Hadar, 1989) that gestures are generated from within the lexical access process, which is strongly left-lateralized (Indefrey & Levelt, 2004).
It remains, however, to be discussed why the eVect of linguistic tasks was not common to all analysed gesture types, but speciWc to depictive gestures with the character viewpoint. There are at least two possibilities. The Wrst possibility is that depictive character viewpoint gestures require RH processes more strongly than other types of gestures. For example, depictive character viewpoint gestures may involve imagination of how the character may act in a given situation. The RH was relatively more activated in tasks involving mental state inferences (e.g., Sax & Wexler, 2005, but see Samson, Apperly, Chiavarino, & Humphreys, 2004) and imagined object use by others (e.g., Ruby & Decety, 2001), though taking spatial perspective of others did not clearly activate the RH as opposed to the LH (David, Bewernick, Cohen, Newen, & Lux, 2006).
The second possibility is also based on the assumption that depictive gestures with character viewpoint can be seen as imitation of an imagined person’s action. Functional imaging studies showed imitation tasks activate not only areas in the LH such as Broca’s area, one of the classic language processing areas (Aziz-Zadeh, Koski, Zaidel, Mazziotta, & Iacoboni, 2006; Grèzes, Armony, Rowe, & Passingham, 2003; Iacoboni et al., 1999; Koski et al., 2002; Krams, Rushworth, Deiber, Frackowiak, & Passingham, 1998; Mühlau et al., 2005; Tanaka & Inui, 2002), but also areas in the RH, including the right homologue of Broca’s area (Heiser, Iacoboni, Maeda, Marcus, & Mazziotta, 2003; Koski et al., 2002; Krams et al., 1998; Mühlau et al., 2005). The cortical network involved in the generation of the underlying semantic representation of depictive gestures with the character viewpoints might span both hemispheres including both Broca’s area and its right homologue. Furthermore, for a given gesture, the generation of the underlying representation may be dominated by the network within one of the hemispheres, depending on the relative

activation levels of the two hemispheres in the concurrent speaking task. For example, the metaphor condition in the current experiment presumably activated the RH relatively strongly for speaking, which might have lead to more frequent generation of semantic representations for character viewpoint gestures in the RH network. This sensitivity to the status of language processing may come about via Broca’s area, which might serve as a “relay” (or an access point) that mediates the activation in the cortical networks for language processing and for processing of character viewpoint gestures. In addition, the idea that Broca’s area plays an important role in the interaction between speaking and gesturing dovetails with recent theories that language and gesture have a close evolutionary relationship (Corballis, 2002; Gentilucci, Benuzzi, Gangitano, & Graimaldi, 2001; Rizzolatti & Arbib, 1998), based on the LH language dominance and the human equivalent of “mirror neurons” (Rizzolatti, Fadiga, Gallese, & Fogassi, 1996), especially in Broca’s area, or on how grasping motion inXuences speech production (Gentilucci et al., 2001).
If the above line of speculation is correct, then observer viewpoint gestures and abstract deictic gestures should have a weaker link to major language processing areas such as Broca’s area and Wernicke’s area. Though it is diYcult to Wnd brain-imaging studies with very similar tasks, the execution of these types of gestures both involves moving or orienting a hand to a speciWc direction or location. Such spatially directed movements are known to involve a network consisting of parietal and dorsal premotor cortices (e.g., AstaWev et al., 2003; Grafton, Mazziotta, Woods, & Phelps, 1992; Karnath & Perenin, 2005; Nishitani, Uutela, Shibasaki, & Hari, 1999; Prado et al., 2005), but not the classical language areas (Broca’s and Wiernicke’s areas). Thus, the cortical network activated for these two types of gestures may not have as good access to the cortical network activated in a particular linguistic task (e.g., stronger right hemisphere involvement in the metaphor task) as the network activated for depictive gestures with the character viewpoint.
Observer viewpoint gestures and abstract deictic gestures had a right-hand preference regardless of the linguistic conditions. This Wnding is compatible not only with the general dexterity of the dominant hand, but also consistent with the idea that the LH plays a crucial role in goaldirected movements. An fMRI study by AstaWev et al. (2003) found that the regions activated during preparation phase for pointing movement are lateralized to the LH for both right- and left-hand pointing. The patient studies also show the crucial role of the LH in praxis in general (Goldenberg, 1996; Heilman & Rothi, 2003).
Our conclusion that the generation of depictive gestures with the character viewpoint can be generated from the RH is an interesting contrast with the reports on robust left lateralization of sign language processing. Sign language aphasia is typically caused by LH lesions, just like spoken language aphasia (see the review in Emmorey, 2002). Furthermore, generation of verbs in American Sign Language

S. Kita et al. / Brain and Language 101 (2007) 185–197

195

Table 4 Expressions used as stimuli in the three linguistic conditions (the idiomatic meaning of the metaphorical expressions is indicated in the parentheses)

Metaphorical

Concrete

Abstract

To dodge the bullet (To avoid a catastrophic event) To spin a yarn (To make up a story)
To lead someone up the garden path (To purposely mislead someone)
To swim against the tide (To be in an adverse situation) To fall back down to Earth with a bump (To suddenly come to
realize the reality) To get back in the saddle (To persistently attempt to do
something despite repeated failures) To set your sights higher (To have ambitious goals) To tie up loose ends (To Wnalize unresolved details) To turn a corner (To make a new start in life after experiencing
a major problem) To turn the tables (To change circumstances between two
opponents) To sit on the fence (To be indecisive) To spill the beans (To make secrete public)

To avoid being hit by an arrow To create thread by interweaving many Wbres together To guide someone through a park trail
To run up a downwards escalator To fall and hit the ground heavily
To remount a bike
To aim to shoot an elevated target To tie a knot using unconnected ropes To navigate a bend
To rotate a desk around
To perch on a garden wall To spill the marbles

A catastrophe To tell a fairy tale
To purposely mislead someone
To be in an adverse situation A reality check
To be persistent
To be ambitious To Wnalize details To deal with a problem
A change of circumstances
To be indecisive To disclose something conWdential

activated areas in the LH but not in the RH regardless of which hand was used to produce the verb (Corina, JoseRobertson, Guillemin, High, & Braun, 2003). This suggests that production of sign language and co-speech gesture may involve quite distinct division of labor between the two hemispheres.
Finally, the current study did not replicate Miller and Franz’s (2005) Wnding that a linguistic task with relatively high involvement of the RH (spatial description in Miller and Franz’s study) yields more bimanual gestures as opposed to unimanual gestures. We, instead, found that bimanual preference became stronger when the event depicted by gesture was a bimanual action. Thus, we interpret Miller and Franz’s results as stemming not from underlying hemispheric specialization, but from the type of information depicted by depictive gestures during description of complex spatial events and situations, as we suggested in the introduction (cf. Lausberg & Kita, 2003).
To summarize the key Wndings and conclusions, the degree of right-hand preference did not signiWcantly diVer in all linguistic conditions for depictive gestures with the observer viewpoint and abstract deictic gestures. More speciWcally, the right hand was used more often than the left hand in all linguistic conditions in these two types of gestures. In contrast, for depictive gestures with the character viewpoint (i.e., depictive gestures that imitate actions), the degree of right hand preference was signiWcantly attenuated in the metaphor condition than in the concrete and abstract conditions. More speciWcally, the right-hand was used signiWcantly more often than the left hand in the abstract and concrete conditions, but not in the metaphor condition. These Wndings indicate that (1) generation of semantic representations for depictive gestures with the character viewpoint may involve the RH in healthy participants, and (2) the production of depictive gestures with the character viewpoint may be more sensitive to the relative activation levels of the two hemispheres in language processing, as

compared to depictive gestures with the observer viewpoint and abstract deictic gestures.
Appendix A
See Table 4.
Appendix B. Supplementary data
Supplementary data associated with this article can be found, in the online version, at doi:10.1016/ j.bandl.2006.11.006.
References
Anaki, D., Faust, M., & Kravetz, S. (1998). Cerebral hemispheric asymmetries in processing lexical metaphors. Neuropsychologia, 36(7), 691–700.
AstaWev, S. V., Shulman, G. L., Stanley, C. M., Snyder, A. Z., Van Essen, D. C., & Corbetta, M. (2003). Functional organization of human intraparietal and frontal cortex for attending, looking, and pointing. Journal of Neuroscience, 23(11), 4689–4699.
Aziz-Zadeh, L., Koski, L., Zaidel, E., Mazziotta, J., & Iacoboni, M. (2006). Lateralization of the human mirror neuron system. The Journal of Neuroscience, 26, 2964–2974.
Berman, S. M., Mandelkern, M. A., Phan, H., & Zaidel, E. (2003). Complementary hemispheric specialization for word and accent detection. NeuroImage, 19(2), 319–331.
Bisiach, E., & Luzzatti, C. (1978). Unilateral neglect of representational space. Cortex, 14, 129–133.
Bottini, G., Corcoran, R., Sterzi, R., Paulesu, E., Schenone, P., Scarpa, P., et al. (1994). The role of the right-hemisphere in the interpretation of Wgurative aspects of language—a positron emission tomography activation study. Brain, 117, 1241–1253.
Brownell, H. H., Potter, H. H., Michelow, D., & Gardner, H. (1984). Sensitivity to lexical denotation and connotation in brain-damaged patients: a double dissociation. Brain and Language, 22, 253–265.
Bryden, M. P. (1982). Laterality. New York: Academic Press. Butterworth, B., & Hadar, U. (1989). Gesture, speech, and computational
stages: a reply to McNeill. Psychological Review, 96, 168–174. Corballis, M. C. (2002). From hand to mouth: The origins of language.
Princeton, NJ: Princeton University Press.

196

S. Kita et al. / Brain and Language 101 (2007) 185–197

Corina, D. P., Jose-Robertson, L. S., Guillemin, A., High, J., & Braun, A. R. (2003). Language lateralization in a bimanual language. Journal of Cognitive Neuroscience, 15(5), 718–730.
Dalby, J. T., Gibson, D., Grossi, V., & Schneider, R. D. (1980). Lateralized hand gesture during speech. Journal of Motor Behavior, 12(4), 292–297.
David, N., Bewernick, B. H., Cohen, M. X., Newen, A., Lux, S., et al. (2006). Neural representations of self versus other: visual–spatial perspective taking and agency in a virtual ball-tossing game. Journal of Cognitive Neuroscience, 18(6), 898–910.
Emmorey, K. (2002). Language, cognition, and the brain: Insights from sign language research. Mahwah, NJ: Lawrence Erlbaum.
Foundas, A. L., Macauley, B. L., Raymer, A. M., Maher, L. M., Heilman, K. M., & Rothi, L. J. G. (1995). Gesture laterality in aphasic and apraxic stroke patients. Brain and Cognition, 29(2), 204–213.
Gentilucci, M., Benuzzi, F., Gangitano, M., & Graimaldi, S. (2001). Grasp with hand and mouth: a kinematic study on healthy subjects. Journal of Neurophysiology, 86, 1685–1699.
Goldenberg, G. (1996). Defective imitation of gestures in patients with damage in the left or right hemispheres. Journal of Neurology, Neurosurgery, and Psychiatry, 61(2), 176–180.
Grafton, S. T., Mazziotta, J. C., Woods, R. P., & Phelps, M. E. (1992). Human functional-anatomy of visually guided Wnger movements. Brain, 115, 565–587.
Grèzes, J., Armony, J. L., Rowe, J., & Passingham, R. E. (2003). Activations related to “mirror” and “canonical” neurones in the human brain: an fMRI study. Neuroimage, 18(4), 928–937.
Guariglia, C., Piccardi, L., Iaria, G., Nico, D., & Pizzamiglio, L. (2005). Representational neglect and navigation in real space. Neuropsychologia, 43(8), 1138–1143.
Hadar, U., & Yadlin-Gedassy, S. (1994). Conceptual and lexical aspects of gesture: evidence from aphasia. Journal of Neurolinguistics, 8, 57–65.
Hampson, E., & Kimura, D. (1984). Hand movement asymmetries during verbal and nonvebal tasks. Canadian Journal of Psychology, 38, 102–125.
Heilman, K. M., & Rothi, L. J. G. (2003). Apraxia. In K. M. Heilman & E. Valenstein (Eds.), Clinical neuropsychology (4th ed., pp. 215–235). New York: Oxford University Press.
Heiser, M., Iacoboni, M., Maeda, F., Marcus, J., & Mazziotta, J. C. (2003). The essential role of Broca’s area in imitation. European Journal of Neuroscience, 17(5), 1123–1128.
Howell, D. C. (2002). Statistical methods for psychology. PaciWc Grove, CA: Duxbury.
Iacoboni, M., Woods, R. P., Brass, M., Bekkering, H., Mazziotta, J. C., & Rizzolatti, G. (1999). Cortical mechanisms of human imitation. Science, 286(5449), 2526–2528.
Indefrey, P., & Levelt, W. J. M. (2004). The spatial and temporal signatures of word production components. Cognition, 92(1-2), 101–144.
Karnath, H. O., & Perenin, M. T. (2005). Cortical control of visually guided reaching: evidence from patients with optic ataxia. Cerebral Cortex, 15(10), 1561–1569.
Kasher, A., Batori, G., Soroker, N., Graves, D., & Zaidel, E. (1999). EVects of right- and left-hemisphere damage on understanding conversational implicatures. Brain and Language, 68, 566–590.
Kendon, A. (1980). Gesticulation and speech: two aspects of the process of utterance. In M. R. Key (Ed.), The relation between verbal and nonverbal communication (pp. 207–227). The Hague: Mouton.
Kimura, D. (1973a). Manual activity during speaking. 1. Right-handers. Neuropsychologia, 11(1), 45–50.
Kimura, D. (1973b). Manual activity during speaking. 2. Left-handers. Neuropsychologia, 11(1), 51–55.
Kita, S., & Lausberg, H. (in press). Speech-gesture discoordination in split brain patients’ left-hand gestures: evidence for right-hemispheric generation of co-speech gestures. Cortex.
Kita, S., & Özyürek, A. (2003). What does cross-linguistic variation in semantic coordination of speech and gesture reveal? Evidence for an interface representation of spatial thinking and speaking. Journal of Memory and Language, 48, 16–32.

Kita, S., van Gijn, I., & van der Hulst, H. (1998). Movement phases in sign and co-speech gestures, and their transcription by human coders. In I. Wachsmuth & M. Fröhlich (Eds.), Gesture and sign language in human interaction (pp. 23–35). Springer.
Koski, L., Wohlschlager, A., Bekkering, H., Woods, R. P., Dubeau, M. C., Mazziotta, J. C., et al. (2002). Modulation of motor and premotor activity during imitation of target-directed actions. Cerebral Cortex, 12(8), 847–855.
Krams, M., Rushworth, M. F. S., Deiber, M. P., Frackowiak, R. S. J., & Passingham, R. E. (1998). The preparation, execution and suppression of copied movements in the human brain. Experimental Brain Research, 120(3), 386–398.
Krauss, R. M., Chen, Y., & Gottesman, R. F. (2000). Lexical gestures and lexical access: a process model. In D. McNeill (Ed.), Language and gesture (pp. 261–283). Cambridge: Cambridge University Press.
Lausberg, H., Davis, M., & Rothenhäuser, A. (2000). Hemispheric specialization in spontaneous gesticulation in a patient with callosal disconnection. Neuropsychologia, 38, 1654–1663.
Lausberg, H., & Kita, S. (2003). The content of the message inXuences the hand preference in co-speech gestures and in gesturing without speaking. Brain and Language, 86, 57–69.
Lavergne, J., & Kimura, D. (1987). Hand movement asymmetry during speech: no eVect of speaking topic. Neuropsychologia, 25, 689–693.
Levy, J., & Gur, R. C. (1980). Individual diVerences in psychoneurological organization. In J. Herron (Ed.), Neuropsychology of left-handedness (pp. 142–152). New York: Academic Press.
Mashal, N., Faust, M., & Hendler, T. (2005). The role of the right hemisphere in processing nonsalient metaphorical meanings: application of principal component analysis to fMRI data. Neuropsychologia, 43, 2084–2100.
McNeill, D. (1992). Hand and mind. What gestures reveal about thought. Chicago: University of Chicago Press.
McNeill, D., Cassell, J., & Levy, E. T. (1993). Abstract deixis. Semiotica, 95, 5–19.
McNeill, D., & Pedelty, L. L. (1995). Right brain and gesture. In J. K. R. Emmorey (Ed.), Language, gesture, and space (pp. 63–85). Mahwah, NJ: Lawrence Erlbaum Associates.
Miller, K., & Franz, E. A. (2005). Bimanual gestures: expressions of spatial representations that accompany speech processes. Laterality, 10(3), 243–265.
Mosteller, F., & Youtz, C. (1961). Tables of the Freeman–Tukey transformations for the binomial and Poisson distributions. Biometrika, 48(3/ 4), 433–440.
Mühlau, M., Hermsdörfer, J., Goldenberg, G., Wohlschläger, A. M., Castrop, F., Stahl, R., et al. (2005). Left inferior parietal dominance in gesture imitation: an fMRI study. Neuropsychologia, 43(7), 1086–1098.
Nishitani, N., Uutela, K., Shibasaki, H., & Hari, R. (1999). Cortical visuomotor integration during eye pursuit and eye-Wnger pursuit. Journal of Neuroscience, 19(7), 2647–2657.
OldWeld, R. C. (1971). The assessment and analysis of handedness: the Edinburgh Inventory. Neuropsychologia, 9, 97–113.
Prado, J., Clavagnier, S., Otzenberger, H., Scheiber, C., Kennedy, H., & Perenin, M. T. (2005). Two cortical systems for reaching in central and peripheral vision. Neuron, 48(5), 849–858.
Rapp, A. M., Leube, D. T., Erb, M., Grodd, W., & Kircher, T. T. J. (2004). Neural correlates of metaphor processing. Cognitive Brain Research, 20(3), 395–402.
Rauscher, F. H., Krauss, R. M., & Chen, Y. (1996). Gesture, speech, and lexical access: the role of lexical movements in speech production. Psychological Science, 7, 226–231.
Rinaldi, M. C., Marangolo, P., & Baldassarri, F. (2004). Metaphor comprehension in right brain-damaged patients with visuo-verbal and verbal material: a dissociation (re)considered. Cortex, 40(3), 479–490.
Rizzolatti, G., & Arbib, M. A. (1998). Language within our grasp. Trends in Neurosciences, 21(5), 188–194.
Rizzolatti, G., Fadiga, L., Gallese, V., & Fogassi, L. (1996). Premotor cortex and the recognition of motor actions. Cognitive Brain Research, 3(2), 131–141.

S. Kita et al. / Brain and Language 101 (2007) 185–197

197

Ruby, P., & Decety, J. (2001). EVect of subjective perspective taking during simulation of action: a PET investigation of agency. Nature Neuroscience, 4(5), 546–550.
Samson, D., Apperly, I. A., Chiavarino, C., & Humphreys, G. W. (2004). Left temporoparietal junction is necessary for representing someone else’s belief. Nature Neuroscience, 7(5), 499–500.
Sax, R., & Wexler, A. (2005). Making sense of another mind: the role of the right temporo-parietal junction. Neuropsychologia, 43(10), 1391–1399.
Snedecor, G. W., & Cochran, W. G. (1989). Statistical methods (8th ed.). Ames, IA: Iowa State University Press.
Sotillo, M., Carretie, L., Hinojosa, J. A., Tapia, M., Mercado, F., LopezMartin, S., et al. (2005). Neural activity associated with metaphor comprehension: spatial analysis. Neuroscience Letters, 373(1), 5–9.
Sousa-Poza, J. F., Rohrberg, R., & Mercure, A. (1979). EVects of type of information (abstract-concrete) and Weld-dependence on asymmetry of

hand movements during speech. Perceptual and Motor Skills, 48(3), 1323–1330. Stephens, D. (1983). Hemispheric language dominance and gesture hand preference. University of Chicago, Department of Behavioral Sciences. Tanaka, S., & Inui, T. (2002). Cortical involvement for action imitation of hand/arm postures versus Wnger conWgurations: an fMRI study. Neuroreport, 13(13), 1599–1602. Taylor, K. I., & Regard, M. (2003). Language in the right cerebral hemisphere: contributions from reading studies. News In Physiological Sciences, 18, 257–261. Van Lancker, D. (1997). Rags to riches: our increasing appreciation of cognitive and communicative abilities of the human right cerebral hemisphere. Brain and Language, 57(1), 1–11. Winner, E., & Gardner, H. (1977). The Comprehension of metaphor in brain-damaged patients. Brain, 100, 717–729.

